#æ–°0804data - ContentSummaryEn0913 é›†åˆ (è‹±æ–‡æ‘˜è¦) + Neo4j åœ–è­œè³‡æ–™
#ç”¨æˆ¶å•é¡Œ â†’ å‘é‡æª¢ç´¢ â†’ ç²å¾—æ–‡æª” â†’ å¾æ–‡æª”å…§å®¹æå–å¯¦é«” â†’ æŸ¥è©¢ä¸‰å…ƒçµ„
from typing import List, TypedDict, Optional, Dict
from langchain_core.documents.base import Document
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import PromptTemplate
from langgraph.graph import START, StateGraph
import weaviate
from weaviate import connect_to_local
import weaviate.classes as wvc
import google.generativeai as genai
from neo4j import GraphDatabase
import os
import dotenv
import time
import re
import requests
import traceback
import json

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
dotenv.load_dotenv()

# è¨­å®š API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

MISTRAL_API_KEY = os.getenv("J_MISTRAL_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

# Setup LLM
from langchain.chat_models import init_chat_model
llm: BaseChatModel = init_chat_model(model="gemini-2.0-flash", model_provider="google_genai")

# æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
def is_chinese(text):
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
    return bool(chinese_pattern.search(text))

# èª¿ç”¨ Mistral API
def call_mistral(prompt, max_retries=3, base_delay=5):
    """èª¿ç”¨ Mistral APIï¼ˆåŠ å…¥ 429 éŒ¯èª¤è™•ç†ï¼‰"""
    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "mistral-small-latest",
        "temperature": 0.1,
        "top_p": 1,
        "max_tokens": 7800,
        "messages": [{"role": "user", "content": prompt}]
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                # è¶…éå®¹é‡é™åˆ¶ï¼Œç­‰å¾…å¾Œé‡è©¦
                wait_time = base_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                print(f"âš ï¸ API å®¹é‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦... (å˜—è©¦ {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                raise Exception(f"{response.status_code}, {response.text}")
                
        except Exception as e:
            if attempt == max_retries - 1:
                raise Exception(f"Mistral API éŒ¯èª¤ï¼š{e}")
            else:
                print(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼Œ{base_delay} ç§’å¾Œé‡è©¦...")
                time.sleep(base_delay)
    
    raise Exception("è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")
def choose_search_strategy_with_mistral(question: str) -> str:
    """ä½¿ç”¨ Mistral åˆ¤æ–·æ‡‰è©²ä½¿ç”¨å“ªç¨®æœå°‹ç­–ç•¥"""
    
    prompt = f"""è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ‡‰è©²ä½¿ç”¨å“ªç¨®æœå°‹ç­–ç•¥ï¼š

å•é¡Œï¼š{question}

æœå°‹ç­–ç•¥èªªæ˜ï¼š
1. **Global Search (å…¨å±€æœå°‹)**ï¼š
   - é©ç”¨æ–¼éœ€è¦ç†è§£æ•´å€‹è³‡æ–™é›†ã€å¤šå€‹ä¾†æºçš„å•é¡Œ
   - ä¾‹å¦‚ï¼šã€Œå­¸æ ¡æœ‰å“ªäº›å­¸é™¢ï¼Ÿã€ã€Œæ•´é«”æ¶æ§‹å¦‚ä½•ï¼Ÿã€ã€Œæ‰€æœ‰çš„ç³»æ‰€ã€
   - éœ€è¦ç¶œåˆå¤šå€‹ç¤¾ç¾¤çš„è³‡è¨Š

2. **Local Search (å±€éƒ¨æœå°‹)**ï¼š
   - é©ç”¨æ–¼é‡å°ç‰¹å®šå¯¦é«”ã€å…·é«”ç´°ç¯€çš„å•é¡Œ
   - ä¾‹å¦‚ï¼šã€Œè³‡å·¥ç³»çš„é›»è©±ã€ã€ŒæŸæŸå¤§æ¨“çš„åœ°å€ã€ã€Œå¦‚ä½•è¯çµ¡XXéƒ¨é–€ã€
   - éœ€è¦ç²¾ç¢ºå®šä½ç‰¹å®šè³‡è¨Š

è«‹åªå›ç­” "global" æˆ– "local"ï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""
    
    try:
        response = call_mistral(prompt)
        strategy = response["choices"][0]["message"]["content"].strip().lower()
        
        # æ¸…ç†å›æ‡‰ï¼ˆç§»é™¤å¯èƒ½çš„å¼•è™Ÿæˆ–å¤šé¤˜æ–‡å­—ï¼‰
        if 'global' in strategy:
            return 'global'
        elif 'local' in strategy:
            return 'local'
        else:
            # é è¨­ä½¿ç”¨ localï¼ˆæ›´å®‰å…¨ï¼‰
            print(f"âš ï¸ Mistral å›æ‡‰ä¸æ˜ç¢º: {strategy}ï¼Œé è¨­ä½¿ç”¨ local")
            return 'local'
            
    except Exception as e:
        print(f"âŒ Mistral ç­–ç•¥åˆ¤æ–·å¤±æ•—: {e}ï¼Œé è¨­ä½¿ç”¨ local")
        return 'local'
    
# ä½¿ç”¨ Mistral æå–å¯¦é«”
def extract_entities_with_mistral(text: str) -> List[str]:
    """ä½¿ç”¨ Mistral å¾æ–‡å­—ä¸­æå–å¯¦é«”åç¨±"""
    prompt = f"""è«‹å¾ä»¥ä¸‹æ–‡å­—ä¸­æå–é‡è¦çš„å¯¦é«”åç¨±ï¼Œç‰¹åˆ¥æ˜¯ï¼š
    - å­¸æ ¡åç¨±ï¼ˆå¦‚ï¼šåœ‹ç«‹è¯åˆå¤§å­¸ï¼‰
    - å­¸é™¢åç¨±ï¼ˆå¦‚ï¼šè³‡è¨Šå­¸é™¢ï¼‰
    - ç³»æ‰€åç¨±ï¼ˆå¦‚ï¼šè³‡è¨Šå·¥ç¨‹ç³»ï¼‰
    - éƒ¨é–€åç¨±ï¼ˆå¦‚ï¼šå­¸å‹™è™•ã€æ•™å‹™è™•ï¼‰
    - ä¸­å¿ƒåç¨±ï¼ˆå¦‚ï¼šè¨ˆç®—æ©Ÿä¸­å¿ƒï¼‰
    
    æ–‡å­—ï¼š{text}
    
    è«‹åªå›å‚³å¯¦é«”åç¨±ï¼Œæ¯å€‹å¯¦é«”ä¸€è¡Œï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""
    
    try:
        response = call_mistral(prompt)
        entities_text = response["choices"][0]["message"]["content"].strip()
        
        # åˆ†å‰²æˆå€‹åˆ¥å¯¦é«”
        entities = [entity.strip() for entity in entities_text.split('\n') if entity.strip()]
        
        # éæ¿¾æ‰å¤ªçŸ­çš„å¯¦é«”
        filtered_entities = [e for e in entities if len(e) >= 2 and len(e) <= 20]
        
        print(f"ğŸ¤– Mistralæå–åˆ°å¯¦é«”: {filtered_entities}")
        return filtered_entities
        
    except Exception as e:
        print(f"âŒ Mistralå¯¦é«”æå–å¤±æ•—: {e}")
        return []
def extract_concepts_for_global_search(text: str) -> dict:
    """é‡å° Global Search æå–åˆ†å±¤æ¦‚å¿µï¼ˆæ ¸å¿ƒ + è¼”åŠ©ï¼‰
    
    Returns:
        {
            'core': ['æ ¸å¿ƒæ¦‚å¿µ1', 'æ ¸å¿ƒæ¦‚å¿µ2'],      # å¿…é ˆåŒ¹é…
            'auxiliary': ['è¼”åŠ©æ¦‚å¿µ1', 'è¼”åŠ©æ¦‚å¿µ2']  # åŠ åˆ†é …
        }
    """
    prompt = f"""è«‹åˆ†æä»¥ä¸‹å•é¡Œï¼Œæå–é—œéµæ¦‚å¿µä¸¦åˆ†ç‚ºå…©é¡ï¼š

å•é¡Œï¼š{text}

è«‹åˆ†é¡ç‚ºï¼š
1. **æ ¸å¿ƒæ¦‚å¿µ**ï¼ˆå¿…é ˆåŒ¹é…ï¼Œ1-3å€‹ï¼‰ï¼šæŸ¥è©¢çš„ä¸»è¦å°è±¡æˆ–æ ¸å¿ƒä¸»é¡Œ
   - ä¾‹å¦‚å•ã€Œæœ‰å“ªäº›ç§‘ç³»ã€â†’ æ ¸å¿ƒæ¦‚å¿µæ˜¯ã€Œç§‘ç³»ã€ã€Œå­¸ç³»ã€ã€Œå­¸é™¢ã€
   - ä¾‹å¦‚å•ã€Œå­¸ç”Ÿæœå‹™ã€â†’ æ ¸å¿ƒæ¦‚å¿µæ˜¯ã€Œæœå‹™ã€ã€Œå­¸ç”Ÿã€
   
2. **è¼”åŠ©æ¦‚å¿µ**ï¼ˆåŠ åˆ†é …ï¼Œ2-4å€‹ï¼‰ï¼šç›¸é—œçš„é ˜åŸŸã€å±¬æ€§æˆ–èƒŒæ™¯è©å½™
   - ä¾‹å¦‚å•ã€Œæœ‰å“ªäº›ç§‘ç³»ã€â†’ è¼”åŠ©æ¦‚å¿µæ˜¯ã€Œå­¸è¡“å–®ä½ã€ã€Œæ•™è‚²ã€ã€Œæ•™å­¸ã€
   - ä¾‹å¦‚å•ã€Œå­¸ç”Ÿæœå‹™ã€â†’ è¼”åŠ©æ¦‚å¿µæ˜¯ã€Œè¡Œæ”¿ã€ã€Œæ”¯æ´ã€ã€Œè³‡æºã€

å›ç­”æ ¼å¼ï¼ˆåš´æ ¼éµå®ˆï¼‰ï¼š
æ ¸å¿ƒï¼šæ¦‚å¿µ1, æ¦‚å¿µ2, æ¦‚å¿µ3
è¼”åŠ©ï¼šæ¦‚å¿µ4, æ¦‚å¿µ5

å›ç­”ï¼š"""
    
    try:
        response = call_mistral(prompt)
        content = response["choices"][0]["message"]["content"].strip()
        
        # è§£æå›æ‡‰
        core_concepts = []
        auxiliary_concepts = []
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('æ ¸å¿ƒï¼š') or line.startswith('æ ¸å¿ƒ:'):
                core_part = line.split('ï¼š')[-1].split(':')[-1]
                core_concepts = [c.strip() for c in core_part.split(',') if c.strip()]
            elif line.startswith('è¼”åŠ©ï¼š') or line.startswith('è¼”åŠ©:'):
                aux_part = line.split('ï¼š')[-1].split(':')[-1]
                auxiliary_concepts = [c.strip() for c in aux_part.split(',') if c.strip()]
        
        # éæ¿¾é•·åº¦
        core_concepts = [c for c in core_concepts if 2 <= len(c) <= 15][:3]
        auxiliary_concepts = [c for c in auxiliary_concepts if 2 <= len(c) <= 15][:4]
        
        print(f"ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ: {core_concepts}")
        print(f"â• è¼”åŠ©æ¦‚å¿µ: {auxiliary_concepts}")
        
        return {
            'core': core_concepts,
            'auxiliary': auxiliary_concepts
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µæå–å¤±æ•—: {e}")
        # é™ç´šï¼šä½¿ç”¨åŸå§‹æ–¹æ³•
        basic_concepts = text.replace('?', '').replace('ï¼Ÿ', '').split()
        return {
            'core': basic_concepts[:3],
            'auxiliary': []
        }
# ç¿»è­¯ç®¡ç†å™¨ - æ”¯æ´å¤–éƒ¨ç¿»è­¯æª”æ¡ˆ
class TranslationManager:

    def __init__(self, translation_file_path=r"CS_Project\CS_App\translate\translation_mapping.json"):
        self.translation_file = translation_file_path
        self.translation_dict = self._load_translation_dict()
    
    # ç›´æ¥å¾ self.translation_file è®€å–ç¿»è­¯æ˜ å°„æª”æ¡ˆ,ä¸å­˜åœ¨å‰‡å ±éŒ¯
    def _load_translation_dict(self) -> dict:
        try:
            with open(self.translation_file, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
            print(f"âœ… æˆåŠŸè¼‰å…¥ç¿»è­¯æ˜ å°„æª”æ¡ˆ,æ¢ç›®æ•¸:{len(mapping)}")
            return mapping
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¿»è­¯æ˜ å°„æª”æ¡ˆ:{self.translation_file},è«‹å…ˆå»ºç«‹æª”æ¡ˆã€‚")
        except Exception as e:
            print(f"âŒ è¼‰å…¥ç¿»è­¯æ˜ å°„å¤±æ•—: {e}")
            return {}
    
    # å¾æ˜ å°„æª”æ¡ˆä¸­ç²å–ç¿»è­¯
    def get_translation(self, chinese_text: str) -> Optional[str]:
        return self.translation_dict.get(chinese_text)
    
    # ä½¿ç”¨æ˜ å°„æª”æ¡ˆé€²è¡Œç¿»è­¯,æœªå‘½ä¸­å‰‡ä½¿ç”¨Mistral
    def translate_with_mapping(self, text: str) -> str:
        # å…ˆæª¢æŸ¥æ˜ å°„æª”æ¡ˆ
        direct_translation = self.get_translation(text)
        if direct_translation:
            print(f"ğŸ“„ æ˜ å°„æª”æ¡ˆç¿»è­¯: {text} â†’ {direct_translation}")
            return direct_translation
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜ å°„ä¸­çš„è©èª
        translated_parts = []
        remaining_text = text
        
        for chinese_term, english_term in self.translation_dict.items():
            if chinese_term in remaining_text:
                remaining_text = remaining_text.replace(chinese_term, f"__TRANSLATED_{len(translated_parts)}__")
                translated_parts.append((chinese_term, english_term))
        
        if translated_parts:
            # éƒ¨åˆ†å‘½ä¸­,çµ„åˆç¿»è­¯
            final_translation = remaining_text
            for i, (chinese_term, english_term) in enumerate(translated_parts):
                final_translation = final_translation.replace(f"__TRANSLATED_{i}__", english_term)
            
            # å¦‚æœé‚„æœ‰å‰©é¤˜ä¸­æ–‡,ç”¨Mistralç¿»è­¯
            if is_chinese(final_translation):
                final_translation = self._mistral_translate(final_translation)
            
            print(f"ğŸ”„ æ··åˆç¿»è­¯: {text} â†’ {final_translation}")
            return final_translation
        
        # å®Œå…¨æœªå‘½ä¸­,ä½¿ç”¨Mistral
        return self._mistral_translate(text)
    
    # ä½¿ç”¨Mistralé€²è¡Œç¿»è­¯
    def _mistral_translate(self, text: str) -> str:
        try:
            response = call_mistral(f"è«‹å°‡ä»¥ä¸‹ä¸­æ–‡ç¿»è­¯æˆè‹±æ–‡,åªå›å‚³ç¿»è­¯çµæœ:{text}")
            translated = response["choices"][0]["message"]["content"].strip()
            if translated.startswith('"') and translated.endswith('"'):
                translated = translated[1:-1]
            print(f"ğŸ¤– Mistralç¿»è­¯: {text} â†’ {translated}")
            return translated
        except Exception as e:
            print(f"âŒ Mistralç¿»è­¯å¤±æ•—: {e}")
            return text

# åˆå§‹åŒ–ç¿»è­¯ç®¡ç†å™¨
translation_manager = TranslationManager()

# ä½¿ç”¨ç¿»è­¯ç®¡ç†å™¨é€²è¡Œç¿»è­¯ï¼ˆæ˜ å°„æª”æ¡ˆå„ªå…ˆï¼Œç„¶å¾ŒMistralï¼‰
def translate_with_mistral(text: str, max_retries=3, delay=2) -> str:
    for attempt in range(max_retries):
        try:
            return translation_manager.translate_with_mapping(text)
        except Exception as e:
            print(f"âŒ [ç¿»è­¯å¤±æ•—] ç¬¬ {attempt + 1} æ¬¡å˜—è©¦ï¼ŒéŒ¯èª¤ï¼š{e}")
            if attempt < max_retries - 1:
                time.sleep(delay)
    raise Exception("ç¿»è­¯å¤±æ•—ï¼Œè¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")

# Gemini åµŒå…¥å™¨
class GeminiEmbedder:
    def embed_query(self, text: str) -> List[float]:
        try:
            response = genai.embed_content(
                model="models/text-embedding-004",
                content=text
            )
            return response['embedding']
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return [0.0] * 768  

# åˆå§‹åŒ– Weaviate å’Œ Neo4j é€£æ¥
class ContentSummaryEn0913RAG:
    
    def __init__(self):
        try:
            # åˆå§‹åŒ– Weaviate é€£æ¥
            self.client = connect_to_local()
            print("âœ… æˆåŠŸé€£æ¥åˆ° Weaviate")
            
            # åˆå§‹åŒ– Neo4j é€£æ¥
            self.neo4j_driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "neo4j0804"),
                database="nuu-data-0804"
            )
            print("âœ… æˆåŠŸé€£æ¥åˆ° Neo4j")
            
            self.triplet_driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "neo4j0804"),
                database="nuu-triplet"
            )
            print("âœ… æˆåŠŸé€£æ¥åˆ° Neo4j ä¸‰å…ƒçµ„åœ–è­œ")
            
            # åˆå§‹åŒ–åµŒå…¥å™¨
            self.embedder = GeminiEmbedder()
            print("âœ… åˆå§‹åŒ– Gemini åµŒå…¥å™¨")
            
            self._check_community_status()
            
            # åªè¼‰å…¥ ContentSummaryEn0913 é›†åˆ
            try:
                self.collection = self.client.collections.get("ContentSummaryEn0913")
                self.text_field = "english_summary"
                print(f"ğŸ“š å·²è¼‰å…¥ ContentSummaryEn0913 é›†åˆ (æ–‡æœ¬æ¬„ä½: {self.text_field})")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ ContentSummaryEn0913 é›†åˆ: {e}")
                self.collection = None
            
        except Exception as e:
            print(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            self.client = None
            self.neo4j_driver = None
            self.embedder = None
            self.collection = None
    def _check_community_status(self):
        """æª¢æŸ¥ç¤¾ç¾¤è³‡æ–™æ˜¯å¦å·²å»ºæ§‹"""
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run("""
                    MATCH (c:Community)
                    WHERE c.summary IS NOT NULL
                    RETURN count(c) as count
                """)
                count = result.single()['count']
                
                if count == 0:
                    print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ç¤¾ç¾¤æ‘˜è¦è³‡æ–™")
                    print("   è«‹å…ˆåŸ·è¡Œ build_communities.py")
                else:
                    print(f"âœ… å·²è¼‰å…¥ {count} å€‹ç¤¾ç¾¤æ‘˜è¦")
        except:
            print("âš ï¸ ç„¡æ³•æª¢æŸ¥ç¤¾ç¾¤ç‹€æ…‹")
    def _determine_best_level_with_mistral(self, query: str) -> int:
        """ä½¿ç”¨ Mistral åˆ¤æ–·æœ€ä½³æª¢ç´¢å±¤ç´š
        
        Returns:
            0: åº•å±¤ï¼ˆå…·é«”å¯¦é«”ç´°ç¯€ï¼‰
            1: ä¸­å±¤ï¼ˆé¡åˆ¥ã€éƒ¨é–€å±¤ç´šï¼‰
            2: é«˜å±¤ï¼ˆå…¨å±€ã€æ¶æ§‹å±¤ç´šï¼‰
        """
        prompt = f"""è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ‡‰è©²åœ¨çŸ¥è­˜åœ–è­œçš„å“ªå€‹å±¤ç´šæª¢ç´¢ï¼š

    å•é¡Œï¼š{query}

    å±¤ç´šèªªæ˜ï¼š
    - **Level 2 (é«˜å±¤ç´š)**ï¼šå›ç­”éœ€è¦æ•´é«”æ¶æ§‹ã€å…¨å±€è¦–è§’ã€è·¨é ˜åŸŸç¶œåˆè³‡è¨Š
    ä¾‹å¦‚ï¼šã€Œå­¸æ ¡æœ‰å“ªäº›å­¸é™¢ï¼Ÿã€ã€Œæ•´é«”çµ„ç¹”æ¶æ§‹ã€ã€Œæ‰€æœ‰çš„é™¢ç³»ã€
    
    - **Level 1 (ä¸­å±¤ç´š)**ï¼šå›ç­”éœ€è¦æŸå€‹é¡åˆ¥ã€æŸå€‹é ˜åŸŸçš„ç¶œåˆè³‡è¨Š
    ä¾‹å¦‚ï¼šã€Œæœ‰å“ªäº›ç§‘ç³»ï¼Ÿã€ã€Œå·¥ç¨‹ç›¸é—œçš„ç³»æ‰€ã€ã€Œå­¸ç”Ÿæœå‹™æœ‰å“ªäº›ï¼Ÿã€
    
    - **Level 0 (åº•å±¤ç´š)**ï¼šå›ç­”éœ€è¦å…·é«”å¯¦é«”çš„è©³ç´°è³‡è¨Š
    ä¾‹å¦‚ï¼šã€Œè³‡å·¥ç³»çš„é›»è©±ã€ã€ŒæŸæŸå¤§æ¨“åœ¨å“ªè£¡ã€ã€Œå¦‚ä½•è¯çµ¡XXéƒ¨é–€ã€

    è«‹åªå›ç­”æ•¸å­— 0ã€1 æˆ– 2ï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""
        
        try:
            response = call_mistral(prompt)
            level_str = response["choices"][0]["message"]["content"].strip()
            
            # æå–æ•¸å­—
            import re
            match = re.search(r'[0-2]', level_str)
            if match:
                level = int(match.group())
                print(f"ğŸ¯ Mistral é¸æ“‡å±¤ç´š: Level {level}")
                return level
            else:
                print(f"âš ï¸ Mistral å›æ‡‰ä¸æ˜ç¢º: {level_str}ï¼Œé è¨­ä½¿ç”¨ Level 1")
                return 1
                
        except Exception as e:
            print(f"âŒ Mistral å±¤ç´šåˆ¤æ–·å¤±æ•—: {e}ï¼Œé è¨­ä½¿ç”¨ Level 1")
            return 1     
    def find_relevant_communities(self, query: str, limit: int = None, level: int = None) -> List[dict]:
        """æ ¹æ“šæŸ¥è©¢æ‰¾å‡ºç›¸é—œç¤¾ç¾¤ï¼ˆæ”¹é€²ç‰ˆï¼šæ”¯æ´æ™ºèƒ½å±¤ç´š + åˆ†å±¤æ¦‚å¿µï¼‰
        
        Args:
            query: æŸ¥è©¢å•é¡Œ
            limit: è¿”å›çš„ç¤¾ç¾¤æ•¸é‡ï¼ˆNone = è‡ªå‹•æ ¹æ“šå±¤ç´šæ±ºå®šï¼‰
            level: æŒ‡å®šæœå°‹çš„å±¤ç´šï¼ˆNone = è‡ªå‹•é¸æ“‡æœ€ä½³å±¤ç´šï¼‰
        """
        if not self.triplet_driver:
            return []
        
        # 1. æ™ºèƒ½é¸æ“‡å±¤ç´š
        if level is None:
            level = self._determine_best_level_with_mistral(query)
        else:
            print(f"ğŸ¯ æ‰‹å‹•æŒ‡å®šå±¤ç´š: Level {level}")
        
        # 2. æ ¹æ“šå±¤ç´šæ±ºå®šæª¢ç´¢æ•¸é‡
        if limit is None:
            if level >= 2:
                limit = 5   # é«˜å±¤ç´šç¤¾ç¾¤è¼ƒå°‘
            elif level == 1:
                limit = 8   # ä¸­å±¤ç´šéœ€è¦æ›´å¤š
            else:
                limit = 15  # åº•å±¤ç´šå¯èƒ½éœ€è¦å¾ˆå¤š
        
        # 3. æå–åˆ†å±¤æ¦‚å¿µ
        concepts_dict = extract_concepts_for_global_search(query)
        core_concepts = concepts_dict.get('core', [])
        auxiliary_concepts = concepts_dict.get('auxiliary', [])
        
        if not core_concepts:
            print("âš ï¸ æœªæå–åˆ°æ ¸å¿ƒæ¦‚å¿µï¼Œä½¿ç”¨é—œéµè©åŒ¹é…")
            return self._find_communities_by_keywords(query, limit)
        
        # 4. å®šç¾©å­¸æ ¡ç›¸é—œé—œéµè©
        school_keywords = ['åœ‹ç«‹è¯åˆå¤§å­¸', 'National United University', 'è¯å¤§', 'NUU']
        
        # 5. æ ¹æ“šå±¤ç´šèª¿æ•´æŸ¥è©¢ç­–ç•¥
        if level >= 2:
            # é«˜å±¤ç´šï¼šåªè¦æ ¸å¿ƒæ¦‚å¿µåŒ¹é…ï¼Œå­¸æ ¡é—œéµè©å¯é¸
            where_clause = "WHERE core_matches > 0"
            order_clause = "ORDER BY c.level ASC, core_matches DESC, aux_matches DESC, school_matches DESC"
        elif level == 1:
            # ä¸­å±¤ç´šï¼šæ ¸å¿ƒæ¦‚å¿µå¿…é ˆï¼Œå­¸æ ¡é—œéµè©åŠ åˆ†
            where_clause = "WHERE core_matches > 0"
            order_clause = "ORDER BY c.level ASC, core_matches DESC, school_matches DESC, aux_matches DESC"
        else:
            # åº•å±¤ç´šï¼šæ ¸å¿ƒæ¦‚å¿µ + å­¸æ ¡é—œéµè©éƒ½è¦åŒ¹é…
            where_clause = "WHERE core_matches > 0 AND school_matches > 0"
            order_clause = "ORDER BY c.level ASC, school_matches DESC, core_matches DESC, aux_matches DESC"
        
        # 6. æ§‹å»ºæŸ¥è©¢
        query_cypher = f"""
        MATCH (c:Community {{level: $level}})
        WHERE c.summary IS NOT NULL
        
        WITH c, $core_concepts as core_concepts, 
            $aux_concepts as aux_concepts, 
            $school_keywords as school_keywords
        
        // è¨ˆç®—å„é¡åŒ¹é…æ•¸
        WITH c, 
            SIZE([core IN core_concepts WHERE c.summary CONTAINS core]) as core_matches,
            SIZE([aux IN aux_concepts WHERE c.summary CONTAINS aux]) as aux_matches,
            SIZE([school IN school_keywords WHERE c.summary CONTAINS school]) as school_matches
        
        {where_clause}
        
        RETURN 
            c.id as community_id,
            c.level as level,
            c.summary as summary,
            c.entity_count as entity_count,
            core_matches as core_relevance,
            aux_matches as aux_relevance,
            school_matches as school_relevance
        {order_clause}, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(
                    query_cypher,
                    core_concepts=core_concepts,
                    aux_concepts=auxiliary_concepts,
                    school_keywords=school_keywords,
                    level=level,
                    limit=limit
                )
                communities = [dict(record) for record in result]
                
                if not communities:
                    print(f"âš ï¸ Level {level} ç„¡åŒ¹é…çµæœï¼Œå˜—è©¦å›é€€ç­–ç•¥...")
                    return self._fallback_multi_level_search(query, level, limit)
                
                print(f"âœ… åœ¨ Level {level} æ‰¾åˆ° {len(communities)} å€‹ç›¸é—œç¤¾ç¾¤")
                for i, comm in enumerate(communities[:3], 1):
                    preview = comm['summary'][:80] + "..." if len(comm['summary']) > 80 else comm['summary']
                    print(f"  [{i}] ç¤¾ç¾¤ {comm['community_id']} (æ ¸å¿ƒ:{comm['core_relevance']}, è¼”åŠ©:{comm['aux_relevance']}, å­¸æ ¡:{comm['school_relevance']})")
                    print(f"      {preview}")
                
                return communities
                
        except Exception as e:
            print(f"âŒ æŸ¥è©¢ç›¸é—œç¤¾ç¾¤å¤±æ•—: {e}")
            traceback.print_exc()
            return []
    def _fallback_multi_level_search(self, query: str, original_level: int, limit: int) -> List[dict]:
        """ç•¶æŒ‡å®šå±¤ç´šç„¡çµæœæ™‚ï¼Œå˜—è©¦ç›¸é„°å±¤ç´š"""
        print(f"ğŸ”„ åŸ·è¡Œå¤šå±¤ç´šå›é€€æœå°‹ï¼ˆåŸå±¤ç´š: {original_level}ï¼‰")
        
        # å®šç¾©å›é€€é †åº
        if original_level == 2:
            fallback_levels = [1, 0]
        elif original_level == 1:
            fallback_levels = [2, 0]
        else:  # original_level == 0
            fallback_levels = [1, 2]
        
        all_communities = []
        
        for level in fallback_levels:
            print(f"   å˜—è©¦ Level {level}...")
            
            concepts_dict = extract_concepts_for_global_search(query)
            core_concepts = concepts_dict.get('core', [])
            auxiliary_concepts = concepts_dict.get('auxiliary', [])
            school_keywords = ['åœ‹ç«‹è¯åˆå¤§å­¸', 'National United University', 'è¯å¤§', 'NUU']
            
            # æ”¾å¯¬æ¢ä»¶ï¼šåªè¦æ ¸å¿ƒæ¦‚å¿µåŒ¹é…å³å¯
            query_cypher = """
            MATCH (c:Community {level: $level})
            WHERE c.summary IS NOT NULL
            
            WITH c, $core_concepts as core_concepts
            WITH c, 
                SIZE([core IN core_concepts WHERE c.summary CONTAINS core]) as core_matches
            
            WHERE core_matches > 0
            
            RETURN 
                c.id as community_id,
                c.level as level,
                c.summary as summary,
                c.entity_count as entity_count,
                core_matches as core_relevance
            ORDER BY core_matches DESC, c.entity_count DESC
            LIMIT $limit
            """
            
            try:
                with self.triplet_driver.session(database="nuu-triplet") as session:
                    result = session.run(
                        query_cypher,
                        core_concepts=core_concepts,
                        level=level,
                        limit=max(3, limit // 2)  # æ¯å±¤ç´šå–è¼ƒå°‘æ•¸é‡
                    )
                    level_communities = [dict(record) for record in result]
                    
                    if level_communities:
                        print(f"      âœ… Level {level} æ‰¾åˆ° {len(level_communities)} å€‹ç¤¾ç¾¤")
                        all_communities.extend(level_communities)
                        
                        # å¦‚æœå·²ç¶“æ‰¾åˆ°è¶³å¤ æ•¸é‡ï¼Œåœæ­¢å›é€€
                        if len(all_communities) >= 3:
                            break
            except Exception as e:
                print(f"      âŒ Level {level} æŸ¥è©¢å¤±æ•—: {e}")
                continue
        
        # å»é‡ï¼ˆæ ¹æ“š community_idï¼‰
        seen_ids = set()
        unique_communities = []
        for comm in all_communities:
            if comm['community_id'] not in seen_ids:
                seen_ids.add(comm['community_id'])
                unique_communities.append(comm)
        
        print(f"âœ… å¤šå±¤ç´šå›é€€å®Œæˆï¼Œç¸½å…±æ‰¾åˆ° {len(unique_communities)} å€‹ç¤¾ç¾¤")
        return unique_communities[:limit]
    def _find_communities_without_school_filter(self, concepts: List[str], limit: int) -> List[dict]:
        """é™ç´šæŸ¥è©¢ï¼šåªåŒ¹é…æ¦‚å¿µï¼Œä¸éæ¿¾å­¸æ ¡"""
        query_cypher = """
        MATCH (c:Community)
        WHERE c.summary IS NOT NULL
        WITH c, $concepts as concepts
        WITH c, 
            SIZE([concept IN concepts WHERE c.summary CONTAINS concept]) as concept_matches
        WHERE concept_matches > 0
        RETURN 
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count,
            concept_matches as relevance
        ORDER BY relevance DESC, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query_cypher, concepts=concepts, limit=limit)
                communities = [dict(record) for record in result]
                print(f"âš ï¸ é™ç´šæŸ¥è©¢æ‰¾åˆ° {len(communities)} å€‹ç¤¾ç¾¤ï¼ˆæœªéæ¿¾å­¸æ ¡ï¼‰")
                return communities
        except Exception as e:
            print(f"âŒ é™ç´šæŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    def _find_communities_by_keywords(self, query: str, limit: int = 5) -> List[dict]:
        """ä½¿ç”¨é—œéµè©åŒ¹é…æ‰¾å‡ºç›¸é—œç¤¾ç¾¤(æ”¹é€²ç‰ˆ)"""
        
        # å¾å•é¡Œä¸­æå–å¤šå€‹å¯èƒ½çš„é—œéµè©
        words = query.replace('?', '').replace('?', '').split()
        keywords = [w for w in words if len(w) >= 2][:5]  # å–æœ€å¤š5å€‹é—œéµè©
        
        if not keywords:
            keywords = [query[:10]]  # å¦‚æœæ²’æœ‰é—œéµè©å°±ç”¨å‰10å€‹å­—
        
        print(f"ğŸ”‘ ä½¿ç”¨é—œéµè©: {keywords}")
        
        query_cypher = """
        MATCH (c:Community)
        WHERE c.summary IS NOT NULL
        WITH c, $keywords as keywords
        WITH c,
            SIZE([kw IN keywords WHERE c.summary CONTAINS kw]) as match_count
        WHERE match_count > 0
        RETURN 
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count,
            match_count as relevance
        ORDER BY relevance DESC, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query_cypher, keywords=keywords, limit=limit)
                communities = [dict(record) for record in result]
                
                if communities:
                    print(f"âœ… é—œéµè©åŒ¹é…æ‰¾åˆ° {len(communities)} å€‹ç¤¾ç¾¤")
                
                return communities
                
        except Exception as e:
            print(f"âŒ é—œéµè©åŒ¹é…å¤±æ•—: {e}")
            return []

    def _get_communities_for_entities(self, entities: List[str], limit: int = 3) -> List[dict]:
        """æ ¹æ“šå¯¦é«”ç²å–ç›¸é—œç¤¾ç¾¤æ‘˜è¦ï¼ˆç”¨æ–¼æ–‡æª”å¢å¼·ï¼‰"""
        if not entities:
            return []
        
        query = """
        UNWIND $entities AS entity_name
        MATCH (e:Entity)
        WHERE e.name CONTAINS entity_name OR e.name = entity_name
        WITH DISTINCT e.communityId as commId
        MATCH (c:Community {id: commId})
        WHERE c.summary IS NOT NULL
        RETURN DISTINCT
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count
        ORDER BY c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query, entities=entities, limit=limit)
                return [dict(record) for record in result]
        except Exception as e:
            print(f"âŒ ç²å–ç¤¾ç¾¤æ‘˜è¦å¤±æ•—: {e}")
            return []
            
    # æ­¥é©Ÿ1: å¾Weaviate ContentSummaryEn0913æœå°‹ï¼Œæ­¥é©Ÿ2: ç”¨neo4j_idå»Neo4jæŸ¥è©¢å®Œæ•´è³‡è¨Š
    def search(self, query: str, limit: int = 10) -> List[Document]:
        if not self.client or not self.embedder or not self.collection or not self.neo4j_driver:
            return [Document(page_content="âŒ ç³»çµ±é€£æ¥å¤±æ•—", metadata={})]
        
        try:
            # æ­¥é©Ÿ 1: Weaviate å‘é‡æœå°‹
            print(f"ğŸ” æ­¥é©Ÿ1: åœ¨ Weaviate ContentSummaryEn0913 ä¸­é€²è¡Œå‘é‡æœå°‹...")
            query_vector = self.embedder.embed_query(query)
            weaviate_results = self._weaviate_vector_search(query_vector, limit)
            
            if not weaviate_results:
                print("âŒ Weaviate æœå°‹ç„¡çµæœ")
                return [Document(page_content="æœå°‹ç„¡çµæœ", metadata={})]
            
            # æ­¥é©Ÿ 2: æå– neo4j_id ä¸¦æŸ¥è©¢ Neo4j
            neo4j_ids = []
            weaviate_data = {}
            
            for result in weaviate_results:
                neo4j_id = result['neo4j_id']
                if neo4j_id:
                    neo4j_ids.append(neo4j_id)
                    weaviate_data[neo4j_id] = result
            
            if not neo4j_ids:
                print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ neo4j_id")
                return [Document(page_content="æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„Neo4jè³‡æ–™", metadata={})]
            
            print(f"ğŸ” æ­¥é©Ÿ2: ä½¿ç”¨ {len(neo4j_ids)} å€‹ neo4j_id æŸ¥è©¢ Neo4j...")
            enhanced_documents = self._query_neo4j_by_ids(neo4j_ids, weaviate_data)
            
            print(f"âœ… ç¸½å…±ç”Ÿæˆ {len(enhanced_documents)} ç­†å¢å¼·æ–‡æª”")
            return enhanced_documents
            
        except Exception as e:
            print(f"âŒ æœå°‹éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return [Document(page_content="æœå°‹å¤±æ•—", metadata={})]
        
    # å¾ Weaviate ContentSummaryEn0913 åŸ·è¡Œå‘é‡æœå°‹ï¼Œè¿”å›ç›¸é—œåº¦æœ€é«˜çš„çµæœ
    def _weaviate_vector_search(self, query_vector: List[float], limit: int) -> List[dict]:
        results = []
        
        try:
            response = self.collection.query.near_vector(
                near_vector=query_vector,
                limit=limit,
                return_metadata=wvc.query.MetadataQuery(distance=True)
            )
            
            for obj in response.objects:
                # ç²å– english_summary å…§å®¹
                content = obj.properties.get(self.text_field, '')
                if not content or content.strip() == "":
                    continue
                
                # è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
                distance = float(obj.metadata.distance) if obj.metadata.distance is not None else 1.0
                similarity = max(0, 1.0 - distance)
                
                # æ”¶é›†çµæœ
                result = {
                    'weaviate_uuid': str(obj.uuid),
                    'english_summary': content,
                    'neo4j_id': obj.properties.get('neo4j_id', ''),
                    'article_url': obj.properties.get('article_url', ''),
                    'language': obj.properties.get('language', 'en'),
                    'similarity': similarity,
                    'distance': distance
                }
                results.append(result)
            
            print(f"âœ… Weaviate æœå°‹æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ç­†çµæœ")
            
            # é¡¯ç¤ºå‰3ç­†çµæœçš„é è¦½
            for i, result in enumerate(results[:3], 1):
                preview = result['english_summary'][:100] + "..." if len(result['english_summary']) > 100 else result['english_summary']
                print(f"  [{i}] ç›¸ä¼¼åº¦: {result['similarity']:.3f} | Neo4j ID: {result['neo4j_id']} | {preview}")
            
            return results
            
        except Exception as e:
            print(f"âŒ Weaviate å‘é‡æœå°‹å¤±æ•—: {e}")
            return []
    
    # å„ªåŒ–ç‰ˆæœ¬ï¼šåˆ†éšæ®µæŸ¥è©¢ï¼Œé¿å…è¤‡é›œçš„ collect æ“ä½œ
    def _query_neo4j_by_ids(self, neo4j_ids: List[str], weaviate_data: dict) -> List[Document]:
        if not self.neo4j_driver or not neo4j_ids:
            return []
        
        enhanced_documents = []
        

        for neo4j_id in neo4j_ids:
            print(f"ğŸ” æŸ¥è©¢ Neo4j ID: {neo4j_id}")
            start_time = time.time()
            
            try:
                # åˆ†éšæ®µæŸ¥è©¢ç­–ç•¥
                content_data = self._get_content_info(neo4j_id)
                if not content_data:
                    print(f"âš ï¸ Content è³‡æ–™ä¸å­˜åœ¨: {neo4j_id}")
                    continue
                
                article_data = self._get_article_info(neo4j_id)
                org_data = self._get_organization_info(neo4j_id)
                
                # ã€ä¿®æ”¹ã€‘åŠ å…¥ä¸‰å…ƒçµ„åœ–è­œå¢å¼·
                triplet_data = self._enhance_with_triplet_graph(
                    neo4j_id, 
                    content_data.get('content_text', ''), 
                    article_data.get('article_title', '')
                )

                # åˆä½µè³‡æ–™
                combined_data = {**content_data, **article_data, **org_data, **triplet_data}
                
                # å»ºç«‹å¢å¼·æ–‡æª”
                weaviate_info = weaviate_data.get(neo4j_id, {})
                enhanced_doc = self._create_enhanced_document_optimized(combined_data, weaviate_info)
                enhanced_documents.append(enhanced_doc)
                
                elapsed = time.time() - start_time
                print(f"âœ… æŸ¥è©¢å®Œæˆ ({elapsed:.2f}s)")
                
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âŒ æŸ¥è©¢å¤±æ•— ({elapsed:.2f}s): {e}")
                
                # ä½¿ç”¨ Weaviate å‚™æ¡ˆ
                weaviate_info = weaviate_data.get(neo4j_id, {})
                if weaviate_info:
                    fallback_doc = Document(
                        page_content=weaviate_info.get('english_summary', ''),
                        metadata={
                            'source': 'Weaviate_fallback',
                            'neo4j_id': neo4j_id,
                            'similarity': weaviate_info.get('similarity', 0),
                            'enhanced': False
                        }
                    )
                    enhanced_documents.append(fallback_doc)
        
        return enhanced_documents
    
    # ç²å– Content åŸºæœ¬è³‡è¨Š
    def _get_content_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        RETURN 
            content.neo4j_id as content_neo4j_id,
            content.id as content_id,
            content.text as content_text,
            content.type as content_type,
            content.order as content_order
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Content æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å– Article è³‡è¨Š
    def _get_article_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        RETURN 
            article.neo4j_id as article_neo4j_id,
            article.url as article_url,
            article.title as article_title,
            article.domain as article_domain,
            article.og_image as article_og_image
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Article æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å–çµ„ç¹”è³‡è¨Š - ç°¡åŒ–ç‰ˆæœ¬
    def _get_organization_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        OPTIONAL MATCH (article)-[:BELONGS_TO]->(org:Organization)
        RETURN 
            org.name as org_name,
            org.type as org_type,
            org.unified_number as org_unified_number
        LIMIT 1
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Organization æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ã€æ–°å¢ã€‘é€šéä¸‰å…ƒçµ„åœ–è­œå¢å¼·è³‡æ–™
    def _enhance_with_triplet_graph(self, neo4j_id: str, content_text: str, article_title: str) -> dict:
        """é€šéä¸‰å…ƒçµ„åœ–è­œå¢å¼·è³‡æ–™ï¼Œåªå–10ç­†æ¸¬è©¦"""
        if not self.triplet_driver:
            return {'triplet_relations': [], 'entity_count': 0}
        
        try:
            # ä½¿ç”¨ Mistral æå–å¯¦é«”
            text_for_extraction = ""
            if article_title:
                text_for_extraction += article_title + " "
            if content_text:
                text_for_extraction += content_text[:200]  # åªå–å‰200å­—é¿å…å¤ªé•·
            
            if not text_for_extraction.strip():
                return {'triplet_relations': [], 'entity_count': 0}
            
            # æå–å¯¦é«”
            entities = extract_entities_with_mistral(text_for_extraction)
            
            if not entities:
                return {'triplet_relations': [], 'entity_count': 0}
            
            # åœ¨ä¸‰å…ƒçµ„åœ–è­œä¸­æŸ¥è©¢é—œä¿‚ï¼ˆé™åˆ¶10ç­†ï¼‰
            triplet_relations = self._query_triplet_relationships_limited(entities)
            
            
             # ã€æ–°å¢ã€‘æŸ¥è©¢ç›¸é—œç¤¾ç¾¤æ‘˜è¦
            community_summaries = self._get_communities_for_entities(entities)
            
            print(f"ğŸ”— ä¸‰å…ƒçµ„å¢å¼·å®Œæˆ: {len(entities)} å€‹å¯¦é«”ï¼Œ{len(triplet_relations)} å€‹é—œä¿‚")
            
            return {
                'triplet_relations': triplet_relations,
                'entity_count': len(entities),
                'extracted_entities': entities,
                'community_summaries': community_summaries  
            }
            
        except Exception as e:
            print(f"âŒ ä¸‰å…ƒçµ„åœ–è­œå¢å¼·å¤±æ•—: {e}")
            return {'triplet_relations': [], 'entity_count': 0}

# ã€æ–°å¢ã€‘é™åˆ¶æŸ¥è©¢ä¸‰å…ƒçµ„é—œä¿‚ï¼ˆåªå–10ç­†ï¼‰
    def _query_triplet_relationships_limited(self, entities: List[str]) -> List[dict]:
        """åœ¨ä¸‰å…ƒçµ„åœ–è­œä¸­æŸ¥è©¢å¯¦é«”é—œä¿‚ï¼Œé™åˆ¶10ç­†"""
        if not entities:
            return []
        
        query = """
        UNWIND $entities AS entity_name
        MATCH (s)-[r]->(o)
        WHERE s.name CONTAINS entity_name 
        OR o.name CONTAINS entity_name
        OR s.name = entity_name 
        OR o.name = entity_name
        WITH s, r, o, entity_name
        WHERE type(r) IN ['ORGANIZATIONAL', 'SPATIAL', 'CONTACT', 'ACADEMIC', 
                        'FACILITY', 'SERVICE', 'PERSONNEL', 'TEMPORAL',
                        'COLLABORATION', 'INFORMATION', 'ADMINISTRATIVE', 'ATTRIBUTE']
        RETURN 
            s.name as subject,
            type(r) as relation_type,
            r.original_relation as original_relation,
            o.name as object,
            entity_name as matched_entity
        LIMIT 10
        """
        
        try:
            with self.triplet_driver.session() as session:
                result = session.run(query, entities=entities)
                relations = []
                
                for record in result:
                    relations.append({
                        'subject': record['subject'],
                        'relation_type': record['relation_type'], 
                        'original_relation': record['original_relation'],
                        'object': record['object'],
                        'matched_entity': record['matched_entity']
                    })
                
                return relations
                
        except Exception as e:
            print(f"âŒ æŸ¥è©¢ä¸‰å…ƒçµ„é—œä¿‚å¤±æ•—: {e}")
            return []
        
    # å»ºç«‹å„ªåŒ–ç‰ˆå¢å¼·æ–‡æª”
    def _create_enhanced_document_optimized(self, neo4j_data: dict, weaviate_info: dict) -> Document:
        try:
            content_parts = []
            
            # Weaviate è‹±æ–‡æ‘˜è¦
            base_content = weaviate_info.get('english_summary', '')
            if base_content:
                content_parts.append(base_content)
            
            # Neo4j Content åŸå§‹æ–‡æœ¬
            content_text = neo4j_data.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            # Article è³‡è¨Š
            article_title = neo4j_data.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
                
            article_url = neo4j_data.get('article_url')
            if article_url:
                content_parts.append(f"ã€æ–‡ç« ç¶²å€ã€‘\n{article_url}")
            
            # çµ„ç¹”è³‡è¨Š
            org_name = neo4j_data.get('org_name')
            if org_name:
                org_info = org_name
                org_type = neo4j_data.get('org_type')
                if org_type:
                    org_info += f" ({org_type})"
                content_parts.append(f"ã€æ‰€å±¬çµ„ç¹”ã€‘\n{org_info}")
            
              # ã€æ–°å¢ã€‘ä¸‰å…ƒçµ„åœ–è­œé—œä¿‚è³‡è¨Š
            triplet_relations = neo4j_data.get('triplet_relations', [])
            if triplet_relations:
                relation_text = "ã€çŸ¥è­˜åœ–è­œé—œä¿‚ã€‘\n"
                for i, rel in enumerate(triplet_relations, 1):
                    subject = rel.get('subject', '')
                    relation_type = rel.get('relation_type', '')
                    original_relation = rel.get('original_relation', '')
                    obj = rel.get('object', '')
                    
                    relation_line = f"{i}. {subject} --[{relation_type}]--> {obj}"
                    if original_relation and original_relation != relation_type:
                        relation_line += f" (åŸå§‹é—œä¿‚: {original_relation})"
                    
                    relation_text += relation_line + "\n"
                
                content_parts.append(relation_text)
            # ã€æ–°å¢ã€‘ç¤¾ç¾¤æ‘˜è¦è³‡è¨Š
            community_summaries = neo4j_data.get('community_summaries', [])
            if community_summaries:
                community_text = "ã€ç›¸é—œé ˜åŸŸèƒŒæ™¯ã€‘\n"
                for i, comm in enumerate(community_summaries, 1):
                    summary = comm.get('summary', '')
                    entity_count = comm.get('entity_count', 0)
                    community_text += f"{i}. {summary} (æ¶µè“‹ {entity_count} å€‹ç›¸é—œå¯¦é«”)\n\n"
                content_parts.append(community_text)
            
            # çµ„åˆå…§å®¹
            enhanced_content = "\n\n".join(content_parts) if content_parts else "ç„¡å…§å®¹"
            
            # å»ºç«‹ metadata
            metadata = {
                'source': 'Weaviate+Neo4j+GraphRAG',
                'neo4j_id': neo4j_data.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'enhanced': True,
                'graphrag_enhanced': len(triplet_relations) > 0,  # ã€æ–°å¢ã€‘
                'relation_count': len(triplet_relations),  # ã€æ–°å¢ã€‘
                'entity_count': neo4j_data.get('entity_count', 0),  # ã€æ–°å¢ã€‘
                 'community_count': len(community_summaries),
                'article_title': article_title or '',
                'article_url': neo4j_data.get('article_url', ''),
                'content_type': neo4j_data.get('content_type', ''),
                'organization': org_name or ''
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ æ–‡æª”å»ºç«‹å¤±æ•—: {e}")
            # è¿”å›åŸºç¤ Weaviate æ–‡æª”
            return Document(
                page_content=weaviate_info.get('english_summary', 'Error'),
                metadata={
                    'source': 'Error_fallback',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'enhanced': False,
                    'graphrag_enhanced': False
                }
            )
    
    # æ ¹æ“š Neo4j æŸ¥è©¢çµæœå’Œ Weaviate è³‡è¨Šå»ºç«‹å¢å¼·æ–‡æª”ï¼ˆä¿ç•™åŸå§‹ç‰ˆæœ¬ä½œç‚ºå‚™ç”¨ï¼‰
    def _create_enhanced_document(self, neo4j_record, weaviate_info: dict) -> Document:
        try:
            # åŸºç¤å…§å®¹å¾ Weaviate çš„ english_summary é–‹å§‹
            base_content = weaviate_info.get('english_summary', '')
            content_parts = [base_content] if base_content else []
            
            # æ·»åŠ  Neo4j Content çš„åŸå§‹æ–‡æœ¬
            content_text = neo4j_record.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            # æ·»åŠ  Article è³‡è¨Š
            article_title = neo4j_record.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
            
            article_domain = neo4j_record.get('article_domain')
            if article_domain:
                content_parts.append(f"ã€ç¶²ç«™ä¾†æºã€‘\n{article_domain}")
            
            # æ·»åŠ æ–‡ç« æ‘˜è¦
            article_summary = neo4j_record.get('article_summary')
            if article_summary:
                content_parts.append(f"ã€æ–‡ç« æ‘˜è¦ã€‘\n{article_summary}")
            
            # æ·»åŠ åœ–ç‰‡è³‡è¨Š
            article_og_image = neo4j_record.get('article_og_image')
            if article_og_image:
                content_parts.append(f"ã€æ–‡ç« åœ–ç‰‡ã€‘\n{article_og_image}")
            
            # æ·»åŠ çµ„ç¹”è³‡è¨Š
            organizations = neo4j_record.get('organizations', [])
            valid_organizations = [o for o in organizations if o.get('name')]
            if valid_organizations:
                org_lines = []
                for org in valid_organizations:
                    org_name = org.get('name', '')
                    org_type = org.get('type', '')
                    unified_number = org.get('unified_number', '')
                    
                    org_line = org_name
                    if org_type:
                        org_line += f" ({org_type})"
                    if unified_number:
                        org_line += f" [çµ±ç·¨: {unified_number}]"
                    org_lines.append(org_line)
                
                if org_lines:
                    content_parts.append(f"ã€æ‰€å±¬çµ„ç¹”ã€‘\n" + "\n".join(org_lines))
            
            # æ·»åŠ éƒ¨é–€è³‡è¨Š
            departments = neo4j_record.get('departments', [])
            valid_departments = [d for d in departments if d.get('name')]
            if valid_departments:
                dept_lines = []
                for dept in valid_departments:
                    dept_name = dept.get('name', '')
                    dept_type = dept.get('type', '')
                    dept_description = dept.get('description', '')
                    
                    dept_line = dept_name
                    if dept_type:
                        dept_line += f" ({dept_type})"
                    if dept_description:
                        dept_line += f" - {dept_description}"
                    dept_lines.append(dept_line)
                
                if dept_lines:
                    content_parts.append(f"ã€æ‰€å±¬éƒ¨é–€ã€‘\n" + "\n".join(dept_lines))
            
            # æ·»åŠ è¯çµ¡è³‡è¨Š
            contacts = neo4j_record.get('contacts', [])
            valid_contacts = [c for c in contacts if c.get('value')]
            if valid_contacts:
                contact_lines = []
                for contact in valid_contacts:
                    contact_type = contact.get('type', 'è¯çµ¡æ–¹å¼')
                    contact_value = contact.get('value', '')
                    contact_description = contact.get('description', '')
                    
                    contact_line = f"{contact_type}: {contact_value}"
                    if contact_description:
                        contact_line += f" ({contact_description})"
                    contact_lines.append(contact_line)
                
                if contact_lines:
                    content_parts.append(f"ã€è¯çµ¡è³‡è¨Šã€‘\n" + "\n".join(contact_lines))
            
            # æ·»åŠ åœ°å€è³‡è¨Š
            addresses = neo4j_record.get('addresses', [])
            valid_addresses = [a for a in addresses if a.get('full_address')]
            if valid_addresses:
                address_lines = []
                for addr in valid_addresses:
                    full_addr = addr.get('full_address', '')
                    campus_name = addr.get('campus_name', '')
                    city = addr.get('city', '')
                    district = addr.get('district', '')
                    postal_code = addr.get('postal_code', '')
                    
                    addr_line = full_addr
                    
                    # æ·»åŠ æ ¡å€åç¨±
                    if campus_name:
                        addr_line = f"{campus_name} - {addr_line}"
                    
                    # æ·»åŠ åŸå¸‚å€åŸŸè³‡è¨Š
                    location_info = []
                    if city and district:
                        location_info.append(f"{city}{district}")
                    elif city:
                        location_info.append(city)
                    
                    if postal_code:
                        location_info.append(f"éƒµéå€è™Ÿ: {postal_code}")
                    
                    if location_info:
                        addr_line += f" ({', '.join(location_info)})"
                    
                    address_lines.append(addr_line)
                
                if address_lines:
                    content_parts.append(f"ã€åœ°å€è³‡è¨Šã€‘\n" + "\n".join(address_lines))
            
            # æ·»åŠ ç›¸é—œå…§å®¹
            related_contents = neo4j_record.get('related_contents', [])
            valid_related = [r for r in related_contents if r.get('text')]
            if valid_related:
                related_lines = []
                for related in valid_related[:3]:  # åªé¡¯ç¤ºå‰3å€‹ç›¸é—œå…§å®¹
                    text = related.get('text', '')
                    content_type = related.get('type', 'æœªçŸ¥é¡å‹')
                    order = related.get('order', '')
                    
                    # é™åˆ¶ç›¸é—œå…§å®¹é•·åº¦
                    preview = text[:150] + "..." if len(text) > 150 else text
                    related_line = f"[{content_type}]"
                    if order:
                        related_line += f" (é †åº: {order})"
                    related_line += f" {preview}"
                    
                    related_lines.append(related_line)
                
                if related_lines:
                    content_parts.append(f"ã€ç›¸é—œå…§å®¹ã€‘\n" + "\n".join(related_lines))
            
            # çµ„åˆå®Œæ•´å…§å®¹
            enhanced_content = "\n\n".join(content_parts)
            
            # å»ºç«‹å®Œæ•´çš„ metadata
            metadata = {
                'source': 'Weaviate+Neo4j',
                'weaviate_uuid': weaviate_info.get('weaviate_uuid', ''),
                'neo4j_id': neo4j_record.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'distance': weaviate_info.get('distance', 1.0),
                'enhanced': True,
                
                # Content è³‡è¨Š
                'content_id': neo4j_record.get('content_id', ''),
                'content_type': neo4j_record.get('content_type', ''),
                'content_order': neo4j_record.get('content_order', ''),
                
                # Article è³‡è¨Š
                'article_title': article_title or '',
                'article_url': neo4j_record.get('article_url', ''),
                'article_domain': article_domain or '',
                'article_summary': article_summary or '',
                'article_og_image': article_og_image or '',
                'article_updated_at': neo4j_record.get('article_updated_at', ''),
                
                # çµ„ç¹”è³‡è¨Š
                'organizations': [org.get('name', '') for org in valid_organizations],
                'organization_types': [org.get('type', '') for org in valid_organizations],
                'unified_numbers': [org.get('unified_number', '') for org in valid_organizations],
                
                # éƒ¨é–€è³‡è¨Š
                'departments': [dept.get('name', '') for dept in valid_departments],
                'department_types': [dept.get('type', '') for dept in valid_departments],
                'department_descriptions': [dept.get('description', '') for dept in valid_departments],
                
                # è¯çµ¡è³‡è¨Š
                'contact_types': [c.get('type', '') for c in valid_contacts],
                'contact_values': [c.get('value', '') for c in valid_contacts],
                
                # åœ°å€è³‡è¨Š
                'campus_names': [a.get('campus_name', '') for a in valid_addresses],
                'cities': [a.get('city', '') for a in valid_addresses],
                'districts': [a.get('district', '') for a in valid_addresses],
                'postal_codes': [a.get('postal_code', '') for a in valid_addresses],
                
                # çµ±è¨ˆè³‡è¨Š
                'organization_count': len(valid_organizations),
                'contact_count': len(valid_contacts),
                'address_count': len(valid_addresses),
                'department_count': len(valid_departments),
                'related_content_count': len(valid_related)
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ å»ºç«‹å¢å¼·æ–‡æª”å¤±æ•—: {e}")
            # è¿”å›åŸºç¤æ–‡æª”
            return Document(
                page_content=weaviate_info.get('english_summary', 'å»ºç«‹æ–‡æª”å¤±æ•—'),
                metadata={
                    'source': 'Fallback',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'similarity': weaviate_info.get('similarity', 0),
                    'enhanced': False
                }
            )

# RAG ç³»çµ±ç‹€æ…‹å®šç¾©
class State(TypedDict):
    question: str
    original_question: str
    translated_question: str
    is_chinese_query: bool
    strategy: str  # æ–°å¢ï¼šè¨˜éŒ„æœå°‹ç­–ç•¥
    context: List[Document]
    answer: str
    related_links: List[Dict] 

# åˆå§‹åŒ– ContentSummaryEn0913 RAG
content_summary_rag = ContentSummaryEn0913RAG()

# è™•ç†å•é¡Œ - æª¢æŸ¥æ˜¯å¦ç‚ºä¸­æ–‡ä¸¦é€²è¡Œç¿»
def process_question(state: State):
    question = state["question"]
    original_question = question
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
    is_chinese_query = is_chinese(question)
    
    if is_chinese_query:
        print(f"ğŸˆ¶ æª¢æ¸¬åˆ°ä¸­æ–‡æŸ¥è©¢ï¼Œæº–å‚™ç¿»è­¯...")
        try:
            # ä½¿ç”¨ Mistral ç¿»è­¯ä¸­æ–‡ç‚ºè‹±æ–‡
            translated_question = translate_with_mistral(question)
            print(f"ğŸ”„ ç¿»è­¯çµæœ: {translated_question}")
        except Exception as e:
            print(f"âš ï¸ ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ: {e}")
            translated_question = question
            is_chinese_query = False
    else:
        print(f"ğŸ”¤ æª¢æ¸¬åˆ°è‹±æ–‡æŸ¥è©¢ï¼Œç›´æ¥ä½¿ç”¨")
        translated_question = question
    
    return {
        "question": translated_question,  # ç”¨æ–¼æª¢ç´¢çš„å•é¡Œï¼ˆè‹±æ–‡ï¼‰
        "original_question": original_question,  # åŸå§‹å•é¡Œ
        "translated_question": translated_question,  # ç¿»è­¯å¾Œçš„å•é¡Œ
        "is_chinese_query": is_chinese_query  # æ˜¯å¦ç‚ºä¸­æ–‡æŸ¥è©¢
    }

# æª¢ç´¢ç›¸é—œæ–‡æª” - Weaviate å‘é‡æœå°‹ + Neo4j è³‡æ–™å¢å¼·
def retrieve(state: State):
    question = state["question"]
    
    # ä½¿ç”¨ Mistral åˆ¤æ–·ç­–ç•¥
    print(f"\nğŸ¤” ä½¿ç”¨ Mistral åˆ¤æ–·æœå°‹ç­–ç•¥...")
    strategy = choose_search_strategy_with_mistral(state["original_question"])
    print(f"ğŸ¯ Mistral åˆ¤æ–·çµæœ: {strategy.upper()}")
    
    if strategy == "global":
        # === Global Search: ä½¿ç”¨æ™ºèƒ½å±¤ç´šé¸æ“‡ ===
        print("ğŸŒ åŸ·è¡Œ Global Searchï¼ˆæ™ºèƒ½å±¤ç´šé¸æ“‡ï¼‰")
        
        # è‡ªå‹•é¸æ“‡æœ€ä½³å±¤ç´šä¸¦æª¢ç´¢
        communities = content_summary_rag.find_relevant_communities(
            query=question,
            limit=None,   # è‡ªå‹•æ±ºå®š
            level=None    # è‡ªå‹•é¸æ“‡
        )
        
        if not communities:
            print("âš ï¸ Global Search ç„¡çµæœï¼Œè‡ªå‹•åˆ‡æ›åˆ° Local Search")
            strategy = "local"
        else:
            print(f"âœ… æ‰¾åˆ° {len(communities)} å€‹ç›¸é—œç¤¾ç¾¤")
            community_docs = []
            for comm in communities:
                doc = Document(
                    page_content=comm['summary'],
                    metadata={
                        'source': 'Community',
                        'strategy': 'global',
                        'community_id': comm['community_id'],
                        'level': comm['level'],
                        'entity_count': comm['entity_count'],
                        'core_relevance': comm.get('core_relevance', 0),
                        'aux_relevance': comm.get('aux_relevance', 0)
                    }
                )
                community_docs.append(doc)
            
            return {"context": community_docs, "strategy": strategy}
    
    # === Local Search: ä½¿ç”¨å‘é‡æª¢ç´¢ + åœ–è­œå¢å¼· ===
    print("ğŸ“ åŸ·è¡Œ Local Search (å‘é‡æª¢ç´¢ + åœ–è­œå¢å¼·)")
    documents = content_summary_rag.search(question, limit=10)
    
    # æ¨™è¨˜ç­–ç•¥
    for doc in documents:
        doc.metadata['strategy'] = 'local'
    
    print(f"\nğŸ“Š æª¢ç´¢çµæœçµ±è¨ˆ:")
    print(f"ğŸ” ç¸½å…±æª¢ç´¢åˆ° {len(documents)} ç­†ç›¸é—œè³‡æ–™")
    
    enhanced_count = sum(1 for doc in documents if doc.metadata.get('enhanced', False))
    print(f"âœ¨ å…¶ä¸­ {enhanced_count} ç­†å·²é€šé Neo4j å¢å¼·")
    
    return {"context": documents, "strategy": strategy}

import time

def generate_global(state: State):
    """æ”¹é€²ç‰ˆ Global Search - åŒ…å«ç¤¾ç¾¤æ‘˜è¦çš„å±•ç¤ºèˆ‡é€£çµæ”¶é›†"""
    communities = state["context"]
    
    # === ç¬¬ä¸€éšæ®µï¼šå±•ç¤ºç¤¾ç¾¤æ‘˜è¦ ===
    print(f"\nğŸ“š ç¬¬ä¸€éšæ®µï¼šå±•ç¤º {len(communities)} å€‹ç›¸é—œç¤¾ç¾¤æ‘˜è¦...")
    
    summaries_text = ""
    for i, comm_doc in enumerate(communities, 1):
        comm_id = comm_doc.metadata.get('community_id')
        entity_count = comm_doc.metadata.get('entity_count', 0)
        core_relevance = comm_doc.metadata.get('core_relevance', 0)
        
        summaries_text += f"\nã€ç¤¾ç¾¤ {i} - ID: {comm_id}ã€‘(æ¶µè“‹ {entity_count} å€‹å¯¦é«”ï¼Œæ ¸å¿ƒåŒ¹é…åº¦: {core_relevance})\n"
        summaries_text += f"{comm_doc.page_content}\n"
        summaries_text += "-" * 40 + "\n"
    
    print(f"âœ… ç¤¾ç¾¤æ‘˜è¦æ”¶é›†å®Œæˆ")
    
    # === ç¬¬äºŒéšæ®µï¼šMap - æ¯å€‹ç¤¾ç¾¤ç¨ç«‹ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆ ===
    print(f"\nğŸ—ºï¸  Map éšæ®µï¼šç‚º {len(communities)} å€‹ç¤¾ç¾¤ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆ...")
    partial_answers = []
    
    for i, comm_doc in enumerate(communities, 1):
        comm_id = comm_doc.metadata.get('community_id')
        print(f"   è™•ç†ç¤¾ç¾¤ {i}/{len(communities)} (ID: {comm_id})")
        
        if state["is_chinese_query"]:
            map_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¤¾ç¾¤æ‘˜è¦ï¼Œé‡å°ç”¨æˆ¶å•é¡Œæä¾›ç›¸é—œè³‡è¨Šã€‚

ç¤¾ç¾¤æ‘˜è¦ï¼š
{comm_doc.page_content}

ç”¨æˆ¶å•é¡Œï¼š{state["original_question"]}

è«‹åªä½¿ç”¨é€™å€‹ç¤¾ç¾¤çš„è³‡è¨Šå›ç­”ã€‚å¦‚æœé€™å€‹ç¤¾ç¾¤èˆ‡å•é¡Œç„¡é—œï¼Œè«‹å›ç­”ã€Œæ­¤ç¤¾ç¾¤ç„¡ç›¸é—œè³‡è¨Šã€ã€‚
è«‹ä½¿ç”¨æ¢åˆ—å¼æˆ–æ®µè½å¼æ¸…æ™°çµ„ç¹”ç­”æ¡ˆã€‚

éƒ¨åˆ†ç­”æ¡ˆï¼š"""
        else:
            map_prompt = f"""Please provide relevant information based on the following community summary to answer the user's question.

Community Summary:
{comm_doc.page_content}

User Question: {state["original_question"]}

Only use information from this community. If not relevant, answer "No relevant information."
Please organize answer clearly with bullet points or paragraphs.

Partial Answer:"""
        
        try:
            response = call_mistral(map_prompt)
            partial_answer = response["choices"][0]["message"]["content"].strip()
            
            partial_answers.append({
                'community_id': comm_id,
                'entity_count': comm_doc.metadata.get('entity_count'),
                'core_relevance': comm_doc.metadata.get('core_relevance', 0),
                'answer': partial_answer
            })
            print(f"      âœ… å®Œæˆ")
            
            if i < len(communities):
                time.sleep(0.5)
                
        except Exception as e:
            print(f"      âŒ å¤±æ•—: {e}")
            continue
    
    if not partial_answers:
        return {
            "answer": "æŠ±æ­‰ï¼Œç„¡æ³•å¾ç¤¾ç¾¤è³‡æ–™ä¸­ç”Ÿæˆç­”æ¡ˆã€‚è«‹å˜—è©¦æ›´å…·é«”çš„å•é¡Œã€‚",
            "related_links": []
        }
    
    # === ç¬¬ä¸‰éšæ®µï¼šReduce - åˆä½µç­”æ¡ˆ ===
    print(f"\nğŸ”€ Reduce éšæ®µï¼šç¶œåˆ {len(partial_answers)} å€‹ç¤¾ç¾¤çš„ç­”æ¡ˆ...")
    
    combined_text = "å„ç¤¾ç¾¤æä¾›çš„è³‡è¨Šï¼š\n\n"
    for i, pa in enumerate(partial_answers, 1):
        combined_text += f"ã€ç¤¾ç¾¤ {pa['community_id']} (æ¶µè“‹ {pa['entity_count']} å€‹å¯¦é«”)ã€‘\n"
        combined_text += f"{pa['answer']}\n\n"
    
    if state["is_chinese_query"]:
        reduce_prompt = f"""ä½ æ˜¯åœ‹ç«‹è¯åˆå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯å¾ä¸åŒçŸ¥è­˜ç¤¾ç¾¤ç²å¾—çš„è³‡è¨Šï¼Œè«‹ç¶œåˆé€™äº›è³‡è¨Šæä¾›å®Œæ•´ç­”æ¡ˆã€‚

ç”¨æˆ¶å•é¡Œï¼š{state["original_question"]}

åŸå§‹ç¤¾ç¾¤æ‘˜è¦ï¼š
{summaries_text}

å„ç¤¾ç¾¤æä¾›çš„éƒ¨åˆ†ç­”æ¡ˆï¼š
{combined_text}

è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šæä¾›ä¸€å€‹å®Œæ•´ã€æº–ç¢ºçš„ç­”æ¡ˆï¼š
1. æ•´åˆæ‰€æœ‰ç›¸é—œè³‡è¨Š
2. å»é™¤é‡è¤‡å…§å®¹
3. ç”¨æ¸…æ™°çš„çµæ§‹çµ„ç¹”ç­”æ¡ˆ
4. å¿½ç•¥æ¨™è¨»ã€Œç„¡ç›¸é—œè³‡è¨Šã€çš„éƒ¨åˆ†
5. å¦‚æœæ‰€æœ‰ç¤¾ç¾¤éƒ½ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦å‘ŠçŸ¥
6. åœ¨æœ€å¾ŒåŠ ä¸Šã€Œè³‡è¨Šä¾†æºã€æ®µè½ï¼Œç°¡è¿°æ¶‰åŠçš„ç›¸é—œç¤¾ç¾¤
7. ç›´æ¥å›ç­”ç”¨æˆ¶å•é¡Œ
8. å¦‚æœå•é¡Œèˆ‡è³‡æ–™ç„¡é—œ ï¼Œè«‹ä¸è¦äº‚å›ç­”ï¼Œèª å¯¦å‘ŠçŸ¥

æœ€çµ‚ç­”æ¡ˆï¼š"""
    else:
        reduce_prompt = f"""You are an intelligent assistant for National United University. Below are community summaries and partial answers.

User Question: {state["original_question"]}

Original Community Summaries:
{summaries_text}

Partial Answers from Communities:
{combined_text}

Please provide a complete answer:
1. Integrate all relevant information
2. Remove duplicate content
3. Organize with clear structure
4. Ignore "No relevant information" parts
5. If all communities lack info, be honest
6. Add "Information Sources" section at the end mentioning relevant communities
7. Answer the user's question directly

Final Answer:"""
    
    try:
        response = call_mistral(reduce_prompt)
        final_answer = response["choices"][0]["message"]["content"].strip()
        
        if final_answer.startswith('"') and final_answer.endswith('"'):
            final_answer = final_answer[1:-1]
        
        # ===== æ–°å¢ï¼šGlobal Search ä¹Ÿæ”¶é›†é€£çµï¼ˆå¾ç¤¾ç¾¤æ‘˜è¦ä¸­ï¼‰ =====
        related_links = []
        seen_urls = set()
        
        for comm_doc in communities:
            # å¦‚æœç¤¾ç¾¤æ‘˜è¦ä¸­åŒ…å« URLï¼ˆè¦–è³‡æ–™çµæ§‹è€Œå®šï¼‰
            article_url = comm_doc.metadata.get('article_url', '')
            article_title = comm_doc.metadata.get('article_title', '')
            
            if article_url and article_url not in seen_urls:
                seen_urls.add(article_url)
                related_links.append({
                    'url': article_url,
                    'title': article_title or f"ç¤¾ç¾¤ {comm_doc.metadata.get('community_id')}",
                    'source': 'Community',
                    'community_id': comm_doc.metadata.get('community_id'),
                    'entity_count': comm_doc.metadata.get('entity_count', 0)
                })
        
        related_links = related_links[:10]
        # ==========================================================
        
        return {
            "answer": final_answer,
            "related_links": related_links
        }
        
    except Exception as e:
        print(f"âŒ Reduce éšæ®µå¤±æ•—: {e}")
        fallback_answer = "æ ¹æ“šå¤šå€‹ç›¸é—œç¤¾ç¾¤çš„è³‡è¨Šï¼š\n\n" + combined_text
        return {
            "answer": fallback_answer,
            "related_links": []
        }


def generate_local(state: State):
    """Local Search çš„ç›´æ¥ç”Ÿæˆï¼ˆä½¿ç”¨åŸæœ¬çš„é‚è¼¯ï¼‰"""  
    # ===== æ–°å¢ï¼šæ”¶é›† related_links =====
    related_links = []
    seen_urls = set()
    
    for doc in state["context"]:
        article_url = doc.metadata.get('article_url', '')
        article_title = doc.metadata.get('article_title', '')
        
        if article_url and article_url not in seen_urls:
            seen_urls.add(article_url)
            related_links.append({
                'url': article_url,
                'title': article_title or 'ç›¸é—œæ–‡ç« ',
                'source': doc.metadata.get('source', 'GraphRAG'),
                'similarity': doc.metadata.get('similarity', 0),
                'enhanced': doc.metadata.get('enhanced', False),
                'graphrag_enhanced': doc.metadata.get('graphrag_enhanced', False)
            })
    
    related_links = related_links[:10]
    # =====================================
    # æ•´ç†æª¢ç´¢å…§å®¹
    context_text = ""
    for i, doc in enumerate(state["context"], 1):
        similarity = doc.metadata.get('similarity', 0)
        enhanced = doc.metadata.get('enhanced', False)
        content = doc.page_content
        article_url = doc.metadata.get('article_url', '')
        article_title = doc.metadata.get('article_title', '')
        
        context_text += f"[è³‡æ–™ {i}] ä¾†æº: {'Weaviate+Neo4jå¢å¼·' if enhanced else 'WeaviateåŸºç¤'} (ç›¸ä¼¼åº¦: {similarity:.3f})\n"
        if article_title:
            context_text += f"æ–‡ç« æ¨™é¡Œ: {article_title}\n"
        if article_url:
            context_text += f"æ–‡ç« ç¶²å€: {article_url}\n"
        context_text += f"{content}\n\n"
    
    # æ ¹æ“šæ˜¯å¦ç‚ºä¸­æ–‡æŸ¥è©¢é¸æ“‡ä¸åŒçš„æç¤ºæ¨¡æ¿
    if state["is_chinese_query"]:
        template = """ä½ æ˜¯åœ‹ç«‹è¯åˆå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æª¢ç´¢è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚

        ## æª¢ç´¢èˆ‡å¢å¼·è³‡æ–™ï¼š
        {context}

        ## ç”¨æˆ¶å•é¡Œï¼š
        {original_question}

        ## å›ç­”æŒ‡å¼•ï¼š
        1. è«‹ç”¨ä¸­æ–‡å›ç­”
        2. å„ªå…ˆä½¿ç”¨å¢å¼·å¾Œçš„å®Œæ•´è³‡æ–™
        3. å¦‚æœæœ‰è¯çµ¡è³‡è¨Šã€åœ°å€ç­‰ï¼Œè«‹æ˜ç¢ºåˆ—å‡º
        4. æ¢ç†æ¸…æ™°ï¼Œé‡è¦è³‡è¨Šç”¨é …ç›®ç¬¦è™Ÿ
        5. å¦‚æœ‰ç›¸é—œç¶²å€è«‹æä¾›
        6. ä¸è¦åœ¨å›ç­”ä¸­åŠ å…¥è³‡æ–™ç·¨è™Ÿå¼•ç”¨
        7. ç›´æ¥ä½¿ç”¨è³‡æ–™å…§å®¹ï¼Œä¸éœ€æ¨™è¨»ä¾†æºç·¨è™Ÿ
        8. å¦‚æœè³‡æ–™è·Ÿå•é¡Œç„¡é—œï¼Œè«‹ä¸è¦äº‚ç·¨ç­”æ¡ˆï¼Œèª å¯¦å‘ŠçŸ¥

        å›ç­”ï¼š"""
    else:
         template = """You are an intelligent assistant for National United University. Please answer based on the retrieved data.

        ## Retrieved and Enhanced Data:
        {context}

        ## User Question:
        {original_question}

        ## Guidelines:
        1. Answer in English
        2. Prioritize enhanced comprehensive data
        3. Provide specific contact information and addresses if available
        4. Well-structured with bullet points for important information
        5. Include relevant URLs if available
        6. Do not include data source numbers
        7. Use data content directly without source citations

        Answer:"""
    
    prompt = PromptTemplate.from_template(template)
    messages = prompt.invoke({
        "context": context_text,
        "original_question": state["original_question"]
    })
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ç”Ÿæˆå›ç­”
def generate(state: State):
    """æ ¹æ“šç­–ç•¥é¸æ“‡å°æ‡‰çš„ç”Ÿæˆæ–¹æ³•"""
    strategy = state.get("strategy", "local")
    print(f"\nğŸ’¡ ä½¿ç”¨ {strategy.upper()} ç­–ç•¥ç”Ÿæˆç­”æ¡ˆ...")
    if strategy == "global":
        return generate_global(state)
    else:
        return generate_local(state)
    
# å»ºç«‹ RAG æµç¨‹
graph_builder = StateGraph(State).add_sequence([process_question, retrieve, generate])
graph_builder.add_edge(START, "process_question")
graph = graph_builder.compile()

# RAG å•ç­”ä¸»å‡½æ•¸
def ask_question(question: str) -> tuple:  # ä¿®æ”¹å›å‚³é¡å‹
    """
    å›å‚³ï¼š(answer: str, related_links: List[Dict])
    """
    start_time = time.time()
    result = graph.invoke({"question": question})
    end_time = time.time()
    
    print(f"â±ï¸ ç¸½è™•ç†æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’")
    
    # å›å‚³ç­”æ¡ˆå’Œé€£çµ
    return result["answer"], result.get("related_links", [])

# ä¸»ç¨‹å¼åŸ·è¡Œ
if __name__ == "__main__":
    print(f"\n{'='*60}")
    print(f"ğŸ“ åœ‹ç«‹è¯åˆå¤§å­¸ ContentSummaryEn0913 + Neo4j åœ–è­œå¢å¼· RAG å•ç­”ç³»çµ±")
    print(f"{'='*60}")
    print(f"ğŸ“Š ç³»çµ±ç‹€æ…‹: {'âœ… æ­£å¸¸é‹è¡Œ' if content_summary_rag.client and content_summary_rag.neo4j_driver else 'âŒ é€£æ¥å¤±æ•—'}")
    print(f"ğŸ”§ æ¶æ§‹: å•é¡Œè™•ç†(ç¿»è­¯) â†’ å‘é‡æª¢ç´¢ â†’ Neo4jåœ–è­œå¢å¼· â†’ ç”Ÿæˆ")
    print(f"ğŸ“š è³‡æ–™ä¾†æº: ContentSummaryEn0913 é›†åˆ (è‹±æ–‡æ‘˜è¦) + Neo4j åœ–è­œè³‡æ–™")
    print(f"ğŸ§  åµŒå…¥æ¨¡å‹: Gemini embedding-001")
    print(f"ğŸ” æœå°‹æ–¹æ³•: å‘é‡ç›¸ä¼¼åº¦æœå°‹ + åœ–è­œé—œä¿‚å¢å¼·")
    print(f"ğŸŒ ç¿»è­¯åŠŸèƒ½: ä¸­æ–‡å•é¡Œè‡ªå‹•ç¿»è­¯ç‚ºè‹±æ–‡é€²è¡Œæª¢ç´¢")
    print(f"ğŸ’¬ å›ç­”èªè¨€: æ ¹æ“šå•é¡Œèªè¨€è‡ªå‹•åˆ¤æ–·ï¼ˆä¸­æ–‡å•é¡Œç”¨ä¸­æ–‡å›ç­”ï¼Œè‹±æ–‡å•é¡Œç”¨è‹±æ–‡å›ç­”ï¼‰")
    print(f"{'='*60}\n")
    

    
    # æª¢æŸ¥ Mistral API Key
    if not MISTRAL_API_KEY:
        print("âš ï¸  è­¦å‘Š: æœªè¨­å®š MISTRAL_API_KEYï¼Œä¸­æ–‡ç¿»è­¯åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
    else:
        print("âœ… Mistral API å·²è¨­å®šï¼Œæ”¯æ´ä¸­æ–‡ç¿»è­¯åŠŸèƒ½")
    
    # æª¢æŸ¥ç¿»è­¯æ˜ å°„æª”æ¡ˆ
    try:
        mapping_count = len(translation_manager.translation_dict)
        print(f"âœ… ç¿»è­¯æ˜ å°„æª”æ¡ˆå·²è¼‰å…¥ï¼ŒåŒ…å« {mapping_count} æ¢ç¿»è­¯å°æ‡‰")
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: ç¿»è­¯æ˜ å°„æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    
    # æª¢æŸ¥ç³»çµ±é€£æ¥ç‹€æ…‹
    weaviate_status = "âœ… æ­£å¸¸" if content_summary_rag.client else "âŒ å¤±æ•—"
    neo4j_status = "âœ… æ­£å¸¸" if content_summary_rag.neo4j_driver else "âŒ å¤±æ•—"
    print(f"ğŸ”— Weaviate é€£æ¥: {weaviate_status}")
    print(f"ğŸ”— Neo4j é€£æ¥: {neo4j_status}")
    
    # äº’å‹•å¼å•ç­”
    try:
        while True:
            print("-" * 50)
            question = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (ä¸­æ–‡æˆ–è‹±æ–‡ï¼Œè¼¸å…¥ 'exit' é€€å‡º): ")
            
            if question.lower() in ['exit', 'quit', 'é€€å‡º', 'é›¢é–‹']:
                print("è¬è¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                break
            
            if not question.strip():
                continue
                
            print(f"\nğŸ¤– æ­£åœ¨è™•ç†...")
            try:
                answer = ask_question(question)
                print(f"\nğŸ’¡ å›ç­”:\n{answer}\n")
                        
            except Exception as e:
                print(f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                traceback.print_exc()
                
    except KeyboardInterrupt:
        print(f"\n\nç¨‹å¼ä¸­æ–·ï¼Œè¬è¬ä½¿ç”¨ï¼")