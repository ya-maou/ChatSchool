#æ–°data0804 - ContentSummaryEn0913 é›†åˆ (è‹±æ–‡æ‘˜è¦) + Neo4j é—œéµå­—æœå°‹ æ–°data0804
#é‚„æ²’æ›´æ–° æ–°çš„æ–‡å­—è½‰å‘é‡
from typing import List, TypedDict, Dict, Optional
from langchain_core.documents.base import Document
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import PromptTemplate
from langgraph.graph import START, StateGraph
import weaviate
from weaviate import connect_to_local
import weaviate.classes as wvc
import google.generativeai as genai
from neo4j import GraphDatabase
from pydantic import BaseModel
import os
import dotenv
import time
import re
import requests
import traceback
import json


# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
dotenv.load_dotenv()

# è¨­å®š API Keys
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# è¨­ç½® LM Studio è³‡è¨Š
LMSTUDIO_MODEL = "google/gemma-3-4B" 
LMSTUDIO_URL = "http://192.168.98.34:1234/v1" 

genai.configure(api_key=GOOGLE_API_KEY)

# Setup LLM
from langchain.chat_models import init_chat_model

def init_LMStudio(model: str, base_url: str, api_key: str = ".", configurable_fields: None = None, config_prefix: str | None = None, **kwargs) -> BaseChatModel:
    """ä½¿ç”¨ LangChain é€£æ¥è‡³ LM Studio çš„ OpenAI ç›¸å®¹ API"""
    return init_chat_model(model=model, base_url=base_url, configurable_fields=configurable_fields, config_prefix=config_prefix, model_provider="openai", api_key=api_key, **kwargs)

try:
    # å˜—è©¦é€£æ¥ LM Studio ä½œç‚ºä¸»è¦æ¨¡å‹
    llm = init_LMStudio(model=LMSTUDIO_MODEL, base_url=LMSTUDIO_URL)
    print("âœ… æˆåŠŸé€£æ¥ LM Studio (google/gemma-3-4B)")
except Exception as e:
    print(f"[LMStudio åˆå§‹åŒ–å¤±æ•—] ç„¡æ³•é€£ç·šè‡³ {LMSTUDIO_URL}ã€‚éŒ¯èª¤: {e}")
    # è‹¥å¤±æ•—ï¼Œå›é€€åˆ° Gemini ä½œç‚ºå‚™ç”¨æ¨¡å‹
    llm = init_chat_model(model="gemini-2.0-flash", model_provider="google_genai")
    print("âš ï¸ ä½¿ç”¨ Gemini ä½œç‚ºå‚™ç”¨æ¨¡å‹")

#ç¿»è­¯ç®¡ç†å™¨ - æ”¯æ´å¤–éƒ¨ç¿»è­¯æª”æ¡ˆ
class TranslationManager:
    
    def __init__(self, translation_file_path=r"C:\Users\User\Studio\ChatSchool\CS_Project\CS_App\translate\translation_mapping.json"):
        self.translation_file = translation_file_path
        self.translation_dict = self._load_translation_dict()
    
    def _load_translation_dict(self) -> dict:
        try:
            with open(self.translation_file, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
            print(f"âœ… æˆåŠŸè¼‰å…¥ç¿»è­¯æ˜ å°„æª”æ¡ˆï¼Œæ¢ç›®æ•¸ï¼š{len(mapping)}")
            return mapping
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¿»è­¯æ˜ å°„æª”æ¡ˆï¼š{self.translation_file}ï¼Œè«‹å…ˆå»ºç«‹æª”æ¡ˆã€‚")
        except Exception as e:
            print(f"âŒ è¼‰å…¥ç¿»è­¯æ˜ å°„å¤±æ•—: {e}")
            return {}
        
    def get_translation(self, chinese_text: str) -> Optional[str]:
        return self.translation_dict.get(chinese_text)
    
    def translate_with_mapping(self, text: str) -> str:
        # å…ˆæª¢æŸ¥æ˜ å°„æª”æ¡ˆ
        direct_translation = self.get_translation(text)
        if direct_translation:
            print(f"ğŸ“„ æ˜ å°„æª”æ¡ˆç¿»è­¯: {text} â†’ {direct_translation}")
            return direct_translation
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜ å°„ä¸­çš„è©èª
        translated_parts = []
        remaining_text = text
        
        for chinese_term, english_term in self.translation_dict.items():
            if chinese_term in remaining_text:
                remaining_text = remaining_text.replace(chinese_term, f"__TRANSLATED_{len(translated_parts)}__")
                translated_parts.append((chinese_term, english_term))
        
        if translated_parts:
            # éƒ¨åˆ†å‘½ä¸­ï¼Œçµ„åˆç¿»è­¯
            final_translation = remaining_text
            for i, (chinese_term, english_term) in enumerate(translated_parts):
                final_translation = final_translation.replace(f"__TRANSLATED_{i}__", english_term)
            
            # å¦‚æœé‚„æœ‰å‰©é¤˜ä¸­æ–‡ï¼Œç”¨ LMStudio ç¿»è­¯
            if is_chinese(final_translation):
                final_translation = self._lmstudio_translate(final_translation)
            
            print(f"ğŸ”„ æ··åˆç¿»è­¯: {text} â†’ {final_translation}")
            return final_translation
        
        # å®Œå…¨æœªå‘½ä¸­ï¼Œä½¿ç”¨ LMStudio
        return self._lmstudio_translate(text)
    
    def _lmstudio_translate(self, text: str) -> str:
        try:
            from langchain_core.messages import SystemMessage, HumanMessage
            
            translation_prompt = f"è«‹å°‡ä»¥ä¸‹ä¸­æ–‡ç¿»è­¯æˆè‹±æ–‡ï¼Œè‹¥æœ‰ã€è¯åˆå¤§å­¸ã€çš„å­—æ¨£ï¼Œè«‹ä¸€å¾‹ç¿»æˆã€National United Universityã€ã€‚åªå›å‚³ç¿»è­¯çµæœï¼Œä¸è¦åŠ ä¸Šä»»ä½•è§£é‡‹æˆ–å‰ç¶´ã€‚\n\nåŸæ–‡ï¼š{text}"
            
            response = llm.invoke([
                SystemMessage(content='You are a professional translator. Translate Chinese to English only. Output only the translation without any explanation.'),
                HumanMessage(content=translation_prompt)
            ])
            
            translated = response.content.strip()
            
            # ç§»é™¤å¯èƒ½çš„å¼•è™Ÿæˆ–å‰ç¶´
            if translated.startswith('"') and translated.endswith('"'):
                translated = translated[1:-1]
            
            # ç§»é™¤å¸¸è¦‹çš„å‰ç¶´è©
            prefixes_to_remove = ["ç¿»è­¯ï¼š", "Translation:", "è‹±æ–‡ï¼š", "English:", "ç­”ï¼š", "Answer:"]
            for prefix in prefixes_to_remove:
                if translated.startswith(prefix):
                    translated = translated[len(prefix):].strip()
            
            print(f"ğŸ¤– LMStudioç¿»è­¯: {text} â†’ {translated}")
            return translated
            
        except Exception as e:
            print(f"âŒ LMStudioç¿»è­¯å¤±æ•—: {e}")
            return text

# æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
def is_chinese(text):
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
    return bool(chinese_pattern.search(text))

# åˆ†å‰²é—œéµå­—å‡½æ•¸
def split_keywords(s: str):
    return [kw.strip() for kw in re.split(r"[ã€,ï¼Œ]", s) if len(kw.strip()) > 1]

# Gemini åµŒå…¥å™¨
class GeminiEmbedder:
    def embed_query(self, text: str) -> List[float]:
        try:
            response = genai.embed_content(
                model="models/text-embedding-004",
                content=text
            )
            return response['embedding']
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return [0.0] * 768

# é—œéµå­—æœå°‹çµæ§‹
class Search(BaseModel):
    keywords: List[str]


#æ··åˆRAGç³»çµ±ï¼šWeaviateèªç¾©æª¢ç´¢ + Neo4jé—œéµå­—æª¢ç´¢ + åˆ†é¡åˆ†æ
class HybridRAG:
    
    # åˆå§‹åŒ–é€£æ¥
    def __init__(self):
        self.weaviate_client = None
        self.neo4j_driver = None
        self.embedder = None
        self.weaviate_collection = None
        
        # åˆå§‹åŒ–ç¿»è­¯ç®¡ç†å™¨
        self.translator = TranslationManager()
        
        # å˜—è©¦é€£æ¥ Weaviate
        try:
            self.weaviate_client = connect_to_local(
                skip_init_checks=True,
                additional_config=wvc.init.AdditionalConfig(
                    timeout=wvc.init.Timeout(init=60, query=30, insert=30)
                )
            )
            print("âœ… æˆåŠŸé€£æ¥åˆ° Weaviate")
            
            # è¼‰å…¥ Weaviate é›†åˆ
            try:
                self.weaviate_collection = self.weaviate_client.collections.get("ContentSummaryEn0913")
                self.text_field = "english_summary"
                print(f"ğŸ“š å·²è¼‰å…¥ ContentSummaryEn0913 é›†åˆ")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ ContentSummaryEn0913 é›†åˆ: {e}")
                self.weaviate_collection = None
                
        except Exception as e:
            print(f"âŒ Weaviate é€£æ¥å¤±æ•—: {e}")
            self.weaviate_client = None
        
        # å˜—è©¦é€£æ¥ Neo4j
        try:
            self.neo4j_driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "neo4j0804"),
                database="nuu-data-0804",
                max_connection_lifetime=3600,
                max_connection_pool_size=50,
                connection_acquisition_timeout=60,
                connection_timeout=20,
                resolver=None
            )
            
            # æ¸¬è©¦é€£æ¥
            with self.neo4j_driver.session() as session:
                result = session.run("RETURN 1 as test")
                result.single()
            print("âœ… æˆåŠŸé€£æ¥åˆ° Neo4j")
            
        except Exception as e:
            print(f"âŒ Neo4j é€£æ¥å¤±æ•—: {e}")
            self.neo4j_driver = None
        
        # åˆå§‹åŒ–åµŒå…¥å™¨
        try:
            self.embedder = GeminiEmbedder()
            print("âœ… åˆå§‹åŒ– Gemini åµŒå…¥å™¨")
        except Exception as e:
            print(f"âŒ åµŒå…¥å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
            self.embedder = None
            
        
    # åŸºæ–¼åˆ†é¡çš„Neo4jé—œéµå­—æª¢ç´¢
    def neo4j_keyword_search(self, keywords: List[str], limit: int = 10) -> List[Document]:
        if not self.neo4j_driver or not keywords:
            print("âš ï¸ Neo4jè·¯å¾‘ä¸å¯ç”¨ï¼šç¼ºå°‘é€£æ¥æˆ–é—œéµå­—")
            return []
        
        try:
            print(f"ğŸ” [Neo4jé—œéµå­—è·¯å¾‘] æœå°‹é—œéµå­—: {keywords}")
            return self._neo4j_global_search(keywords, limit)
            
        except Exception as e:
            print(f"âŒ Neo4j é—œéµå­—æœå°‹å¤±æ•—: {e}")
            return []
    def neo4j_keyword_search_enhanced(self, keywords: List[str], limit: int = 10) -> List[Document]:
        """
        Neo4j å¢å¼·æª¢ç´¢ï¼šé—œéµå­—ç²¾ç¢ºåŒ¹é… + åœ–è­œé—œè¯åˆ†æ
        """
        if not self.neo4j_driver or not keywords:
            print("âš ï¸ Neo4jè·¯å¾‘ä¸å¯ç”¨ï¼šç¼ºå°‘é€£æ¥æˆ–é—œéµå­—")
            return []
        
        try:
            print(f"ğŸ” [Neo4jå¢å¼·] æœå°‹é—œéµå­—: {keywords}")
            
            cypher = """
            // 1. æ‰¾åˆ°åŒ¹é…çš„ Keyword ç¯€é»
            MATCH (k:Keyword)
            WHERE any(kw IN $keywords WHERE 
                toLower(k.name) CONTAINS toLower(kw) OR 
                toLower(kw) CONTAINS toLower(k.name))
            
            // 2. é€éåœ–è­œæ‰¾åˆ°ç›¸é—œæ–‡ç« 
            MATCH (a:Article)-[:HAS_KEYWORD]->(k)
            
            // 3. ç²å–æ–‡ç« çš„å…¶ä»–é—œéµå­—ï¼ˆç”¨æ–¼è¨ˆç®—é—œè¯åº¦ï¼‰
            WITH a, k, 
                [(a)-[:HAS_KEYWORD]->(ak:Keyword) | ak.name] as article_keywords
            
            // 4. è¨ˆç®—é—œéµå­—åŒ¹é…åˆ†æ•¸
            WITH a, k, article_keywords,
                CASE 
                    WHEN k.name IN $keywords THEN 2.0  // å®Œå…¨åŒ¹é…
                    ELSE 1.0  // éƒ¨åˆ†åŒ¹é…
                END as keyword_score,
                // è¨ˆç®—é—œéµå­—é‡ç–Šåº¦ï¼ˆæ–‡ç« åŒ…å«å¤šå°‘å€‹æŸ¥è©¢é—œéµå­—ï¼‰
                size([kw IN $keywords WHERE any(article_k IN article_keywords WHERE 
                    toLower(article_k) CONTAINS toLower(kw))]) as keyword_overlap
            
            // 5. ç²å–é—œéµå­—çš„åˆ†é¡è³‡è¨Š
            OPTIONAL MATCH (k)-[:BELONGS_TO_CATEGORY]->(cat:Category)
            
            // 6. ç²å–å…§å®¹
            OPTIONAL MATCH (a)-[:HAS_CONTENT]->(c:Content)
            
            // 7. è¨ˆç®—æœ€çµ‚åˆ†æ•¸
            WITH DISTINCT a, c, 
                collect(DISTINCT k.name) AS keywords_found,
                collect(DISTINCT cat.name) AS categories_found,
                article_keywords,
                max(keyword_score) as max_keyword_score,
                max(keyword_overlap) as overlap_count,
                // ç¶œåˆåˆ†æ•¸ = åŒ¹é…åˆ†æ•¸ + é‡ç–Šåº¦åŠ æˆ
                (max(keyword_score) + max(keyword_overlap) * 0.5) as match_score
            
            RETURN 
                a.url AS article_url, 
                a.title AS article_title,
                a.domain AS article_domain,
                collect(DISTINCT {
                    id: c.id, 
                    text: c.text, 
                    type: c.type, 
                    order: c.order
                }) AS contents,
                keywords_found,
                categories_found,
                article_keywords,
                overlap_count,
                match_score
            ORDER BY match_score DESC, overlap_count DESC
            LIMIT $limit
            """
            
            with self.neo4j_driver.session() as session:
                result = session.run(cypher, keywords=keywords, limit=limit)
                records = result.data()
            
            print(f"âœ… [Neo4jå¢å¼·] æ‰¾åˆ° {len(records)} ç­†çµæœ")
            
            # é¡¯ç¤ºåŒ¹é…è©³æƒ…
            for i, rec in enumerate(records[:3], 1):
                print(f"  [{i}] åˆ†æ•¸: {rec.get('match_score', 0):.2f} | "
                    f"é‡ç–Š: {rec.get('overlap_count', 0)} | "
                    f"é—œéµå­—: {rec.get('keywords_found', [])} | "
                    f"æ¨™é¡Œ: {rec.get('article_title', '')[:40]}")
            
            return self._convert_neo4j_to_documents(records, "Neo4j_Enhanced")
            
        except Exception as e:
            print(f"âŒ Neo4j å¢å¼·æœå°‹å¤±æ•—: {e}")
            traceback.print_exc()
            return []
        
    # Neo4j å…¨åŸŸé—œéµå­—æœå°‹
    def _neo4j_global_search(self, keywords: List[str], limit: int = 10) -> List[Document]:
        try:
            cypher = """
            MATCH (k:Keyword)
            WHERE any(kw IN $keywords WHERE 
                toLower(k.name) CONTAINS toLower(kw) OR 
                toLower(kw) CONTAINS toLower(k.name))
            
            MATCH (a:Article)-[:HAS_KEYWORD]->(k)
            OPTIONAL MATCH (a)-[:HAS_CONTENT]->(c:Content)
            OPTIONAL MATCH (k)-[:BELONGS_TO_CATEGORY]->(cat:Category)
            
            WITH DISTINCT a, c, k, cat,
                CASE 
                WHEN k.name IN $keywords THEN 1.0
                ELSE 0.8
                END as keyword_score
            
            RETURN a.url AS article_url, 
                a.title AS article_title,
                a.domain AS article_domain,
                collect(DISTINCT {
                    id: c.id, 
                    text: c.text, 
                    type: c.type, 
                    order: c.order
                }) AS contents,
                collect(DISTINCT k.name) AS keywords_found,
                collect(DISTINCT cat.name) AS categories_found,
                max(keyword_score) as match_score
            ORDER BY match_score DESC
            LIMIT $limit
            """
            
            with self.neo4j_driver.session() as session:
                result = session.run(cypher, keywords=keywords, limit=limit)
                records = result.data()
            
            print(f"âœ… [å…¨åŸŸæœå°‹] æ‰¾åˆ° {len(records)} ç­†çµæœ")
            return self._convert_neo4j_to_documents(records, "Neo4j_Global")
            
        except Exception as e:
            print(f"âŒ Neo4j å…¨åŸŸæœå°‹å¤±æ•—: {e}")
            return []

    # å°‡Neo4jè¨˜éŒ„è½‰æ›ç‚ºDocumentæ ¼å¼
    def _convert_neo4j_to_documents(self, records: List[Dict], source_type: str) -> List[Document]:
        documents = []
        
        for rec in records:
            text_parts = []
            if rec.get('article_title'):
                text_parts.append(f"æ¨™é¡Œ: {rec['article_title']}")
            if rec.get('article_url'):
                text_parts.append(f"ç¶²å€: {rec['article_url']}")
            if rec.get('contents'):
                content_texts = []
                for content in rec['contents']:
                    if content and content.get('text'):
                        content_text = content['text']
                        content_type = content.get('type', '')
                        if content_type:
                            content_texts.append(f"[{content_type}] {content_text}")
                        else:
                            content_texts.append(content_text)
            if content_texts:
                text_parts.append(f"å…§å®¹: {' '.join(content_texts)}")
            if rec.get('keywords_found'):
                text_parts.append(f"é—œéµå­—: {', '.join(rec['keywords_found'])}")
            if rec.get('categories_found'):
                text_parts.append(f"åˆ†é¡: {', '.join(rec['categories_found'])}")
            if rec.get('contacts'):
                for contact in rec['contacts']:
                    if contact and contact.get('value'):
                        text_parts.append(f"è¯çµ¡æ–¹å¼({contact.get('type', '')}): {contact['value']} {contact.get('department', '')}")
            if rec.get('addresses'):
                for address in rec['addresses']:
                    if address and address.get('full_address'):
                        text_parts.append(f"åœ°å€: {address['full_address']} (åŸå¸‚: {address.get('city', '')}, å€åŸŸ: {address.get('district', '')})")
            if rec.get('departments_found'):
                text_parts.append(f"éƒ¨é–€: {', '.join(rec['departments_found'])}")
            
            content = "\n".join(text_parts)
            match_score = rec.get('match_score', 0.5)
            
            doc = Document(
                page_content=content,
                metadata={
                    'source': source_type,
                    'article_url': rec.get('article_url', ''),
                    'article_title': rec.get('article_title', ''),
                    'match_score': match_score,
                    'enhanced': True
                }
            )
            documents.append(doc)
        
        return documents
    
    # Weaviate èªç¾©æª¢ç´¢è·¯å¾‘
    def weaviate_search(self, query: str, limit: int = 10) -> List[Document]:
        if not self.weaviate_client or not self.embedder or not self.weaviate_collection or not self.neo4j_driver:
            print("âš ï¸ Weaviateè·¯å¾‘ä¸å¯ç”¨ï¼šç¼ºå°‘å¿…è¦çµ„ä»¶")
            return []
        
        try:
            print(f"ğŸ” [Weaviateè·¯å¾‘] é€²è¡Œèªç¾©æª¢ç´¢...")
            query_vector = self.embedder.embed_query(query)
            weaviate_results = self._weaviate_vector_search(query_vector, limit)
            
            if not weaviate_results:
                print("âŒ Weaviate æœå°‹ç„¡çµæœ")
                return []
            
            # æå– neo4j_id ä¸¦æŸ¥è©¢ Neo4j ç²å–å®Œæ•´è³‡è¨Š
            neo4j_ids = []
            weaviate_data = {}
            
            for result in weaviate_results:
                neo4j_id = result['neo4j_id']
                if neo4j_id:
                    neo4j_ids.append(neo4j_id)
                    weaviate_data[neo4j_id] = result
            
            if not neo4j_ids:
                print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ neo4j_id")
                return []
            
            print(f"ğŸ” [Weaviateè·¯å¾‘] ä½¿ç”¨ {len(neo4j_ids)} å€‹ ID æŸ¥è©¢ Neo4j...")
            enhanced_documents = self._query_neo4j_by_ids(neo4j_ids, weaviate_data, source="Weaviate+Neo4j")
            
            print(f"âœ… [Weaviateè·¯å¾‘] ç”Ÿæˆ {len(enhanced_documents)} ç­†å¢å¼·æ–‡æª”")
            return enhanced_documents
            
        except Exception as e:
            print(f"âŒ Weaviate æœå°‹å¤±æ•—: {e}")
            return []
    def weaviate_search_with_prf_adaptive(self, query: str, limit: int = 10, use_prf: bool = True) -> List[Document]:
        """
        è‡ªé©æ‡‰ PRFï¼šæ ¹æ“šæŸ¥è©¢è¤‡é›œåº¦å‹•æ…‹èª¿æ•´åƒæ•¸
        """
        if not self.weaviate_client or not self.embedder or not self.weaviate_collection or not self.neo4j_driver:
            print("âš ï¸ Weaviate+PRFè·¯å¾‘ä¸å¯ç”¨ï¼šç¼ºå°‘å¿…è¦çµ„ä»¶")
            return []
        
        try:
            # éšæ®µ0ï¼šè©•ä¼°æŸ¥è©¢è¤‡é›œåº¦
            query_complexity = self._assess_query_complexity(query)
            print(f"ğŸ“Š [PRF] æŸ¥è©¢è¤‡é›œåº¦è©•ä¼°: {query_complexity}")
            
            # éšæ®µ1ï¼šåˆå§‹èªç¾©æª¢ç´¢
            print(f"ğŸ” [Weaviate+PRF] éšæ®µ1ï¼šåˆå§‹èªç¾©æª¢ç´¢...")
            query_vector = self.embedder.embed_query(query)
            initial_results = self._weaviate_vector_search(query_vector, limit=10)
            
            if not initial_results:
                print("âŒ Weaviate åˆå§‹æœå°‹ç„¡çµæœ")
                return []
            
            print(f"âœ… ç²å¾— {len(initial_results)} ç­†åˆå§‹çµæœ")
            
            if not use_prf:
                return self._build_documents_from_weaviate(initial_results)
            
            #å‹•æ…‹èª¿æ•´ PRF åƒæ•¸
            prf_config = self._get_prf_config(query_complexity)
            print(f"âš™ï¸ [PRF] é…ç½®: Top-{prf_config['top_n']} æ–‡ç« , "
                f"æœ€å¤š {prf_config['max_keywords']} å€‹é—œéµå­—, "
                f"éæ¿¾åˆ° {prf_config['target_filtered']} å€‹")
            
            # éšæ®µ2ï¼šPRF é—œéµå­—æ“´å±•
            print(f"ğŸ“Š [PRF] éšæ®µ2ï¼šå¾ Top-{prf_config['top_n']} æ–‡æª”æå–é—œéµå­—...")
            
            top_n_urls = []
            for result in initial_results[:prf_config['top_n']]:
                article_url = result.get('article_url')
                if article_url:
                    top_n_urls.append(article_url)
            
            if not top_n_urls:
                print("âš ï¸ ç„¡æ³•æå– URLsï¼Œè¿”å›åˆå§‹çµæœ")
                return self._build_documents_from_weaviate(initial_results)
            
            # ä½¿ç”¨è‡ªé©æ‡‰åœ–è­œæ“´å±•
            expanded_keywords = self._extract_and_expand_keywords_from_graph_adaptive(
                top_n_urls, 
                max_total_keywords=prf_config['max_keywords']
            )
            
            if not expanded_keywords:
                print("âš ï¸ PRF ç„¡æ³•æ“´å±•é—œéµå­—ï¼Œè¿”å›åˆå§‹çµæœ")
                return self._build_documents_from_weaviate(initial_results)
            
            # éšæ®µ3ï¼šMistral éæ¿¾
            print(f"ğŸ¤– [PRF] éšæ®µ3ï¼šç”¨ Mistral éæ¿¾é—œéµå­—...")
            filtered_keywords = self._filter_keywords_with_lmstudio_adaptive(
                query, 
                expanded_keywords,
                target_count=prf_config['target_filtered']
            )
            
            if not filtered_keywords:
                print("âš ï¸ PRF é—œéµå­—éæ¿¾å¾Œç„¡çµæœï¼Œè¿”å›åˆå§‹çµæœ")
                return self._build_documents_from_weaviate(initial_results)
            
            print(f"âœ… [PRF] éæ¿¾å¾Œçš„æ“´å±•é—œéµå­—: {filtered_keywords}")
            
            # éšæ®µ4ï¼šæ“´å±•æŸ¥è©¢é‡æ–°æª¢ç´¢
            print(f"ğŸ” [PRF] éšæ®µ4ï¼šç”¨æ“´å±•æŸ¥è©¢é‡æ–°æª¢ç´¢...")
            
            #  æ™ºèƒ½çµ„åˆæŸ¥è©¢ï¼ˆé™åˆ¶é•·åº¦ï¼‰
            max_expand_kw = min(3, len(filtered_keywords))
            expanded_query = f"{query} {' '.join(filtered_keywords[:max_expand_kw])}"
            
            # é˜²æ­¢æŸ¥è©¢éé•·
            if len(expanded_query) > 200:
                print(f"âš ï¸ æ“´å±•æŸ¥è©¢éé•· ({len(expanded_query)} å­—å…ƒ)ï¼Œæˆªæ–·åˆ° 200")
                expanded_query = expanded_query[:200]
            
            print(f"   æ“´å±•æŸ¥è©¢: {expanded_query}")
            
            expanded_vector = self.embedder.embed_query(expanded_query)
            expanded_results = self._weaviate_vector_search(expanded_vector, limit=10)
            
            # éšæ®µ5ï¼šæ™ºèƒ½åˆä½µ
            print(f"ğŸ”— [PRF] éšæ®µ5ï¼šåˆä½µçµæœ...")
            merged_results = self._merge_prf_results_smart(initial_results, expanded_results)
            
            print(f"âœ… [PRF] æœ€çµ‚çµæœ: {len(merged_results)} ç­†")
            
            return self._build_documents_from_weaviate(merged_results)
            
        except Exception as e:
            print(f"âŒ Weaviate+PRF æœå°‹å¤±æ•—: {e}")
            traceback.print_exc()
            return []
    def _assess_query_complexity(self, query: str) -> str:
        """
        è©•ä¼°æŸ¥è©¢è¤‡é›œåº¦
        
        Returns:
            'simple' | 'medium' | 'complex'
        """
        word_count = len(query.split())
        has_multiple_entities = ',' in query or 'and' in query.lower() or 'å’Œ' in query
        
        if word_count <= 5 and not has_multiple_entities:
            return 'simple'
        elif word_count <= 10:
            return 'medium'
        else:
            return 'complex'

    def _get_prf_config(self, complexity: str) -> dict:
        """
        æ ¹æ“šè¤‡é›œåº¦è¿”å› PRF é…ç½®
        """
        configs = {
            'simple': {
                'top_n': 2,              # åªç”¨ Top-2 æ–‡ç« 
                'max_keywords': 30,      # æœ€å¤š 30 å€‹å€™é¸
                'target_filtered': 3     # éæ¿¾åˆ° 3 å€‹
            },
            'medium': {
                'top_n': 3,              # Top-3 æ–‡ç« 
                'max_keywords': 50,      # æœ€å¤š 50 å€‹å€™é¸
                'target_filtered': 5     # éæ¿¾åˆ° 5 å€‹
            },
            'complex': {
                'top_n': 5,              # Top-5 æ–‡ç« 
                'max_keywords': 80,      # æœ€å¤š 80 å€‹å€™é¸
                'target_filtered': 8     # éæ¿¾åˆ° 8 å€‹
            }
        }
        return configs.get(complexity, configs['medium'])
    # Weaviate å‘é‡æœå°‹
    def _weaviate_vector_search(self, query_vector: List[float], limit: int) -> List[dict]:
        results = []
        
        try:
            response = self.weaviate_collection.query.near_vector(
                near_vector=query_vector,
                limit=limit,
                return_metadata=wvc.query.MetadataQuery(distance=True)
            )
            
            for obj in response.objects:
                content = obj.properties.get(self.text_field, '')
                if not content or content.strip() == "":
                    continue
                
                distance = float(obj.metadata.distance) if obj.metadata.distance is not None else 1.0
                similarity = max(0, 1.0 - distance)
                
                result = {
                    'weaviate_uuid': str(obj.uuid),
                    'english_summary': content,
                    'neo4j_id': obj.properties.get('neo4j_id', ''),
                    'article_url': obj.properties.get('article_url', ''),
                    'language': obj.properties.get('language', 'en'),
                    'similarity': similarity,
                    'distance': distance
                }
                results.append(result)
            
            return results
            
        except Exception as e:
            print(f"âŒ Weaviate å‘é‡æœå°‹å¤±æ•—: {e}")
            return []
    
    # æ ¹æ“š neo4j_id åˆ—è¡¨æŸ¥è©¢ Neo4j ä¸­çš„å®Œæ•´è³‡æ–™
    def _query_neo4j_by_ids(self, neo4j_ids: List[str], weaviate_data: dict, source: str = "Enhanced") -> List[Document]:
        if not self.neo4j_driver or not neo4j_ids:
            return []
        
        enhanced_documents = []
        
        for neo4j_id in neo4j_ids:
            print(f"ğŸ” æŸ¥è©¢ Neo4j ID: {neo4j_id}")
            start_time = time.time()
            
            try:
                # åˆ†éšæ®µæŸ¥è©¢ç­–ç•¥
                content_data = self._get_content_info(neo4j_id)
                if not content_data:
                    print(f"âš ï¸ Content è³‡æ–™ä¸å­˜åœ¨: {neo4j_id}")
                    continue
                
                article_data = self._get_article_info(neo4j_id)
                org_data = self._get_organization_info(neo4j_id)
                
                # åˆä½µè³‡æ–™
                combined_data = {**content_data, **article_data, **org_data}
                
                # å»ºç«‹å¢å¼·æ–‡æª”
                weaviate_info = weaviate_data.get(neo4j_id, {})
                enhanced_doc = self._create_enhanced_document_optimized(combined_data, weaviate_info)
                enhanced_documents.append(enhanced_doc)
                
                elapsed = time.time() - start_time
                print(f"âœ… æŸ¥è©¢å®Œæˆ ({elapsed:.2f}s)")
                
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âŒ æŸ¥è©¢å¤±æ•— ({elapsed:.2f}s): {e}")
                
                # ä½¿ç”¨ Weaviate å‚™æ¡ˆ
                weaviate_info = weaviate_data.get(neo4j_id, {})
                if weaviate_info:
                    fallback_doc = Document(
                        page_content=weaviate_info.get('english_summary', ''),
                        metadata={
                            'source': 'Weaviate_fallback',
                            'neo4j_id': neo4j_id,
                            'similarity': weaviate_info.get('similarity', 0),
                            'enhanced': False
                        }
                    )
                    enhanced_documents.append(fallback_doc)
        
        return enhanced_documents
    def _extract_and_expand_keywords_from_graph_adaptive(self, article_urls: List[str], max_total_keywords: int = 50) -> List[str]:
        """
        å‹•æ…‹è‡ªé©æ‡‰çš„åœ–è­œé—œéµå­—æ“´å±•
        
        Args:
            article_urls: Top-N æ–‡ç«  URLs
            max_total_keywords: æœ€å¤§é—œéµå­—ç¸½æ•¸ï¼ˆé è¨­ 50ï¼‰
        
        Returns:
            ç²¾é¸çš„é—œéµå­—åˆ—è¡¨
        """
        
        query = """
        // 1. æ‰¾åˆ° Top-N æ–‡ç« 
        UNWIND $article_urls AS url
        MATCH (a:Article {url: url})
        
        // 2. æå–é€™äº›æ–‡ç« çš„é—œéµå­—
        MATCH (a)-[:HAS_KEYWORD]->(k:Keyword)
        
        // 3. æ‰¾åˆ°é€™äº›é—œéµå­—çš„åˆ†é¡
        OPTIONAL MATCH (k)-[:BELONGS_TO_CATEGORY]->(cat:Category)
        
        //  4. é€éåˆ†é¡æ‰¾ç›¸é—œé—œéµå­—
        OPTIONAL MATCH (cat)<-[:BELONGS_TO_CATEGORY]-(related_k:Keyword)
        WHERE related_k <> k
        
        //  æ ¸å¿ƒæ”¹é€²ï¼šåœ¨åœ–è­œå±¤é¢å°±é™åˆ¶æ¯å€‹åŸå§‹é—œéµå­—çš„æ“´å±•æ•¸é‡
        WITH k, cat, 
            k.name as original_keyword,
            cat.name as category,
            // æ¯å€‹åŸå§‹é—œéµå­—æœ€å¤šæ“´å±• 10 å€‹ç›¸é—œè©
            [related IN collect(DISTINCT related_k.name)[0..10] | related] as related_keywords,
            size(collect(DISTINCT related_k)) as total_related_count
        
        // 5. è¨ˆç®—æ“´å±•è³ªé‡åˆ†æ•¸
        WITH original_keyword, 
            category,
            related_keywords,
            total_related_count,
            // æ“´å±•è³ªé‡ = æœ‰é™çš„æ“´å±•æ•¸ï¼ˆ10å€‹ï¼‰å’Œç¸½æ“´å±•èƒ½åŠ›çš„å¹³è¡¡
            (size(related_keywords) * 1.0 / (1.0 + log(total_related_count))) as expansion_quality
        
        // 6. æŒ‰è³ªé‡æ’åºï¼Œé™åˆ¶ç¸½æ•¸
        ORDER BY expansion_quality DESC
        LIMIT $max_original_keywords  // é™åˆ¶åŸå§‹é—œéµå­—æ•¸é‡ï¼ˆé è¨­ 10ï¼‰
        RETURN original_keyword, category, related_keywords, total_related_count, expansion_quality
        """
        
        try:
            # å‹•æ…‹è¨ˆç®—åƒæ•¸
            max_original_keywords = min(10, len(article_urls) * 3)  # æ¯ç¯‡æ–‡ç« æœ€å¤š 3 å€‹åŸå§‹é—œéµå­—
            
            with self.neo4j_driver.session() as session:
                result = session.run(
                    query, 
                    article_urls=article_urls,
                    max_original_keywords=max_original_keywords
                )
                records = result.data()
            
            if not records:
                print("âš ï¸ åœ–è­œä¸­æœªæ‰¾åˆ°é—œéµå­—")
                return []
            
            # æ”¶é›†é—œéµå­—ï¼šåŸå§‹ + é™é‡æ“´å±•
            all_keywords = []
            keyword_sources = {}
            
            for record in records:
                # åŸå§‹é—œéµå­—ï¼ˆé«˜å„ªå…ˆç´šï¼‰
                original = record.get('original_keyword')
                category = record.get('category')
                
                if original:
                    all_keywords.append(original)
                    keyword_sources[original] = 'original'
                
                # æ“´å±•é—œéµå­—ï¼ˆæ¯å€‹åŸå§‹è©æœ€å¤š 10 å€‹ï¼‰
                related = record.get('related_keywords', [])
                for kw in related[:10]:  # é›™é‡ä¿éšª
                    if kw:
                        all_keywords.append(kw)
                        keyword_sources[kw] = f'expanded_from_{category}'
            
            # å»é‡ä¿æŒé †åº
            unique_keywords = []
            seen = set()
            for kw in all_keywords:
                if kw not in seen:
                    unique_keywords.append(kw)
                    seen.add(kw)
            
            # â­ å¦‚æœé‚„æ˜¯å¤ªå¤šï¼ŒæŒ‰ä¾†æºå„ªå…ˆç´šæˆªæ–·
            if len(unique_keywords) > max_total_keywords:
                print(f"âš ï¸ é—œéµå­—æ•¸é‡ {len(unique_keywords)} è¶…éé™åˆ¶ {max_total_keywords}ï¼Œé€²è¡Œæˆªæ–·")
                
                # å„ªå…ˆä¿ç•™åŸå§‹é—œéµå­—
                original_kws = [kw for kw in unique_keywords if keyword_sources.get(kw) == 'original']
                expanded_kws = [kw for kw in unique_keywords if keyword_sources.get(kw) != 'original']
                
                remaining_slots = max_total_keywords - len(original_kws)
                unique_keywords = original_kws + expanded_kws[:remaining_slots]
            
            print(f"ğŸ“Š [åœ–è­œæ“´å±•] å¾ {len(article_urls)} ç¯‡æ–‡ç« æå–:")
            print(f"   - åŸå§‹é—œéµå­—: {sum(1 for v in keyword_sources.values() if v == 'original')} å€‹")
            print(f"   - æ“´å±•é—œéµå­—: {sum(1 for v in keyword_sources.values() if v.startswith('expanded'))} å€‹")
            print(f"   - ç¸½è¨ˆ: {len(unique_keywords)} å€‹ï¼ˆé™åˆ¶ {max_total_keywords}ï¼‰")
            
            return unique_keywords
            
        except Exception as e:
            print(f"âŒ åœ–è­œé—œéµå­—æå–å¤±æ•—: {e}")
            traceback.print_exc()
            return []
    def _filter_keywords_with_lmstudio_adaptive(self, original_query: str, candidate_keywords: List[str], target_count: int = None) -> List[str]:
        """
        å‹•æ…‹è‡ªé©æ‡‰çš„é—œéµå­—éæ¿¾ï¼ˆä½¿ç”¨ LMStudioï¼‰
        """
        try:
            # å‹•æ…‹è¨ˆç®—å€™é¸æ•¸é‡ä¸Šé™
            if len(candidate_keywords) > 100:
                print(f"âš ï¸ å€™é¸é—œéµå­—éå¤š ({len(candidate_keywords)} å€‹)ï¼Œæ™ºèƒ½æ¡æ¨£åˆ° 50 å€‹")
                
                step = len(candidate_keywords) // 20
                sampled = (
                    candidate_keywords[:20] +
                    candidate_keywords[20:-10:max(1, step)] +
                    candidate_keywords[-10:]
                )
                candidates = list(dict.fromkeys(sampled))[:50]
            else:
                candidates = candidate_keywords[:30]
            
            if not candidates:
                return []
            
            # å‹•æ…‹è¨ˆç®—ç›®æ¨™æ•¸é‡
            if target_count is None:
                query_words = len(original_query.split())
                if query_words <= 5:
                    target_count = "3-5"
                elif query_words <= 10:
                    target_count = "5-8"
                else:
                    target_count = "8-12"
            else:
                target_count = str(target_count)
            
            prompt = f"""ä½ æ˜¯ä¸€å€‹é—œéµå­—éæ¿¾å°ˆå®¶ã€‚çµ¦å®šåŸå§‹æŸ¥è©¢å’Œå€™é¸é—œéµå­—åˆ—è¡¨ï¼Œè«‹é¸å‡ºæœ€ç›¸é—œçš„ {target_count} å€‹é—œéµå­—ç”¨æ–¼æ“´å±•æŸ¥è©¢ã€‚

    åŸå§‹æŸ¥è©¢ï¼š{original_query}

    å€™é¸é—œéµå­—åˆ—è¡¨ï¼ˆå…± {len(candidates)} å€‹ï¼‰ï¼š
    {', '.join(candidates)}

    ä»»å‹™ï¼š
    1. é¸å‡ºèˆ‡åŸå§‹æŸ¥è©¢èªç¾©æœ€ç›¸é—œçš„é—œéµå­—
    2. å„ªå…ˆé¸æ“‡èƒ½å¹«åŠ©æ‰¾åˆ°æ›´å¤šç›¸é—œæ–‡æª”çš„é—œéµå­—
    3. æ’é™¤ä¸ç›¸é—œæˆ–éæ–¼å¯¬æ³›çš„é—œéµå­—
    4. â­ åš´æ ¼æ§åˆ¶æ•¸é‡åœ¨ {target_count} å€‹ä»¥å…§

    è«‹åªå›å‚³é¸ä¸­çš„é—œéµå­—ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦æœ‰å…¶ä»–æ–‡å­—ã€‚
    ä¾‹å¦‚ï¼šé—œéµå­—1, é—œéµå­—2, é—œéµå­—3"""

            from langchain_core.messages import SystemMessage, HumanMessage
            
            response = llm.invoke([
                SystemMessage(content='You are a keyword filter expert. Select most relevant keywords only.'),
                HumanMessage(content=prompt)
            ])
            
            filtered_text = response.content.strip()
            
            # è§£æå›æ‡‰
            filtered_keywords = [kw.strip() for kw in filtered_text.split(',') if kw.strip()]
            
            # å¼·åˆ¶é™åˆ¶æ•¸é‡
            max_count = int(target_count.split('-')[-1]) if '-' in target_count else int(target_count)
            filtered_keywords = filtered_keywords[:max_count]
            
            print(f"ğŸ¤– [LMStudioéæ¿¾] {len(candidates)} å€‹å€™é¸ â†’ {len(filtered_keywords)} å€‹ä¿ç•™")
            if filtered_keywords:
                print(f"   ä¿ç•™çš„é—œéµå­—: {filtered_keywords}")
            
            return filtered_keywords
            
        except Exception as e:
            print(f"âŒ LMStudio éæ¿¾å¤±æ•—: {e}")
            fallback_count = 5 if len(candidate_keywords) > 10 else 3
            fallback = candidate_keywords[:fallback_count]
            print(f"âš ï¸ ä½¿ç”¨å‚™æ¡ˆé—œéµå­—: {fallback}")
            return fallback
    def _merge_prf_results_smart(self, initial_results: List[dict], expanded_results: List[dict]) -> List[dict]:
        """
        æ™ºèƒ½åˆä½µï¼šä¿ç•™åˆå§‹ Top-10ï¼ˆä¸»è¦ï¼‰ï¼Œè£œå…… PRF æ–°ç™¼ç¾çš„æ–‡æª”
        ç ”ç©¶å»ºè­°ï¼šåŸå§‹æŸ¥è©¢ä¿ç•™ä¸»è¦æ¬Šé‡ï¼Œæ“´å±•ç‚ºè¼”
        """
        seen_ids = set()
        merged = []
        
        # ç¬¬ä¸€å„ªå…ˆï¼šä¿ç•™åˆå§‹ Top-10ï¼ˆç›¸ä¼¼åº¦ä¸è®Šï¼Œé€™äº›æ˜¯æœ€ç›¸é—œçš„ï¼‰
        print(f"ğŸ”— [åˆä½µ] éšæ®µ1ï¼šä¿ç•™åˆå§‹ Top-10")
        for i, result in enumerate(initial_results[:10], 1):
            result_id = result.get('weaviate_uuid') or result.get('neo4j_id')
            if result_id:
                result_copy = result.copy()
                result_copy['prf_source'] = 'initial_top10'
                result_copy['rank'] = i
                merged.append(result_copy)
                seen_ids.add(result_id)
        
        print(f"   å·²åŠ å…¥ {len(merged)} ç­†åˆå§‹çµæœ")
        
        # ç¬¬äºŒå„ªå…ˆï¼šè£œå…… PRF æ“´å±•ç™¼ç¾çš„æ–°æ–‡æª”ï¼ˆæ¬Šé‡é™ä½ 0.5ï¼‰
        print(f"ğŸ”— [åˆä½µ] éšæ®µ2ï¼šè£œå…… PRF æ–°ç™¼ç¾çš„æ–‡æª”")
        prf_added = 0
        for result in expanded_results:
            result_id = result.get('weaviate_uuid') or result.get('neo4j_id')
            if result_id and result_id not in seen_ids:
                result_copy = result.copy()
                # é™ä½ PRF è£œå……æ–‡æª”çš„æ¬Šé‡ï¼ˆå› ç‚ºæ˜¯æ“´å±•æŸ¥è©¢æ‰¾åˆ°çš„ï¼‰
                result_copy['similarity'] = result.get('similarity', 0) * 0.5
                result_copy['prf_source'] = 'prf_supplement'
                merged.append(result_copy)
                seen_ids.add(result_id)
                prf_added += 1
                
                # æœ€å¤šè£œå…… 5 ç­†
                if prf_added >= 5:
                    break
        
        print(f"   PRF è£œå……äº† {prf_added} ç­†æ–°æ–‡æª”")
        
        # æ’åºï¼šåˆå§‹çµæœå„ªå…ˆï¼Œç„¶å¾ŒæŒ‰ç›¸ä¼¼åº¦
        merged.sort(key=lambda x: (
            0 if x.get('prf_source') == 'initial_top10' else 1,  # åˆå§‹çµæœæ’å‰é¢
            -x.get('similarity', 0)  # ç›¸ä¼¼åº¦é™åº
        ))
        
        final_count = min(len(merged), 15)  # æœ€å¤šè¿”å› 15 ç­†
        print(f"ğŸ”— [åˆä½µ] æœ€çµ‚: {final_count} ç­†æ–‡æª”ï¼ˆåˆå§‹ {len(initial_results)} + PRFè£œå…… {prf_added}ï¼‰")
        
        return merged[:15]
    def _build_documents_from_weaviate(self, weaviate_results: List[dict]) -> List[Document]:
        """
        å¾ Weaviate çµæœå»ºç«‹æ–‡æª”ï¼ˆéœ€è¦æŸ¥è©¢ Neo4j ç²å–å®Œæ•´è³‡è¨Šï¼‰
        """
        if not weaviate_results:
            return []
        
        neo4j_ids = []
        weaviate_data = {}
        
        for result in weaviate_results:
            neo4j_id = result.get('neo4j_id')
            if neo4j_id:
                neo4j_ids.append(neo4j_id)
                weaviate_data[neo4j_id] = result
        
        if not neo4j_ids:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ neo4j_id")
            return []
        
        print(f"ğŸ” ä½¿ç”¨ {len(neo4j_ids)} å€‹ ID æŸ¥è©¢ Neo4j ç²å–å®Œæ•´è³‡è¨Š...")
        enhanced_documents = self._query_neo4j_by_ids(neo4j_ids, weaviate_data, source="Weaviate+Neo4j+PRF")
    
        return enhanced_documents
    # ç²å– Content åŸºæœ¬è³‡è¨Š
    def _get_content_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        RETURN 
            content.neo4j_id as content_neo4j_id,
            content.id as content_id,
            content.text as content_text,
            content.type as content_type,
            content.order as content_order
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Content æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å– Article è³‡è¨Š
    def _get_article_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        RETURN 
            article.neo4j_id as article_neo4j_id,
            article.url as article_url,
            article.title as article_title,
            article.domain as article_domain,
            article.og_image as article_og_image
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Article æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å– Organization è³‡è¨Š
    def _get_organization_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        OPTIONAL MATCH (article)-[:BELONGS_TO]->(org:Organization)
        RETURN 
            org.name as org_name,
            org.type as org_type,
            org.unified_number as org_unified_number
        LIMIT 1
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Organization æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # å„ªåŒ–ç‰ˆå¢å¼·æ–‡æª”å»ºç«‹
    def _create_enhanced_document_optimized(self, neo4j_data: dict, weaviate_info: dict) -> Document:
        try:
            content_parts = []
            
            # Weaviate è‹±æ–‡æ‘˜è¦
            base_content = weaviate_info.get('english_summary', '')
            if base_content:
                content_parts.append(base_content)
            
            # Neo4j Content åŸå§‹æ–‡æœ¬
            content_text = neo4j_data.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            # Article è³‡è¨Š
            article_title = neo4j_data.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
                
            article_url = neo4j_data.get('article_url')
            if article_url:
                content_parts.append(f"ã€æ–‡ç« ç¶²å€ã€‘\n{article_url}")
            
            # çµ„ç¹”è³‡è¨Š
            org_name = neo4j_data.get('org_name')
            if org_name:
                org_info = org_name
                org_type = neo4j_data.get('org_type')
                if org_type:
                    org_info += f" ({org_type})"
                content_parts.append(f"ã€æ‰€å±¬çµ„ç¹”ã€‘\n{org_info}")
            
            # çµ„åˆå…§å®¹
            enhanced_content = "\n\n".join(content_parts) if content_parts else "ç„¡å…§å®¹"
            
            # å»ºç«‹ metadata
            metadata = {
                'source': 'Weaviate+Neo4j_optimized',
                'neo4j_id': neo4j_data.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'enhanced': True,
                'article_title': article_title or '',
                'article_url': neo4j_data.get('article_url', ''),
                'content_type': neo4j_data.get('content_type', ''),
                'organization': org_name or ''
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ æ–‡æª”å»ºç«‹å¤±æ•—: {e}")
            # è¿”å›åŸºç¤ Weaviate æ–‡æª”
            return Document(
                page_content=weaviate_info.get('english_summary', 'Error'),
                metadata={
                    'source': 'Error_fallback',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'enhanced': False
                }
            )
            
    # å»ºç«‹å¢å¼·æ–‡æª” (ä¿ç•™å‚™ç”¨)
    def _create_enhanced_document(self, neo4j_record, weaviate_info: dict, source: str) -> Document:
        try:
            base_content = weaviate_info.get('english_summary', '')
            content_parts = [base_content] if base_content else []
            
            content_text = neo4j_record.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            article_title = neo4j_record.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
            
            article_domain = neo4j_record.get('article_domain')
            if article_domain:
                content_parts.append(f"ã€ç¶²ç«™ä¾†æºã€‘\n{article_domain}")
            
            # æ·»åŠ è¯çµ¡è³‡è¨Š
            contacts = neo4j_record.get('contacts', [])
            valid_contacts = [c for c in contacts if c.get('value')]
            if valid_contacts:
                contact_lines = []
                for contact in valid_contacts:
                    contact_type = contact.get('type', 'è¯çµ¡æ–¹å¼')
                    contact_value = contact.get('value', '')
                    contact_dept = contact.get('department', '')
                    
                    contact_line = f"{contact_type}: {contact_value}"
                    if contact_dept:
                        contact_line += f" ({contact_dept})"
                    contact_lines.append(contact_line)
                
                if contact_lines:
                    content_parts.append(f"ã€è¯çµ¡è³‡è¨Šã€‘\n" + "\n".join(contact_lines))
            
            # æ·»åŠ åœ°å€è³‡è¨Š
            addresses = neo4j_record.get('addresses', [])
            valid_addresses = [a for a in addresses if a.get('full_address')]
            if valid_addresses:
                address_lines = []
                for addr in valid_addresses:
                    full_addr = addr.get('full_address', '')
                    city = addr.get('city', '')
                    district = addr.get('district', '')
                    
                    addr_line = full_addr
                    if city and district:
                        addr_line += f" ({city}{district})"
                    elif city:
                        addr_line += f" ({city})"
                    
                    address_lines.append(addr_line)
                
                if address_lines:
                    content_parts.append(f"ã€åœ°å€è³‡è¨Šã€‘\n" + "\n".join(address_lines))
            
            # æ·»åŠ éƒ¨é–€è³‡è¨Š
            departments = neo4j_record.get('departments', [])
            valid_departments = [d for d in departments if d.get('name')]
            if valid_departments:
                dept_lines = []
                for dept in valid_departments:
                    dept_name = dept.get('name', '')
                    dept_type = dept.get('type', '')
                    
                    dept_line = dept_name
                    if dept_type:
                        dept_line += f" ({dept_type})"
                    dept_lines.append(dept_line)
                
                if dept_lines:
                    content_parts.append(f"ã€æ‰€å±¬éƒ¨é–€ã€‘\n" + "\n".join(dept_lines))
            
            enhanced_content = "\n\n".join(content_parts)
            
            metadata = {
                'source': source,
                'weaviate_uuid': weaviate_info.get('weaviate_uuid', ''),
                'neo4j_id': neo4j_record.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'distance': weaviate_info.get('distance', 1.0),
                'enhanced': True,
                'article_title': article_title or '',
                'article_url': neo4j_record.get('article_url', ''),
                'article_domain': article_domain or '',
                'contact_count': len(valid_contacts),
                'address_count': len(valid_addresses),
                'department_count': len(valid_departments),
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ å»ºç«‹å¢å¼·æ–‡æª”å¤±æ•—: {e}")
            return Document(
                page_content=weaviate_info.get('english_summary', 'å»ºç«‹æ–‡æª”å¤±æ•—'),
                metadata={
                    'source': f'{source}_error',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'similarity': weaviate_info.get('similarity', 0),
                    'enhanced': False
                }
            )

# RAG ç³»çµ±ç‹€æ…‹å®šç¾©
class State(TypedDict):
    question: str
    original_question: str
    translated_question: str
    is_chinese_query: bool
    keywords: List[str]
    weaviate_context: List[Document]
    neo4j_context: List[Document]
    merged_context: List[Document]
    answer: str
    related_links: List[Dict]

# åˆå§‹åŒ–æ··åˆRAGç³»çµ±
hybrid_rag = HybridRAG()

# æ„åœ–æ¨ç† - ä½¿ç”¨ Mistral åˆ†æä¸¦æ¾„æ¸…ç”¨æˆ¶å•é¡Œæ„åœ–
def infer_intent(state: State):
    original_question = state["question"]
    
    print(f"ğŸ§  é–‹å§‹æ„åœ–æ¨ç†: {original_question}")
    
    try:
        from langchain_core.messages import SystemMessage, HumanMessage
        
        # æ§‹å»ºæ„åœ–æ¨ç†æç¤ºè©
        intent_prompt = f"""ä½ æ˜¯ä¸€å€‹æ™ºèƒ½å•é¡Œåˆ†æåŠ©æ‰‹ï¼Œå°ˆé–€åˆ†æç”¨æˆ¶åœ¨å¤§å­¸è³‡è¨Šç³»çµ±ä¸­çš„æŸ¥è©¢æ„åœ–ã€‚

ç”¨æˆ¶å•é¡Œï¼šã€Œ{original_question}ã€

è«‹åˆ†æé€™å€‹å•é¡Œä¸¦ä»¥ JSON æ ¼å¼å›ç­”ï¼š

{{
  "å•é¡Œé¡å‹": "æŸ¥è©¢é¡/è«®è©¢é¡/æ¯”è¼ƒé¡/å…¶ä»–",
  "æ ¸å¿ƒæ„åœ–": "ç”¨æˆ¶çœŸæ­£æƒ³è¦çš„æ ¸å¿ƒéœ€æ±‚",
  "é—œéµå¯¦é«”": ["æ¶‰åŠçš„ç³»æ‰€", "éƒ¨é–€", "æœå‹™", "äººå“¡ç­‰"],
  "æ™‚ç©ºé™åˆ¶": ["æ™‚é–“é™åˆ¶", "åœ°é»é™åˆ¶", "æ¢ä»¶é™åˆ¶ç­‰"],
  "æ¾„æ¸…å•é¡Œ": ["å¯èƒ½çš„æ˜ç¢ºè§£é‡‹1", "å¯èƒ½çš„æ˜ç¢ºè§£é‡‹2"],
  "å»ºè­°é—œéµå­—": ["æœ€é‡è¦çš„é—œéµå­—1", "é—œéµå­—2", "é—œéµå­—3"],
  "æª¢ç´¢ç­–ç•¥": "éœ€è¦çš„è³‡è¨Šé¡å‹ï¼ˆè¯çµ¡æ–¹å¼/åœ°å€/æ”¿ç­–æ–‡ä»¶ç­‰ï¼‰",
  "æœ€ä½³æŸ¥è©¢": "é‡æ–°è¡¨è¿°çš„æ˜ç¢ºå•é¡Œ"
}}

è«‹ç¢ºä¿å›å‚³æœ‰æ•ˆçš„ JSON æ ¼å¼ã€‚"""

        # èª¿ç”¨ LMStudio é€²è¡Œæ„åœ–æ¨ç†
        response = llm.invoke([
            SystemMessage(content='You are an intent analysis expert. Analyze queries and return valid JSON only.'),
            HumanMessage(content=intent_prompt)
        ])
        
        intent_result = response.content.strip()
        
        # æ¸…ç† JSON æ ¼å¼
        if intent_result.startswith('```json'):
            intent_result = intent_result.replace('```json', '').replace('```', '').strip()
        elif intent_result.startswith('```'):
            intent_result = intent_result.replace('```', '').strip()
        
        # å˜—è©¦è§£æ JSON å›æ‡‰
        try:
            intent_analysis = json.loads(intent_result)
            
            print(f"âœ… æ„åœ–æ¨ç†å®Œæˆ:")
            print(f"   ğŸ“‹ å•é¡Œé¡å‹: {intent_analysis.get('å•é¡Œé¡å‹', 'N/A')}")
            print(f"   ğŸ¯ æ ¸å¿ƒæ„åœ–: {intent_analysis.get('æ ¸å¿ƒæ„åœ–', 'N/A')}")
            print(f"   ğŸ·ï¸  é—œéµå¯¦é«”: {intent_analysis.get('é—œéµå¯¦é«”', [])}")
            print(f"   â° æ™‚ç©ºé™åˆ¶: {intent_analysis.get('æ™‚ç©ºé™åˆ¶', [])}")
            print(f"   ğŸ”‘ å»ºè­°é—œéµå­—: {intent_analysis.get('å»ºè­°é—œéµå­—', [])}")
            print(f"   ğŸ” æª¢ç´¢ç­–ç•¥: {intent_analysis.get('æª¢ç´¢ç­–ç•¥', 'N/A')}")
            print(f"   ğŸ“ æœ€ä½³æŸ¥è©¢: {intent_analysis.get('æœ€ä½³æŸ¥è©¢', 'N/A')}")
            
            # æ›´æ–°å•é¡Œç‚ºæœ€ä½³æŸ¥è©¢ç‰ˆæœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
            best_query = intent_analysis.get('æœ€ä½³æŸ¥è©¢', '')
            if best_query and best_query.strip() and best_query != original_question:
                refined_question = best_query.strip()
                print(f"ğŸ“ å•é¡Œé‡æ–°è¡¨è¿°: {original_question} â†’ {refined_question}")
            else:
                refined_question = original_question
            
            return {
                "question": refined_question,
                "original_question": original_question,
                "intent_analysis": intent_analysis,
                "refined_question": refined_question
            }
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSONè§£æå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ: {e}")
            print(f"åŸå§‹å›æ‡‰: {intent_result}")
            return {
                "question": original_question,
                "original_question": original_question,
                "intent_analysis": {},
                "refined_question": original_question
            }
            
    except Exception as e:
        print(f"âŒ æ„åœ–æ¨ç†å¤±æ•—: {e}")
        traceback.print_exc()
        return {
            "question": original_question,
            "original_question": original_question,
            "intent_analysis": {},
            "refined_question": original_question
        }
    
# å¢å¼·çš„å•é¡Œè™•ç† - æ•´åˆæ„åœ–æ¨ç†çµæœ
def enhanced_process_question(state: State):
    question = state["question"]
    intent_analysis = state.get("intent_analysis", {})
    original_question = state.get("original_question", question)
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
    is_chinese_query = is_chinese(original_question)
    
    if is_chinese_query:
        print(f"ğŸˆ¶ æª¢æ¸¬åˆ°ä¸­æ–‡æŸ¥è©¢ï¼Œæº–å‚™ç¿»è­¯...")
        try:
            translated_question = hybrid_rag.translator.translate_with_mapping(question)
            print(f"ğŸ”„ ç¿»è­¯çµæœ: {translated_question}")
        except Exception as e:
            print(f"âš ï¸ ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ: {e}")
            translated_question = question
            is_chinese_query = False
    else:
        print(f"ğŸ”¤ æª¢æ¸¬åˆ°è‹±æ–‡æŸ¥è©¢ï¼Œç›´æ¥ä½¿ç”¨")
        translated_question = question
    
    # æ“·å–é—œéµå­— - æ•´åˆæ„åœ–æ¨ç†çš„å»ºè­°
    try:
        # å…ˆå˜—è©¦ä½¿ç”¨æ„åœ–æ¨ç†çš„å»ºè­°é—œéµå­—
        suggested_keywords = intent_analysis.get("å»ºè­°é—œéµå­—", [])
        key_entities = intent_analysis.get("é—œéµå¯¦é«”", [])
        
        if suggested_keywords or key_entities:
            # çµåˆå»ºè­°é—œéµå­—å’Œé—œéµå¯¦é«”
            combined_suggested = list(set(suggested_keywords + key_entities))
            print(f"ğŸ’¡ ä½¿ç”¨æ„åœ–æ¨ç†å»ºè­°çš„é—œéµå­—: {combined_suggested}")
            keywords = [kw.strip() for kw in combined_suggested if len(kw.strip()) > 1]
        else:
            # å¾Œå‚™ï¼šä½¿ç”¨ LLM æ“·å–é—œéµå­—
            structured_llm = llm.with_structured_output(Search)
            query = structured_llm.invoke(
                f"""è«‹å¾ä½¿ç”¨è€…çš„å•é¡Œä¸­æ“·å–æœ‰æ„ç¾©çš„é—œéµè©ï¼š
                - å„ªå…ˆä¿ç•™å®Œæ•´è©çµ„ï¼Œä¾‹å¦‚ç§‘ç³»åç¨±ã€Œè³‡è¨Šå·¥ç¨‹ã€ã€æ©Ÿæ§‹åã€åœ°é»ã€æ´»å‹•åç¨±ç­‰ã€‚
                - è‹¥é‡åˆ°äººåï¼Œè«‹åªæ“·å–ç´”äººåï¼Œä¸è¦åŒ…å«è·ç¨±æˆ–èº«ä»½ç¨±å‘¼è©ã€‚
                - å…¶é¤˜é—œéµè©å¯å…©å­—ç‚ºä¸€å–®ä½ã€‚
                - æ’é™¤è™›è©ã€‚

                ä½¿ç”¨è€…å•é¡Œï¼š{original_question}"""
            )
            
            keywords = [kw.strip() for kw in query.keywords if len(kw.strip()) > 1]
            print(f"ğŸ“ LLMæ“·å–é—œéµå­—: {keywords}")
        
    except Exception as e:
        print(f"âŒ é—œéµå­—æ“·å–å¤±æ•—: {e}")
        keywords = []
    
    return {
        "question": translated_question,
        "original_question": original_question,
        "translated_question": translated_question,
        "is_chinese_query": is_chinese_query,
        "keywords": keywords,
        "intent_analysis": intent_analysis
    }

#ä¸¦è¡Œæª¢ç´¢ï¼šWeaviateèªç¾©æª¢ç´¢ + Neo4jåˆ†é¡é—œéµå­—æª¢ç´¢
def parallel_retrieve(state: State):
    translated_question = state["translated_question"]
    keywords = state["keywords"]
    
    print(f"\nğŸ”„ é–‹å§‹ä¸¦è¡Œæª¢ç´¢ï¼ˆWeaviateå«PRF + Neo4jå¢å¼·ï¼‰...")
    print(f"ğŸ“ Weaviate æŸ¥è©¢: {translated_question}")
    print(f"ğŸ”‘ Neo4j é—œéµå­—: {keywords}")
    
    import concurrent.futures
    
    weaviate_results = []
    neo4j_results = []
    
    def weaviate_search_task():
        return hybrid_rag.weaviate_search_with_prf_adaptive( 
            translated_question, limit=10, use_prf=True
        )
    
    def neo4j_search_task():
        # ä½¿ç”¨å¢å¼·çš„ Neo4j æª¢ç´¢ï¼ˆé—œéµå­—åŒ¹é… + åœ–è­œé—œè¯åˆ†æï¼‰
        return hybrid_rag.neo4j_keyword_search_enhanced(keywords, limit=10)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        weaviate_future = executor.submit(weaviate_search_task)
        neo4j_future = executor.submit(neo4j_search_task)
        
        try:
            weaviate_results = weaviate_future.result(timeout=45)  # PRF éœ€è¦æ›´å¤šæ™‚é–“
        except Exception as e:
            print(f"âš ï¸ Weaviate+PRF æª¢ç´¢å¤±æ•—: {e}")
            traceback.print_exc()
            weaviate_results = []
        
        try:
            neo4j_results = neo4j_future.result(timeout=30)
        except Exception as e:
            print(f"âš ï¸ Neo4j æª¢ç´¢å¤±æ•—: {e}")
            traceback.print_exc()
            neo4j_results = []
    
    print(f"\nğŸ“Š ä¸¦è¡Œæª¢ç´¢çµæœ:")
    print(f"ğŸ” Weaviate+PRF è·¯å¾‘: {len(weaviate_results)} ç­†çµæœ")
    print(f"ğŸ” Neo4jå¢å¼· è·¯å¾‘: {len(neo4j_results)} ç­†çµæœ")
    
    return {
        "weaviate_context": weaviate_results,
        "neo4j_context": neo4j_results
    }

#åˆä½µå’Œå»é‡å…©å€‹æª¢ç´¢è·¯å¾‘çš„çµæœ
def merge_results(state: State):
    weaviate_docs = state["weaviate_context"]
    neo4j_docs = state["neo4j_context"]
    
    print(f"\nğŸ”— é–‹å§‹åˆä½µçµæœ...")
    
    # ä½¿ç”¨URLé€²è¡Œå»é‡
    seen_urls = set()
    merged_docs = []
    
    # å„ªå…ˆåŠ å…¥ Weaviate çµæœï¼ˆèªç¾©ç›¸ä¼¼åº¦è¼ƒé«˜ï¼‰
    for doc in weaviate_docs:
        url = doc.metadata.get('article_url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged_docs.append(doc)
            print(f"âœ… åŠ å…¥Weaviateçµæœ: {doc.metadata.get('article_title', 'No Title')[:50]}")
        elif not url:  # æ²’æœ‰URLçš„ä¹ŸåŠ å…¥
            merged_docs.append(doc)
    
    # åŠ å…¥ Neo4j çµæœï¼ˆé¿å…é‡è¤‡ï¼‰
    for doc in neo4j_docs:
        url = doc.metadata.get('article_url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged_docs.append(doc)
            print(f"âœ… åŠ å…¥Neo4jçµæœ: {doc.metadata.get('article_title', 'No Title')[:50]}")
        elif not url:  # æ²’æœ‰URLçš„ä¹ŸåŠ å…¥
            merged_docs.append(doc)
    
    # æŒ‰ç›¸ä¼¼åº¦å’Œä¾†æºé€²è¡Œæ’åº
    def sort_key(doc):
        similarity = doc.metadata.get('similarity', 0)
        match_score = doc.metadata.get('match_score', 0)
        is_enhanced = doc.metadata.get('enhanced', False)
        source_priority = 0
        
        if 'Weaviate+Neo4j' in doc.metadata.get('source', ''):
            source_priority = 3  # æœ€é«˜å„ªå…ˆç´š
        elif 'Neo4j_Category' in doc.metadata.get('source', ''):
            source_priority = 2.5  # åˆ†é¡æœå°‹å„ªå…ˆæ–¼å…¨åŸŸæœå°‹
        elif 'Neo4j_Global' in doc.metadata.get('source', ''):
            source_priority = 2
        else:
            source_priority = 1
        
        # ç¶œåˆåˆ†æ•¸ï¼šä¾†æºå„ªå…ˆç´š + èªç¾©ç›¸ä¼¼åº¦ + é—œéµå­—åŒ¹é…åˆ†æ•¸
        combined_score = source_priority + similarity + match_score
        return (combined_score, is_enhanced)
    
    merged_docs.sort(key=sort_key, reverse=True)
    
    # é™åˆ¶æœ€çµ‚çµæœæ•¸é‡
    final_docs = merged_docs[:15]
    
    print(f"ğŸ¯ æœ€çµ‚åˆä½µçµæœ: {len(final_docs)} ç­†æ–‡æª”")
    
    # é¡¯ç¤ºå‰5ç­†çµæœæ¦‚è¿°
    for i, doc in enumerate(final_docs[:5], 1):
        source = doc.metadata.get('source', 'Unknown')
        similarity = doc.metadata.get('similarity', 0)
        match_score = doc.metadata.get('match_score', 0)
        title = doc.metadata.get('article_title', 'No Title')
        enhanced = doc.metadata.get('enhanced', False)
        
        print(f"  [{i}] {source} | ç›¸ä¼¼åº¦: {similarity:.3f} | åŒ¹é…: {match_score:.3f} | {'å¢å¼·' if enhanced else 'åŸºç¤'} | {title[:30]}")
    
    return {"merged_context": final_docs}

# æ··åˆRAGç”Ÿæˆå›ç­”
def generate(state: State):
    docs = state["merged_context"]
     # ===== æ–°å¢ï¼šæ”¶é›† related_links =====
    related_links = []
    seen_urls = set()
    
    for doc in docs:
        article_url = doc.metadata.get('article_url', '')
        article_title = doc.metadata.get('article_title', '')
        
        # å»é‡ä¸¦åŠ å…¥é€£çµ
        if article_url and article_url not in seen_urls:
            seen_urls.add(article_url)
            related_links.append({
                'url': article_url,
                'title': article_title or 'ç›¸é—œæ–‡ç« ',
                'source': doc.metadata.get('source', 'KGRAG'),
                'similarity': doc.metadata.get('similarity', 0),
                'match_score': doc.metadata.get('match_score', 0)
            })
    
    # é™åˆ¶é€£çµæ•¸é‡ï¼ˆä¾‹å¦‚æœ€å¤š 10 å€‹ï¼‰
    related_links = related_links[:10]
    # =====================================
    # æ•´ç†æª¢ç´¢å…§å®¹
    context_text = ""
    for i, doc in enumerate(docs, 1):
        similarity = doc.metadata.get('similarity', 0)
        match_score = doc.metadata.get('match_score', 0)
        enhanced = doc.metadata.get('enhanced', False)
        source = doc.metadata.get('source', 'Unknown')
        content = doc.page_content
        article_url = doc.metadata.get('article_url', '')
        article_title = doc.metadata.get('article_title', '')
        
        context_text += f"[è³‡æ–™ {i}] ä¾†æº: {source} (ç›¸ä¼¼åº¦: {similarity:.3f}, åŒ¹é…: {match_score:.3f})\n"
        if article_title:
            context_text += f"æ–‡ç« æ¨™é¡Œ: {article_title}\n"
        if article_url:
            context_text += f"æ–‡ç« ç¶²å€: {article_url}\n"
        context_text += f"{content}\n\n"
    
    # æ ¹æ“šæ˜¯å¦ç‚ºä¸­æ–‡æŸ¥è©¢é¸æ“‡ä¸åŒçš„æç¤ºæ¨¡æ¿
    if state["is_chinese_query"]:
        # ä¸­æ–‡æŸ¥è©¢ - ç”¨ä¸­æ–‡å›ç­”
        template = """ä½ æ˜¯åœ‹ç«‹è¯åˆå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ··åˆæª¢ç´¢è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚
        é€™äº›è³‡æ–™ä¾†è‡ªå…©å€‹æª¢ç´¢è·¯å¾‘ï¼š
        1. Weaviate ContentSummaryEn0913 å‘é‡èªç¾©æœå°‹ + Neo4j åœ–è­œå¢å¼·
        2. Neo4j åˆ†é¡é—œéµå­—æª¢ç´¢ï¼ˆå„ªå…ˆåœ¨é æ¸¬åˆ†é¡ä¸­æœå°‹ï¼‰+ å…¨åŸŸå¾Œå‚™æœå°‹

        ## æ··åˆæª¢ç´¢èˆ‡å¢å¼·è³‡æ–™ï¼š
        {context}

        ## ç”¨æˆ¶åŸå§‹å•é¡Œï¼ˆä¸­æ–‡ï¼‰ï¼š
        {original_question}

        ## ç¿»è­¯å¾Œçš„æŸ¥è©¢å•é¡Œï¼ˆè‹±æ–‡ï¼‰ï¼š
        {translated_question}

        ## é—œéµå­—åˆ†æçµæœï¼š
        æ“·å–é—œéµå­—: {keywords}

        ## å›ç­”æŒ‡å¼•ï¼š
        1. **è«‹ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œ**
        2. å„ªå…ˆä½¿ç”¨ä¾†æºæ¨™è¨»ç‚º "Weaviate+Neo4j" çš„å¢å¼·è³‡æ–™
        3. å…¶æ¬¡ä½¿ç”¨ "Neo4j_Category" çš„åˆ†é¡æœå°‹çµæœ
        4. å¦‚æœ‰ç›¸é—œç¶²å€è«‹æä¾›
        5. å¦‚æœæœ‰è¯çµ¡è³‡è¨Šï¼Œè«‹æä¾›å…·é«”çš„è¯çµ¡æ–¹å¼ï¼ˆé›»è©±ã€ä¿¡ç®±ç­‰ï¼‰
        6. å¦‚æœæœ‰åœ°å€è³‡è¨Šï¼Œè«‹æä¾›è©³ç´°çš„åœ°å€ä½ç½®
        7. å¦‚æœæœ‰éƒ¨é–€è³‡è¨Šï¼Œè«‹èªªæ˜ç›¸é—œçš„éƒ¨é–€å’Œè·è²¬
        8. å„ªå…ˆå¼•ç”¨ç›¸ä¼¼åº¦å’ŒåŒ¹é…åˆ†æ•¸è¼ƒé«˜çš„è³‡æ–™
        9. å¦‚æœè³‡æ–™ä¸è¶³ï¼Œè«‹èª å¯¦èªªæ˜ï¼Œä½†ç›¡å¯èƒ½æä¾›ç›¸é—œè³‡è¨Š
        10. å›ç­”è¦æ¢ç†æ¸…æ™°ï¼Œé‡è¦è³‡è¨Šç”¨é …ç›®ç¬¦è™Ÿåˆ—å‡º
        11. **é‡è¦ï¼šè«‹ä¸è¦åœ¨å›ç­”ä¸­åŠ å…¥ä»»ä½•è³‡æ–™ç·¨è™Ÿå¼•ç”¨**
        12. **ç›´æ¥ä½¿ç”¨è³‡æ–™å…§å®¹å›ç­”å•é¡Œï¼Œä¿æŒè‡ªç„¶æµæš¢**
        13.ä¸è¦è¼¸å‡º(è³‡æ–™4ï¼‰é€™ç¨®å…§å®¹

        ## å›ç­”ï¼š"""
    else:
        # è‹±æ–‡æŸ¥è©¢ - ç”¨è‹±æ–‡å›ç­”
        template = """You are an intelligent assistant for National United University. Please answer the user's question based on the following hybrid retrieval data.
        This data comes from two retrieval paths:
        1. Weaviate ContentSummaryEn0913 vector semantic search + Neo4j graph enhancement
        2. Neo4j category keyword search (prioritizing predicted categories) + global fallback search

        ## Hybrid Retrieval and Enhanced Data:
        {context}

        ## User Question:
        {original_question}

        ## Keywords Analysis:
        Extracted Keywords: {keywords}

        ## Answer Guidelines:
        1. **Please answer in English**
        2. Prioritize using enhanced data marked as "Weaviate+Neo4j" source
        3. Then use "Neo4j_Category" category search results
        4. Provide contact information, addresses, and department details when available
        5. Prioritize data with higher similarity and match scores
        6. If data is insufficient, explain honestly but provide relevant information
        7. Structure the answer clearly with bullet points for important information
        8. **Do not include any data source numbers in your answer**
        9. **Keep the answer natural and fluent**

        ## Answer:"""
    
    prompt = PromptTemplate.from_template(template)
    messages = prompt.invoke({
        "context": context_text,
        "original_question": state["original_question"],
        "translated_question": state.get("translated_question", state["original_question"]),
        "keywords": ", ".join(state.get("keywords", [])),
    })
    
    response = llm.invoke(messages)
    return {
        "answer": response.content,
        "related_links": related_links  # æ–°å¢
    }


# å»ºç«‹å¢å¼·çš„æ··åˆRAGæµç¨‹ - åŒ…å«æ„åœ–æ¨ç†
def create_enhanced_graph():
    enhanced_graph_builder = StateGraph(State).add_sequence([
        infer_intent,         
        enhanced_process_question,    
        parallel_retrieve, 
        merge_results,
        generate
    ])
    enhanced_graph_builder.add_edge(START, "infer_intent")
    return enhanced_graph_builder.compile()


# å¢å¼·æ··åˆRAGå•ç­”ä¸»å‡½æ•¸ - åŒ…å«æ„åœ–æ¨ç†
def ask_enhanced_hybrid_rag(question: str) -> tuple:  # ä¿®æ”¹å›å‚³é¡å‹
    """
    å›å‚³ï¼š(answer: str, related_links: List[Dict])
    """
    start_time = time.time()
    
    enhanced_graph = create_enhanced_graph()
    result = enhanced_graph.invoke({
        "question": question, 
        "original_question": question
    })
    
    end_time = time.time()
    
    print(f"â±ï¸ ç¸½è™•ç†æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’")
    
    # å›å‚³ç­”æ¡ˆå’Œé€£çµ
    return result["answer"], result.get("related_links", [])



if __name__ == "__main__":
    print(f"\n{'='*70}")
    print(f"ğŸ“ åœ‹ç«‹è¯åˆå¤§å­¸ æ··åˆRAGå•ç­”ç³»çµ± (LMStudioç‰ˆ)")
    print(f"{'='*70}")
    print(f"ğŸ“Š ç³»çµ±æ¶æ§‹: é›™è·¯ä¸¦è¡Œæª¢ç´¢ + åˆ†é¡åˆ†æ + çµæœèåˆ")
    print(f"ğŸ¤– ä¸»è¦æ¨¡å‹: LM Studio (google/gemma-3-4B)")
    print(f"ğŸ›¤ï¸  è·¯å¾‘1: Weaviateèªç¾©æª¢ç´¢ + PRF â†’ Neo4jåœ–è­œå¢å¼·")
    print(f"ğŸ›¤ï¸  è·¯å¾‘2: Neo4jåˆ†é¡é—œéµå­—æª¢ç´¢ â†’ å…¨åŸŸå¾Œå‚™æœå°‹")
    print(f"ğŸ”— çµæœèåˆ: URLå»é‡ + å¤šé‡åˆ†æ•¸æ’åº")
    print(f"{'='*70}\n")
    
    # æª¢æŸ¥ç³»çµ±ç‹€æ…‹
    weaviate_status = "âœ… æ­£å¸¸" if hybrid_rag.weaviate_client else "âŒ å¤±æ•—"
    neo4j_status = "âœ… æ­£å¸¸" if hybrid_rag.neo4j_driver else "âŒ å¤±æ•—"
    translation_file_status = "âœ… å·²è¼‰å…¥" if hybrid_rag.translator.translation_dict else "âŒ è¼‰å…¥å¤±æ•—"
    
    print(f"ğŸ”— Weaviate é€£æ¥: {weaviate_status}")
    print(f"ğŸ”— Neo4j é€£æ¥: {neo4j_status}")
    print(f"ğŸ”— LM Studio: {'âœ… æ­£å¸¸' if isinstance(llm, BaseChatModel) else 'âš ï¸ ä½¿ç”¨å‚™ç”¨æ¨¡å‹'}")
    print(f"ğŸ“„ ç¿»è­¯æ˜ å°„æª”æ¡ˆ: {translation_file_status}")
    
    
    if not hybrid_rag.weaviate_client and not hybrid_rag.neo4j_driver:
        print("\nâš ï¸ æ³¨æ„ï¼šæ‰€æœ‰æ•¸æ“šåº«é€£æ¥å¤±æ•—ï¼Œç³»çµ±å°‡é‹è¡Œåœ¨æ¼”ç¤ºæ¨¡å¼")
    elif not hybrid_rag.weaviate_client:
        print("\nâš ï¸ Weaviate é€£æ¥å¤±æ•—ï¼Œå°‡åƒ…ä½¿ç”¨ Neo4j é—œéµå­—æª¢ç´¢")
    elif not hybrid_rag.neo4j_driver:
        print("\nâš ï¸ Neo4j é€£æ¥å¤±æ•—ï¼Œå°‡åƒ…ä½¿ç”¨ Weaviate èªç¾©æª¢ç´¢")
    
    # äº’å‹•å¼å•ç­”
    try:
        while True:
            print("-" * 60)
            question = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (ä¸­æ–‡æˆ–è‹±æ–‡ï¼Œè¼¸å…¥ 'exit' é€€å‡º): ")
            
            if question.lower() in ['exit', 'quit', 'é€€å‡º', 'é›¢é–‹']:
                print("è¬è¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                break
            
            if not question.strip():
                continue
                
            print(f"\nğŸ¤– æ­£åœ¨è™•ç†...")
            try:
                answer = ask_enhanced_hybrid_rag(question)
                print(f"\nğŸ’¡ å›ç­”:\n{answer}\n")
                        
            except Exception as e:
                print(f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                traceback.print_exc()
                
    except KeyboardInterrupt:
        print(f"\n\nç¨‹å¼ä¸­æ–·ï¼Œè¬è¬ä½¿ç”¨ï¼")