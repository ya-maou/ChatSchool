#Graphrag_system_1016.py
from typing import List, TypedDict, Optional
from langchain_core.documents.base import Document
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import PromptTemplate
from langgraph.graph import START, StateGraph
import weaviate
from weaviate import connect_to_local
import weaviate.classes as wvc
import google.generativeai as genai
from neo4j import GraphDatabase
import os
import dotenv
import time
import re
import requests
import traceback
import json

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
dotenv.load_dotenv()

# è¨­å®š API Keys
GOOGLE_API_KEY = os.getenv("J_GOOGLE_API_KEY")

MISTRAL_API_KEY = os.getenv("J_MISTRAL_API_KEY")

genai.configure(api_key=GOOGLE_API_KEY)

# Setup LLM
from langchain.chat_models import init_chat_model
llm: BaseChatModel = init_chat_model(model="gemini-2.0-flash", model_provider="google_genai")

# æª¢æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
def is_chinese(text):
    chinese_pattern = re.compile(r'[\u4e00-\u9fff]+')
    return bool(chinese_pattern.search(text))

# èª¿ç”¨ Mistral API
def call_mistral(prompt, max_retries=3, base_delay=5):
    """èª¿ç”¨ Mistral APIï¼ˆåŠ å…¥ 429 éŒ¯èª¤è™•ç†ï¼‰"""
    url = "https://api.mistral.ai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {MISTRAL_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "mistral-small-latest",
        "temperature": 0.1,
        "top_p": 1,
        "max_tokens": 7800,
        "messages": [{"role": "user", "content": prompt}]
    }
    
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload)
            
            if response.status_code == 200:
                return response.json()
            elif response.status_code == 429:
                # è¶…éå®¹é‡é™åˆ¶ï¼Œç­‰å¾…å¾Œé‡è©¦
                wait_time = base_delay * (2 ** attempt)  # æŒ‡æ•¸é€€é¿
                print(f"âš ï¸ API å®¹é‡é™åˆ¶ (429)ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦... (å˜—è©¦ {attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                raise Exception(f"{response.status_code}, {response.text}")
                
        except Exception as e:
            if attempt == max_retries - 1:
                raise Exception(f"Mistral API éŒ¯èª¤ï¼š{e}")
            else:
                print(f"âš ï¸ è«‹æ±‚å¤±æ•—ï¼Œ{base_delay} ç§’å¾Œé‡è©¦...")
                time.sleep(base_delay)
    
    raise Exception("è¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")

# ä½¿ç”¨ Mistral åˆ¤æ–·æœå°‹ç­–ç•¥
def choose_search_strategy_with_mistral(question: str) -> str:  
    prompt = f"""è«‹åˆ¤æ–·ä»¥ä¸‹å•é¡Œæ‡‰è©²ä½¿ç”¨å“ªç¨®æœå°‹ç­–ç•¥ï¼š

å•é¡Œï¼š{question}

æœå°‹ç­–ç•¥èªªæ˜ï¼š
1. **Global Search (å…¨å±€æœå°‹)**ï¼š
   - é©ç”¨æ–¼éœ€è¦ç†è§£æ•´å€‹è³‡æ–™é›†ã€å¤šå€‹ä¾†æºçš„å•é¡Œ
   - ä¾‹å¦‚ï¼šã€Œå­¸æ ¡æœ‰å“ªäº›å­¸é™¢ï¼Ÿã€ã€Œæ•´é«”æ¶æ§‹å¦‚ä½•ï¼Ÿã€ã€Œæ‰€æœ‰çš„ç³»æ‰€ã€
   - éœ€è¦ç¶œåˆå¤šå€‹ç¤¾ç¾¤çš„è³‡è¨Š

2. **Local Search (å±€éƒ¨æœå°‹)**ï¼š
   - é©ç”¨æ–¼é‡å°ç‰¹å®šå¯¦é«”ã€å…·é«”ç´°ç¯€çš„å•é¡Œ
   - ä¾‹å¦‚ï¼šã€Œè³‡å·¥ç³»çš„é›»è©±ã€ã€ŒæŸæŸå¤§æ¨“çš„åœ°å€ã€ã€Œå¦‚ä½•è¯çµ¡XXéƒ¨é–€ã€
   - éœ€è¦ç²¾ç¢ºå®šä½ç‰¹å®šè³‡è¨Š

è«‹åªå›ç­” "global" æˆ– "local"ï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""
    
    try:
        response = call_mistral(prompt)
        strategy = response["choices"][0]["message"]["content"].strip().lower()
        
        # æ¸…ç†å›æ‡‰ï¼ˆç§»é™¤å¯èƒ½çš„å¼•è™Ÿæˆ–å¤šé¤˜æ–‡å­—ï¼‰
        if 'global' in strategy:
            return 'global'
        elif 'local' in strategy:
            return 'local'
        else:
            # é è¨­ä½¿ç”¨ localï¼ˆæ›´å®‰å…¨ï¼‰
            print(f"âš ï¸ Mistral å›æ‡‰ä¸æ˜ç¢º: {strategy}ï¼Œé è¨­ä½¿ç”¨ local")
            return 'local'
            
    except Exception as e:
        print(f"âŒ Mistral ç­–ç•¥åˆ¤æ–·å¤±æ•—: {e}ï¼Œé è¨­ä½¿ç”¨ local")
        return 'local'
    
# ä½¿ç”¨ Mistral æå–å¯¦é«”
def extract_entities_with_mistral(text: str) -> List[str]:
    """ä½¿ç”¨ Mistral å¾æ–‡å­—ä¸­æå–å¯¦é«”åç¨±"""
    prompt = f"""è«‹å¾ä»¥ä¸‹æ–‡å­—ä¸­æå–é‡è¦çš„å¯¦é«”åç¨±ï¼Œç‰¹åˆ¥æ˜¯ï¼š
    - å­¸æ ¡åç¨±ï¼ˆå¦‚ï¼šåœ‹ç«‹è¯åˆå¤§å­¸ï¼‰
    - å­¸é™¢åç¨±ï¼ˆå¦‚ï¼šè³‡è¨Šå­¸é™¢ï¼‰
    - ç³»æ‰€åç¨±ï¼ˆå¦‚ï¼šè³‡è¨Šå·¥ç¨‹ç³»ï¼‰
    - éƒ¨é–€åç¨±ï¼ˆå¦‚ï¼šå­¸å‹™è™•ã€æ•™å‹™è™•ï¼‰
    - ä¸­å¿ƒåç¨±ï¼ˆå¦‚ï¼šè¨ˆç®—æ©Ÿä¸­å¿ƒï¼‰
    
    æ–‡å­—ï¼š{text}
    
    è«‹åªå›å‚³å¯¦é«”åç¨±ï¼Œæ¯å€‹å¯¦é«”ä¸€è¡Œï¼Œä¸è¦å…¶ä»–èªªæ˜ï¼š"""
    
    try:
        response = call_mistral(prompt)
        entities_text = response["choices"][0]["message"]["content"].strip()
        
        # åˆ†å‰²æˆå€‹åˆ¥å¯¦é«”
        entities = [entity.strip() for entity in entities_text.split('\n') if entity.strip()]
        
        # éæ¿¾æ‰å¤ªçŸ­çš„å¯¦é«”
        filtered_entities = [e for e in entities if len(e) >= 2 and len(e) <= 20]
        
        print(f"ğŸ¤– Mistralæå–åˆ°å¯¦é«”: {filtered_entities}")
        return filtered_entities
        
    except Exception as e:
        print(f"âŒ Mistralå¯¦é«”æå–å¤±æ•—: {e}")
        return []

#é‡å° Global Search æå–åˆ†å±¤æ¦‚å¿µï¼ˆæ ¸å¿ƒ + è¼”åŠ©ï¼‰
def extract_concepts_for_global_search(text: str) -> dict:
    """
    Returns:
        {
            'core': ['æ ¸å¿ƒæ¦‚å¿µ1', 'æ ¸å¿ƒæ¦‚å¿µ2'],      # å¿…é ˆåŒ¹é…
            'auxiliary': ['è¼”åŠ©æ¦‚å¿µ1', 'è¼”åŠ©æ¦‚å¿µ2']  # åŠ åˆ†é …
        }
    """
    prompt = f"""è«‹åˆ†æä»¥ä¸‹å•é¡Œï¼Œæå–é—œéµæ¦‚å¿µä¸¦åˆ†ç‚ºå…©é¡ï¼š

å•é¡Œï¼š{text}

    è«‹åˆ†é¡ç‚ºï¼š
    1. **æ ¸å¿ƒæ¦‚å¿µ**ï¼ˆå¿…é ˆåŒ¹é…ï¼Œ1-3å€‹ï¼‰ï¼šæŸ¥è©¢çš„ä¸»è¦å°è±¡æˆ–æ ¸å¿ƒä¸»é¡Œ
    - ä¾‹å¦‚å•ã€Œæœ‰å“ªäº›ç§‘ç³»ã€â†’ æ ¸å¿ƒæ¦‚å¿µæ˜¯ã€Œç§‘ç³»ã€ã€Œå­¸ç³»ã€ã€Œå­¸é™¢ã€
    - ä¾‹å¦‚å•ã€Œå­¸ç”Ÿæœå‹™ã€â†’ æ ¸å¿ƒæ¦‚å¿µæ˜¯ã€Œæœå‹™ã€ã€Œå­¸ç”Ÿã€
    
    2. **è¼”åŠ©æ¦‚å¿µ**ï¼ˆåŠ åˆ†é …ï¼Œ2-4å€‹ï¼‰ï¼šç›¸é—œçš„é ˜åŸŸã€å±¬æ€§æˆ–èƒŒæ™¯è©å½™
    - ä¾‹å¦‚å•ã€Œæœ‰å“ªäº›ç§‘ç³»ã€â†’ è¼”åŠ©æ¦‚å¿µæ˜¯ã€Œå­¸è¡“å–®ä½ã€ã€Œæ•™è‚²ã€ã€Œæ•™å­¸ã€
    - ä¾‹å¦‚å•ã€Œå­¸ç”Ÿæœå‹™ã€â†’ è¼”åŠ©æ¦‚å¿µæ˜¯ã€Œè¡Œæ”¿ã€ã€Œæ”¯æ´ã€ã€Œè³‡æºã€

    å›ç­”æ ¼å¼ï¼ˆåš´æ ¼éµå®ˆï¼‰ï¼š
    æ ¸å¿ƒï¼šæ¦‚å¿µ1, æ¦‚å¿µ2, æ¦‚å¿µ3
    è¼”åŠ©ï¼šæ¦‚å¿µ4, æ¦‚å¿µ5

    å›ç­”ï¼š"""
    
    try:
        response = call_mistral(prompt)
        content = response["choices"][0]["message"]["content"].strip()
        
        # è§£æå›æ‡‰
        core_concepts = []
        auxiliary_concepts = []
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('æ ¸å¿ƒï¼š') or line.startswith('æ ¸å¿ƒ:'):
                core_part = line.split('ï¼š')[-1].split(':')[-1]
                core_concepts = [c.strip() for c in core_part.split(',') if c.strip()]
            elif line.startswith('è¼”åŠ©ï¼š') or line.startswith('è¼”åŠ©:'):
                aux_part = line.split('ï¼š')[-1].split(':')[-1]
                auxiliary_concepts = [c.strip() for c in aux_part.split(',') if c.strip()]
        
        # éæ¿¾é•·åº¦
        core_concepts = [c for c in core_concepts if 2 <= len(c) <= 15][:3]
        auxiliary_concepts = [c for c in auxiliary_concepts if 2 <= len(c) <= 15][:4]
        
        print(f"ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ: {core_concepts}")
        print(f"â• è¼”åŠ©æ¦‚å¿µ: {auxiliary_concepts}")
        
        return {
            'core': core_concepts,
            'auxiliary': auxiliary_concepts
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µæå–å¤±æ•—: {e}")
        # é™ç´šï¼šä½¿ç”¨åŸå§‹æ–¹æ³•
        basic_concepts = text.replace('?', '').replace('ï¼Ÿ', '').split()
        return {
            'core': basic_concepts[:3],
            'auxiliary': []
        }
# ç¿»è­¯ç®¡ç†å™¨ - æ”¯æ´å¤–éƒ¨ç¿»è­¯æª”æ¡ˆ
class TranslationManager:

    def __init__(self, translation_file_path=r"C:\Users\User\Studio\ChatSchool\CS_Project\CS_App\translate\translation_mapping.json"):
        self.translation_file = translation_file_path
        self.translation_dict = self._load_translation_dict()
    
    # ç›´æ¥å¾ self.translation_file è®€å–ç¿»è­¯æ˜ å°„æª”æ¡ˆ,ä¸å­˜åœ¨å‰‡å ±éŒ¯
    def _load_translation_dict(self) -> dict:
        try:
            with open(self.translation_file, 'r', encoding='utf-8') as f:
                mapping = json.load(f)
            print(f"âœ… æˆåŠŸè¼‰å…¥ç¿»è­¯æ˜ å°„æª”æ¡ˆ,æ¢ç›®æ•¸:{len(mapping)}")
            return mapping
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¿»è­¯æ˜ å°„æª”æ¡ˆ:{self.translation_file},è«‹å…ˆå»ºç«‹æª”æ¡ˆã€‚")
        except Exception as e:
            print(f"âŒ è¼‰å…¥ç¿»è­¯æ˜ å°„å¤±æ•—: {e}")
            return {}
    
    # å¾æ˜ å°„æª”æ¡ˆä¸­ç²å–ç¿»è­¯
    def get_translation(self, chinese_text: str) -> Optional[str]:
        return self.translation_dict.get(chinese_text)
    
    # ä½¿ç”¨æ˜ å°„æª”æ¡ˆé€²è¡Œç¿»è­¯,æœªå‘½ä¸­å‰‡ä½¿ç”¨Mistral
    def translate_with_mapping(self, text: str) -> str:
        # å…ˆæª¢æŸ¥æ˜ å°„æª”æ¡ˆ
        direct_translation = self.get_translation(text)
        if direct_translation:
            print(f"ğŸ“„ æ˜ å°„æª”æ¡ˆç¿»è­¯: {text} â†’ {direct_translation}")
            return direct_translation
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ˜ å°„ä¸­çš„è©èª
        translated_parts = []
        remaining_text = text
        
        for chinese_term, english_term in self.translation_dict.items():
            if chinese_term in remaining_text:
                remaining_text = remaining_text.replace(chinese_term, f"__TRANSLATED_{len(translated_parts)}__")
                translated_parts.append((chinese_term, english_term))
        
        if translated_parts:
            # éƒ¨åˆ†å‘½ä¸­,çµ„åˆç¿»è­¯
            final_translation = remaining_text
            for i, (chinese_term, english_term) in enumerate(translated_parts):
                final_translation = final_translation.replace(f"__TRANSLATED_{i}__", english_term)
            
            # å¦‚æœé‚„æœ‰å‰©é¤˜ä¸­æ–‡,ç”¨Mistralç¿»è­¯
            if is_chinese(final_translation):
                final_translation = self._mistral_translate(final_translation)
            
            print(f"ğŸ”„ æ··åˆç¿»è­¯: {text} â†’ {final_translation}")
            return final_translation
        
        # å®Œå…¨æœªå‘½ä¸­,ä½¿ç”¨Mistral
        return self._mistral_translate(text)
    
    # ä½¿ç”¨Mistralé€²è¡Œç¿»è­¯
    def _mistral_translate(self, text: str) -> str:
        try:
            response = call_mistral(f"è«‹å°‡ä»¥ä¸‹ä¸­æ–‡ç¿»è­¯æˆè‹±æ–‡,åªå›å‚³ç¿»è­¯çµæœ:{text}")
            translated = response["choices"][0]["message"]["content"].strip()
            if translated.startswith('"') and translated.endswith('"'):
                translated = translated[1:-1]
            print(f"ğŸ¤– Mistralç¿»è­¯: {text} â†’ {translated}")
            return translated
        except Exception as e:
            print(f"âŒ Mistralç¿»è­¯å¤±æ•—: {e}")
            return text

# åˆå§‹åŒ–ç¿»è­¯ç®¡ç†å™¨
translation_manager = TranslationManager()

# ä½¿ç”¨ç¿»è­¯ç®¡ç†å™¨é€²è¡Œç¿»è­¯ï¼ˆæ˜ å°„æª”æ¡ˆå„ªå…ˆï¼Œç„¶å¾ŒMistralï¼‰
def translate_with_mistral(text: str, max_retries=3, delay=2) -> str:
    for attempt in range(max_retries):
        try:
            return translation_manager.translate_with_mapping(text)
        except Exception as e:
            print(f"âŒ [ç¿»è­¯å¤±æ•—] ç¬¬ {attempt + 1} æ¬¡å˜—è©¦ï¼ŒéŒ¯èª¤ï¼š{e}")
            if attempt < max_retries - 1:
                time.sleep(delay)
    raise Exception("ç¿»è­¯å¤±æ•—ï¼Œè¶…éæœ€å¤§é‡è©¦æ¬¡æ•¸")

# Gemini åµŒå…¥å™¨
class GeminiEmbedder:
    def embed_query(self, text: str) -> List[float]:
        try:
            response = genai.embed_content(
                model="models/text-embedding-004",
                content=text
            )
            return response['embedding']
        except Exception as e:
            print(f"Error generating embedding: {e}")
            return [0.0] * 768  

# åˆå§‹åŒ– Weaviate å’Œ Neo4j é€£æ¥
class ContentSummaryEn0913RAG:
    
    def __init__(self):
        try:
            # åˆå§‹åŒ– Weaviate é€£æ¥
            self.client = connect_to_local()
            print("âœ… æˆåŠŸé€£æ¥åˆ° Weaviate")
            
            # åˆå§‹åŒ– Neo4j é€£æ¥
            self.neo4j_driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "neo4j0804"),
                database="nuu-data-0804"
            )
            print("âœ… æˆåŠŸé€£æ¥åˆ° Neo4j")
            
            self.triplet_driver = GraphDatabase.driver(
                "bolt://localhost:7687",
                auth=("neo4j", "neo4j0804"),
                database="nuu-triplet"
            )
            print("âœ… æˆåŠŸé€£æ¥åˆ° Neo4j ä¸‰å…ƒçµ„åœ–è­œ")
            
            # åˆå§‹åŒ–åµŒå…¥å™¨
            self.embedder = GeminiEmbedder()
            print("âœ… åˆå§‹åŒ– Gemini åµŒå…¥å™¨")
            
            self._check_community_status()
            
            # åªè¼‰å…¥ ContentSummaryEn0913 é›†åˆ
            try:
                self.collection = self.client.collections.get("ContentSummaryEn0913")
                self.text_field = "english_summary"
                print(f"ğŸ“š å·²è¼‰å…¥ ContentSummaryEn0913 é›†åˆ (æ–‡æœ¬æ¬„ä½: {self.text_field})")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•è¼‰å…¥ ContentSummaryEn0913 é›†åˆ: {e}")
                self.collection = None
            
        except Exception as e:
            print(f"âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•—: {e}")
            self.client = None
            self.neo4j_driver = None
            self.embedder = None
            self.collection = None
    
    #æª¢æŸ¥ç¤¾ç¾¤è³‡æ–™æ˜¯å¦å·²å»ºæ§‹
    def _check_community_status(self):
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run("""
                    MATCH (c:Community)
                    WHERE c.summary IS NOT NULL
                    RETURN count(c) as count
                """)
                count = result.single()['count']
                
                if count == 0:
                    print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°ç¤¾ç¾¤æ‘˜è¦è³‡æ–™")
                    print("   è«‹å…ˆåŸ·è¡Œ build_communities.py")
                else:
                    print(f"âœ… å·²è¼‰å…¥ {count} å€‹ç¤¾ç¾¤æ‘˜è¦")
        except:
            print("âš ï¸ ç„¡æ³•æª¢æŸ¥ç¤¾ç¾¤ç‹€æ…‹")
    # ä½¿ç”¨ Mistral åˆ¤æ–·æœ€ä½³æª¢ç´¢å±¤ç´šï¼ˆæ”¹é€²ç‰ˆæç¤ºè©ï¼‰
    def _determine_best_level_with_mistral(self, query: str) -> int:
        """
        ä¿®æ”¹é»ï¼š
        1. âœ… æ›´è©³ç´°çš„å±¤ç´šèªªæ˜å’Œç¯„ä¾‹
        2. âœ… åŠ å…¥åˆ¤æ–·ç­–ç•¥ï¼ˆé è¨­ Level 1ï¼‰
        3. âœ… æ¸…æ™°çš„æ ¼å¼è¦æ±‚
        
        Returns:
            0: åº•å±¤ï¼ˆå…·é«”å¯¦é«”ç´°ç¯€ï¼‰
            1: ä¸­å±¤ï¼ˆé¡åˆ¥ã€é ˜åŸŸå±¤ç´šï¼‰- è«–æ–‡é¡¯ç¤ºé€šå¸¸æœ€ä½³ â­
            2: é«˜å±¤ï¼ˆå…¨å±€ã€æ¶æ§‹å±¤ç´šï¼‰
        """
        prompt = f"""ä½ æ˜¯ä¸€å€‹çŸ¥è­˜åœ–è­œæŸ¥è©¢å°ˆå®¶ã€‚è«‹æ ¹æ“šå•é¡Œçš„**è³‡è¨Šç¯„åœå’Œç´°ç¯€ç¨‹åº¦**ï¼Œé¸æ“‡æœ€é©åˆçš„ç¤¾ç¾¤æ‘˜è¦å±¤ç´šã€‚

        ## ä½¿ç”¨è€…å•é¡Œ
        {query}

        ## å±¤ç´šå®šç¾©èˆ‡é—œéµåˆ¤æ–·

        ### Level 0 (åº•å±¤ - å…·é«”å¯¦é«”)
        **é—œéµï¼š** éœ€è¦**ç²¾ç¢ºæ•¸æ“š**ï¼ˆé›»è©±ã€åœ°å€ã€æ™‚é–“ç­‰ï¼‰æˆ–**å–®ä¸€å¯¦é«”**çš„è©³ç´°è³‡è¨Šã€‚
        **ç¯„ä¾‹ï¼š** ã€ŒXXç³»çš„é›»è©±ï¼Ÿã€
        **æ±ºç­–æ¢ä»¶ï¼š** åŒ…å«æ˜ç¢ºå¯¦é«”å + è«‹æ±‚ç²¾ç¢ºæ•¸æ“šï¼ˆé›»è©±/åœ°å€/æ™‚é–“ï¼‰ã€‚

        ### Level 1 (ä¸­å±¤ - é ˜åŸŸé¡åˆ¥) â­ é è¨­å±¤ç´š
        **é—œéµï¼š** éœ€è¦**åˆ—èˆ‰**ã€**æ¯”è¼ƒ**æˆ–**ç¸½çµ**æŸä¸€**ç‰¹å®šé ˜åŸŸ/é¡åˆ¥**çš„è³‡è¨Šã€‚
        **ç¯„ä¾‹ï¼š** ã€Œå­¸æ ¡æœ‰å“ªäº›å·¥ç¨‹ç›¸é—œçš„ç³»æ‰€ï¼Ÿã€
        **æ±ºç­–æ¢ä»¶ï¼š** ç¯„åœä¸­ç­‰ï¼Œèšç„¦ç‰¹å®šé ˜åŸŸ/é¡åˆ¥ï¼Œæ˜¯**é 0 é 2** æ™‚çš„é è¨­é¸æ“‡ã€‚

        ### Level 2 (é«˜å±¤ - å…¨å±€æ¶æ§‹)
        **é—œéµï¼š** éœ€è¦**æ•´é«”è¦–è§’**ã€**è·¨é ˜åŸŸç¶œåˆ**æˆ–**å…¨é¢ç¸½è¦½**ã€‚
        **ç¯„ä¾‹ï¼š** ã€Œå­¸æ ¡çš„æ•´é«”çµ„ç¹”æ¶æ§‹æ˜¯ä»€éº¼ï¼Ÿã€
        **æ±ºç­–æ¢ä»¶ï¼š** ä½¿ç”¨ã€Œæ•´é«”ã€ã€Œæ‰€æœ‰ã€ã€Œå…¨éƒ¨ã€ã€Œç¸½å…±ã€ç­‰å…¨å±€æ€§è©å½™ã€‚

        ---

        ## å›ç­”æ ¼å¼èˆ‡ç­–ç•¥ï¼ˆé‡è¦ï¼ï¼‰

        1. **å›ç­”ç­–ç•¥ï¼š** Level 1 ç‚º GraphRAG è«–æ–‡é©—è­‰çš„**æœ€ä½³é è¨­å±¤ç´š**ã€‚åªæœ‰ç•¶å•é¡Œæ˜ç¢ºç¬¦åˆ Level 0 æˆ– Level 2 çš„æ¢ä»¶æ™‚æ‰æ”¹è®Šã€‚
        2. **æ ¼å¼è¦æ±‚ï¼š** è«‹åªå›ç­”ä¸€å€‹æ•¸å­—ï¼š0ã€1 æˆ– 2ã€‚ä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—èªªæ˜ã€‚

        åˆ¤æ–·çµæœï¼š"""
        
        try:
            response = call_mistral(prompt)
            level_str = response["choices"][0]["message"]["content"].strip()
            
            # æå–æ•¸å­—
            import re
            match = re.search(r'[0-2]', level_str)
            if match:
                level = int(match.group())
                
                # è¨˜éŒ„åˆ¤æ–·çµæœï¼ˆåŠ å…¥ä¸­æ–‡èªªæ˜ï¼‰
                level_names = {
                    0: "åº•å±¤ï¼ˆå…·é«”ç´°ç¯€ï¼‰",
                    1: "ä¸­å±¤ï¼ˆé ˜åŸŸé¡åˆ¥ï¼‰â­ é è¨­æœ€ä½³",
                    2: "é«˜å±¤ï¼ˆå…¨å±€æ¶æ§‹ï¼‰"
                }
                print(f"ğŸ¯ Mistral é¸æ“‡å±¤ç´š: Level {level} - {level_names[level]}")
                
                return level
            else:
                print(f"âš ï¸ Mistral å›æ‡‰ä¸æ˜ç¢º: {level_str}")
                print(f"   **é è¨­ä½¿ç”¨ Level 1ï¼ˆè«–æ–‡è­‰æ˜çš„æœ€ä½³å±¤ç´šï¼‰**")
                return 1  # é è¨­ Level 1ï¼ˆè€Œé 0ï¼‰
                
        except Exception as e:
            print(f"âŒ Mistral å±¤ç´šåˆ¤æ–·å¤±æ•—: {e}")
            print(f"   **é è¨­ä½¿ç”¨ Level 1ï¼ˆè«–æ–‡è­‰æ˜çš„æœ€ä½³å±¤ç´šï¼‰**")
            return 1  # é è¨­ Level 1
    # æ ¹æ“šæŸ¥è©¢æ‰¾å‡ºç›¸é—œç¤¾ç¾¤ï¼ˆæ”¹é€²ç‰ˆï¼šæ”¯æ´æ™ºèƒ½å±¤ç´š + åˆ†å±¤æ¦‚å¿µï¼‰
    def find_relevant_communities(self, query: str, limit: int = None, level: int = None) -> List[dict]:
        """
        Args:
            query: æŸ¥è©¢å•é¡Œ
            limit: è¿”å›çš„ç¤¾ç¾¤æ•¸é‡ï¼ˆNone = è‡ªå‹•æ ¹æ“šå±¤ç´šæ±ºå®šï¼‰
            level: æŒ‡å®šæœå°‹çš„å±¤ç´šï¼ˆNone = è‡ªå‹•é¸æ“‡æœ€ä½³å±¤ç´šï¼‰
        """
        if not self.triplet_driver:
            return []
        
        # 1. æ™ºèƒ½é¸æ“‡å±¤ç´š
        if level is None:
            level = self._determine_best_level_with_mistral(query)
        else:
            print(f"ğŸ¯ æ‰‹å‹•æŒ‡å®šå±¤ç´š: Level {level}")
        
        # 2. æ ¹æ“šå±¤ç´šæ±ºå®šæª¢ç´¢æ•¸é‡
        if limit is None:
            if level >= 2:
                limit = 5   # é«˜å±¤ç´šç¤¾ç¾¤è¼ƒå°‘
            elif level == 1:
                limit = 8   # ä¸­å±¤ç´šéœ€è¦æ›´å¤š
            else:
                limit = 15  # åº•å±¤ç´šå¯èƒ½éœ€è¦å¾ˆå¤š
        
        # 3. æå–åˆ†å±¤æ¦‚å¿µ
        concepts_dict = extract_concepts_for_global_search(query)
        core_concepts = concepts_dict.get('core', [])
        auxiliary_concepts = concepts_dict.get('auxiliary', [])
        
        if not core_concepts:
            print("âš ï¸ æœªæå–åˆ°æ ¸å¿ƒæ¦‚å¿µï¼Œä½¿ç”¨é—œéµè©åŒ¹é…")
            return self._find_communities_by_keywords(query, limit)
        
        # 4. å®šç¾©å­¸æ ¡ç›¸é—œé—œéµè©
        school_keywords = ['åœ‹ç«‹è¯åˆå¤§å­¸', 'National United University', 'è¯å¤§', 'NUU']
        
        # 5. æ ¹æ“šå±¤ç´šèª¿æ•´æŸ¥è©¢ç­–ç•¥
        if level >= 2:
            # é«˜å±¤ç´šï¼šåªè¦æ ¸å¿ƒæ¦‚å¿µåŒ¹é…ï¼Œå­¸æ ¡é—œéµè©å¯é¸
            where_clause = "WHERE core_matches > 0"
            order_clause = "ORDER BY c.level ASC, core_matches DESC, aux_matches DESC, school_matches DESC"
        elif level == 1:
            # ä¸­å±¤ç´šï¼šæ ¸å¿ƒæ¦‚å¿µå¿…é ˆï¼Œå­¸æ ¡é—œéµè©åŠ åˆ†
            where_clause = "WHERE core_matches > 0"
            order_clause = "ORDER BY c.level ASC, core_matches DESC, school_matches DESC, aux_matches DESC"
        else:
            # åº•å±¤ç´šï¼šæ ¸å¿ƒæ¦‚å¿µ + å­¸æ ¡é—œéµè©éƒ½è¦åŒ¹é…
            where_clause = "WHERE core_matches > 0 AND school_matches > 0"
            order_clause = "ORDER BY c.level ASC, school_matches DESC, core_matches DESC, aux_matches DESC"
        
        # 6. æ§‹å»ºæŸ¥è©¢
        query_cypher = f"""
        MATCH (c:Community {{level: $level}})
        WHERE c.summary IS NOT NULL
        
        WITH c, $core_concepts as core_concepts, 
            $aux_concepts as aux_concepts, 
            $school_keywords as school_keywords
        
        // 
        WITH c, 
            SIZE([core IN core_concepts WHERE c.summary CONTAINS core]) as core_matches,
            SIZE([aux IN aux_concepts WHERE c.summary CONTAINS aux]) as aux_matches,
            SIZE([school IN school_keywords WHERE c.summary CONTAINS school]) as school_matches
        
        {where_clause}
        
        RETURN 
            c.id as community_id,
            c.level as level,
            c.summary as summary,
            c.entity_count as entity_count,
            core_matches as core_relevance,
            aux_matches as aux_relevance,
            school_matches as school_relevance
        {order_clause}, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(
                    query_cypher,
                    core_concepts=core_concepts,
                    aux_concepts=auxiliary_concepts,
                    school_keywords=school_keywords,
                    level=level,
                    limit=limit
                )
                communities = [dict(record) for record in result]
                
                if not communities:
                    print(f"âš ï¸ Level {level} ç„¡åŒ¹é…çµæœï¼Œå˜—è©¦å›é€€ç­–ç•¥...")
                    return self._fallback_multi_level_search(query, level, limit)
                
                print(f"âœ… åœ¨ Level {level} æ‰¾åˆ° {len(communities)} å€‹ç›¸é—œç¤¾ç¾¤")
                for i, comm in enumerate(communities[:3], 1):
                    preview = comm['summary'][:80] + "..." if len(comm['summary']) > 80 else comm['summary']
                    print(f"  [{i}] ç¤¾ç¾¤ {comm['community_id']} (æ ¸å¿ƒ:{comm['core_relevance']}, è¼”åŠ©:{comm['aux_relevance']}, å­¸æ ¡:{comm['school_relevance']})")
                    print(f"      {preview}")
                
                return communities
                
        except Exception as e:
            print(f"âŒ æŸ¥è©¢ç›¸é—œç¤¾ç¾¤å¤±æ•—: {e}")
            traceback.print_exc()
            return []
    #ç•¶æŒ‡å®šå±¤ç´šç„¡çµæœæ™‚ï¼Œå˜—è©¦ç›¸é„°å±¤ç´š
    def _fallback_multi_level_search(self, query: str, original_level: int, limit: int) -> List[dict]:
        print(f"ğŸ”„ åŸ·è¡Œå¤šå±¤ç´šå›é€€æœå°‹ï¼ˆåŸå±¤ç´š: {original_level}ï¼‰")
        
        # å®šç¾©å›é€€é †åº
        if original_level == 2:
            fallback_levels = [1, 0]
        elif original_level == 1:
            fallback_levels = [2, 0]
        else:  # original_level == 0
            fallback_levels = [1, 2]
        
        all_communities = []
        
        for level in fallback_levels:
            print(f"   å˜—è©¦ Level {level}...")
            
            concepts_dict = extract_concepts_for_global_search(query)
            core_concepts = concepts_dict.get('core', [])
            auxiliary_concepts = concepts_dict.get('auxiliary', [])
            school_keywords = ['åœ‹ç«‹è¯åˆå¤§å­¸', 'National United University', 'è¯å¤§', 'NUU']
            
            # æ”¾å¯¬æ¢ä»¶ï¼šåªè¦æ ¸å¿ƒæ¦‚å¿µåŒ¹é…å³å¯
            query_cypher = """
            MATCH (c:Community {level: $level})
            WHERE c.summary IS NOT NULL
            
            WITH c, $core_concepts as core_concepts
            WITH c, 
                SIZE([core IN core_concepts WHERE c.summary CONTAINS core]) as core_matches
            
            WHERE core_matches > 0
            
            RETURN 
                c.id as community_id,
                c.level as level,
                c.summary as summary,
                c.entity_count as entity_count,
                core_matches as core_relevance
            ORDER BY core_matches DESC, c.entity_count DESC
            LIMIT $limit
            """
            
            try:
                with self.triplet_driver.session(database="nuu-triplet") as session:
                    result = session.run(
                        query_cypher,
                        core_concepts=core_concepts,
                        level=level,
                        limit=max(3, limit // 2)  # æ¯å±¤ç´šå–è¼ƒå°‘æ•¸é‡
                    )
                    level_communities = [dict(record) for record in result]
                    
                    if level_communities:
                        print(f"      âœ… Level {level} æ‰¾åˆ° {len(level_communities)} å€‹ç¤¾ç¾¤")
                        all_communities.extend(level_communities)
                        
                        # å¦‚æœå·²ç¶“æ‰¾åˆ°è¶³å¤ æ•¸é‡ï¼Œåœæ­¢å›é€€
                        if len(all_communities) >= 3:
                            break
            except Exception as e:
                print(f"      âŒ Level {level} æŸ¥è©¢å¤±æ•—: {e}")
                continue
        
        # å»é‡ï¼ˆæ ¹æ“š community_idï¼‰
        seen_ids = set()
        unique_communities = []
        for comm in all_communities:
            if comm['community_id'] not in seen_ids:
                seen_ids.add(comm['community_id'])
                unique_communities.append(comm)
        
        print(f"âœ… å¤šå±¤ç´šå›é€€å®Œæˆï¼Œç¸½å…±æ‰¾åˆ° {len(unique_communities)} å€‹ç¤¾ç¾¤")
        return unique_communities[:limit]
    # é™ç´šæŸ¥è©¢ï¼šåªåŒ¹é…æ¦‚å¿µï¼Œä¸éæ¿¾å­¸æ ¡
    def _find_communities_without_school_filter(self, concepts: List[str], limit: int) -> List[dict]:
        query_cypher = """
        MATCH (c:Community)
        WHERE c.summary IS NOT NULL
        WITH c, $concepts as concepts
        WITH c, 
            SIZE([concept IN concepts WHERE c.summary CONTAINS concept]) as concept_matches
        WHERE concept_matches > 0
        RETURN 
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count,
            concept_matches as relevance
        ORDER BY relevance DESC, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query_cypher, concepts=concepts, limit=limit)
                communities = [dict(record) for record in result]
                print(f"âš ï¸ é™ç´šæŸ¥è©¢æ‰¾åˆ° {len(communities)} å€‹ç¤¾ç¾¤ï¼ˆæœªéæ¿¾å­¸æ ¡ï¼‰")
                return communities
        except Exception as e:
            print(f"âŒ é™ç´šæŸ¥è©¢å¤±æ•—: {e}")
            return []
    
    # ä½¿ç”¨é—œéµè©åŒ¹é…æ‰¾å‡ºç›¸é—œç¤¾ç¾¤ï¼ˆæ”¹é€²ç‰ˆï¼‰
    def _find_communities_by_keywords(self, query: str, limit: int = 5) -> List[dict]:
        
        # å¾å•é¡Œä¸­æå–å¤šå€‹å¯èƒ½çš„é—œéµè©
        words = query.replace('?', '').replace('?', '').split()
        keywords = [w for w in words if len(w) >= 2][:5]  # å–æœ€å¤š5å€‹é—œéµè©
        
        if not keywords:
            keywords = [query[:10]]  # å¦‚æœæ²’æœ‰é—œéµè©å°±ç”¨å‰10å€‹å­—
        
        print(f"ğŸ”‘ ä½¿ç”¨é—œéµè©: {keywords}")
        
        query_cypher = """
        MATCH (c:Community)
        WHERE c.summary IS NOT NULL
        WITH c, $keywords as keywords
        WITH c,
            SIZE([kw IN keywords WHERE c.summary CONTAINS kw]) as match_count
        WHERE match_count > 0
        RETURN 
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count,
            match_count as relevance
        ORDER BY relevance DESC, c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query_cypher, keywords=keywords, limit=limit)
                communities = [dict(record) for record in result]
                
                if communities:
                    print(f"âœ… é—œéµè©åŒ¹é…æ‰¾åˆ° {len(communities)} å€‹ç¤¾ç¾¤")
                
                return communities
                
        except Exception as e:
            print(f"âŒ é—œéµè©åŒ¹é…å¤±æ•—: {e}")
            return []

    def _get_communities_for_entities(self, entities: List[str], limit: int = 3) -> List[dict]:
        """æ ¹æ“šå¯¦é«”ç²å–ç›¸é—œç¤¾ç¾¤æ‘˜è¦ï¼ˆç”¨æ–¼æ–‡æª”å¢å¼·ï¼‰"""
        if not entities:
            return []
        
        query = """
        UNWIND $entities AS entity_name
        MATCH (e:Entity)
        WHERE e.name CONTAINS entity_name OR e.name = entity_name
        WITH DISTINCT e.communityId as commId
        MATCH (c:Community {id: commId})
        WHERE c.summary IS NOT NULL
        RETURN DISTINCT
            c.id as community_id,
            c.summary as summary,
            c.entity_count as entity_count
        ORDER BY c.entity_count DESC
        LIMIT $limit
        """
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                result = session.run(query, entities=entities, limit=limit)
                return [dict(record) for record in result]
        except Exception as e:
            print(f"âŒ ç²å–ç¤¾ç¾¤æ‘˜è¦å¤±æ•—: {e}")
            return []
            
    # æ­¥é©Ÿ1: å¾Weaviate ContentSummaryEn0913æœå°‹ï¼Œæ­¥é©Ÿ2: ç”¨neo4j_idå»Neo4jæŸ¥è©¢å®Œæ•´è³‡è¨Š
    def search(self, query: str, limit: int = 10) -> List[Document]:
        if not self.client or not self.embedder or not self.collection or not self.neo4j_driver:
            return [Document(page_content="âŒ ç³»çµ±é€£æ¥å¤±æ•—", metadata={})]
        
        try:
            # æ­¥é©Ÿ 1: Weaviate å‘é‡æœå°‹
            print(f"ğŸ” æ­¥é©Ÿ1: åœ¨ Weaviate ContentSummaryEn0913 ä¸­é€²è¡Œå‘é‡æœå°‹...")
            query_vector = self.embedder.embed_query(query)
            weaviate_results = self._weaviate_vector_search(query_vector, limit)
            
            if not weaviate_results:
                print("âŒ Weaviate æœå°‹ç„¡çµæœ")
                return [Document(page_content="æœå°‹ç„¡çµæœ", metadata={})]
            
            # æ­¥é©Ÿ 2: æå– neo4j_id ä¸¦æŸ¥è©¢ Neo4j
            neo4j_ids = []
            weaviate_data = {}
            
            for result in weaviate_results:
                neo4j_id = result['neo4j_id']
                if neo4j_id:
                    neo4j_ids.append(neo4j_id)
                    weaviate_data[neo4j_id] = result
            
            if not neo4j_ids:
                print("âš ï¸ æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ neo4j_id")
                return [Document(page_content="æ²’æœ‰æ‰¾åˆ°å°æ‡‰çš„Neo4jè³‡æ–™", metadata={})]
            
            print(f"ğŸ” æ­¥é©Ÿ2: ä½¿ç”¨ {len(neo4j_ids)} å€‹ neo4j_id æŸ¥è©¢ Neo4j...")
            enhanced_documents = self._query_neo4j_by_ids(neo4j_ids, weaviate_data, query=query)  # â† å‚³å…¥ query
            
            print(f"âœ… ç¸½å…±ç”Ÿæˆ {len(enhanced_documents)} ç­†å¢å¼·æ–‡æª”")
            return enhanced_documents
        except Exception as e:
            print(f"âŒ æœå°‹éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            return [Document(page_content="æœå°‹å¤±æ•—", metadata={})]
        
    # å¾ Weaviate ContentSummaryEn0913 åŸ·è¡Œå‘é‡æœå°‹ï¼Œè¿”å›ç›¸é—œåº¦æœ€é«˜çš„çµæœ
    def _weaviate_vector_search(self, query_vector: List[float], limit: int) -> List[dict]:
        results = []
        
        try:
            response = self.collection.query.near_vector(
                near_vector=query_vector,
                limit=limit,
                return_metadata=wvc.query.MetadataQuery(distance=True)
            )
            
            for obj in response.objects:
                # ç²å– english_summary å…§å®¹
                content = obj.properties.get(self.text_field, '')
                if not content or content.strip() == "":
                    continue
                
                # è¨ˆç®—ç›¸ä¼¼åº¦åˆ†æ•¸
                distance = float(obj.metadata.distance) if obj.metadata.distance is not None else 1.0
                similarity = max(0, 1.0 - distance)
                
                # æ”¶é›†çµæœ
                result = {
                    'weaviate_uuid': str(obj.uuid),
                    'english_summary': content,
                    'neo4j_id': obj.properties.get('neo4j_id', ''),
                    'article_url': obj.properties.get('article_url', ''),
                    'language': obj.properties.get('language', 'en'),
                    'similarity': similarity,
                    'distance': distance
                }
                results.append(result)
            
            print(f"âœ… Weaviate æœå°‹æˆåŠŸï¼Œæ‰¾åˆ° {len(results)} ç­†çµæœ")
            
            # é¡¯ç¤ºå‰3ç­†çµæœçš„é è¦½
            for i, result in enumerate(results[:3], 1):
                preview = result['english_summary'][:100] + "..." if len(result['english_summary']) > 100 else result['english_summary']
                print(f"  [{i}] ç›¸ä¼¼åº¦: {result['similarity']:.3f} | Neo4j ID: {result['neo4j_id']} | {preview}")
            
            return results
            
        except Exception as e:
            print(f"âŒ Weaviate å‘é‡æœå°‹å¤±æ•—: {e}")
            return []
    
    # å„ªåŒ–ç‰ˆæœ¬ï¼šåˆ†éšæ®µæŸ¥è©¢ï¼Œé¿å…è¤‡é›œçš„ collect æ“ä½œ
    def _query_neo4j_by_ids(self, neo4j_ids: List[str], weaviate_data: dict, query: str = None) -> List[Document]:
        if not self.neo4j_driver or not neo4j_ids:
            return []
        
        enhanced_documents = []
        

        for neo4j_id in neo4j_ids:
            print(f"ğŸ” æŸ¥è©¢ Neo4j ID: {neo4j_id}")
            start_time = time.time()
            
            try:
                # åˆ†éšæ®µæŸ¥è©¢ç­–ç•¥
                content_data = self._get_content_info(neo4j_id)
                if not content_data:
                    print(f"âš ï¸ Content è³‡æ–™ä¸å­˜åœ¨: {neo4j_id}")
                    continue
                
                article_data = self._get_article_info(neo4j_id)
                org_data = self._get_organization_info(neo4j_id)
                
                # ã€ä¿®æ”¹ã€‘åŠ å…¥ä¸‰å…ƒçµ„åœ–è­œå¢å¼·
                triplet_data = self._enhance_with_triplet_graph(
                    neo4j_id, 
                    content_data.get('content_text', ''), 
                    article_data.get('article_title', ''),
                    query=query
                )

                # åˆä½µè³‡æ–™
                combined_data = {**content_data, **article_data, **org_data, **triplet_data}
                #  å»ºç«‹å¢å¼·æ–‡æª”
                weaviate_info = weaviate_data.get(neo4j_id, {})
                enhanced_doc = self._create_enhanced_document_optimized(combined_data, weaviate_info)
                enhanced_documents.append(enhanced_doc)
            
                
                elapsed = time.time() - start_time
                print(f"âœ… æŸ¥è©¢å®Œæˆ ({elapsed:.2f}s)")
                
            except Exception as e:
                elapsed = time.time() - start_time
                print(f"âŒ æŸ¥è©¢å¤±æ•— ({elapsed:.2f}s): {e}")
                
                # ä½¿ç”¨ Weaviate å‚™æ¡ˆ
                weaviate_info = weaviate_data.get(neo4j_id, {})
                if weaviate_info:
                    fallback_doc = Document(
                        page_content=weaviate_info.get('english_summary', ''),
                        metadata={
                            'source': 'Weaviate_fallback',
                            'neo4j_id': neo4j_id,
                            'similarity': weaviate_info.get('similarity', 0),
                            'enhanced': False
                        }
                    )
                    enhanced_documents.append(fallback_doc)
        
        return enhanced_documents
    
    # ç²å– Content åŸºæœ¬è³‡è¨Š
    def _get_content_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        RETURN 
            content.neo4j_id as content_neo4j_id,
            content.id as content_id,
            content.text as content_text,
            content.type as content_type,
            content.order as content_order
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Content æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å– Article è³‡è¨Š
    def _get_article_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        RETURN 
            article.neo4j_id as article_neo4j_id,
            article.url as article_url,
            article.title as article_title,
            article.domain as article_domain,
            article.og_image as article_og_image
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Article æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # ç²å–çµ„ç¹”è³‡è¨Š - ç°¡åŒ–ç‰ˆæœ¬
    def _get_organization_info(self, neo4j_id: str) -> dict:
        query = """
        MATCH (content:Content {neo4j_id: $neo4j_id})
        OPTIONAL MATCH (article:Article)-[:HAS_CONTENT]->(content)
        OPTIONAL MATCH (article)-[:BELONGS_TO]->(org:Organization)
        RETURN 
            org.name as org_name,
            org.type as org_type,
            org.unified_number as org_unified_number
        LIMIT 1
        """
        
        try:
            with self.neo4j_driver.session() as session:
                result = session.run(query, neo4j_id=neo4j_id)
                record = result.single()
                return dict(record) if record else {}
        except Exception as e:
            print(f"âŒ Organization æŸ¥è©¢å¤±æ•—: {e}")
            return {}
        
    # é€šéä¸‰å…ƒçµ„åœ–è­œå¢å¼·è³‡æ–™ï¼ˆå„ªå…ˆä½¿ç”¨å•é¡Œç›¸é—œå¯¦é«”ï¼‰
    def _enhance_with_triplet_graph(self, neo4j_id: str, content_text: str, article_title: str, query: str = None):
        if not self.triplet_driver:
            return {'triplet_relations': [], 'entity_count': 0}
        
        try:
            # ========== éšæ®µ 1: å¾å•é¡Œä¸­æŠ½å–å¯¦é«” ==========
            query_entities = []
            if query:
                print(f"   ğŸ” å¾å•é¡Œä¸­æŠ½å–å¯¦é«”...")
                query_entities = extract_entities_with_mistral(query)
                print(f"   å•é¡Œå¯¦é«”: {query_entities}")
            
            # ========== éšæ®µ 2: å¾æ–‡æª”ä¸­æŠ½å–å¯¦é«” ==========
            text_for_extraction = ""
            if article_title:
                text_for_extraction += article_title + " "
            if content_text:
                text_for_extraction += content_text[:200]
            
            if not text_for_extraction.strip():
                return {'triplet_relations': [], 'entity_count': 0}
            
            print(f"   ğŸ“„ å¾æ–‡æª”ä¸­æŠ½å–å¯¦é«”...")
            doc_entities = extract_entities_with_mistral(text_for_extraction)
            print(f"   æ–‡æª”å¯¦é«”: {doc_entities}")
            
            # ========== éšæ®µ 3: æ™ºèƒ½é¸æ“‡å¯¦é«” ==========
            if query_entities and doc_entities:
                # æ‰¾äº¤é›†
                relevant_entities = list(set(query_entities) & set(doc_entities))
                
                if relevant_entities:
                    print(f"   ğŸ¯ æ‰¾åˆ° {len(relevant_entities)} å€‹å•é¡Œç›¸é—œå¯¦é«”")
                    entities = relevant_entities
                else:
                    # ç„¡äº¤é›†ï¼šåˆä½µå¾Œå–å‰ 5 å€‹
                    print(f"   âš ï¸ å•é¡Œå¯¦é«”ä¸åœ¨æ–‡æª”ä¸­ï¼Œåˆä½µä½¿ç”¨")
                    entities = (query_entities + doc_entities)[:5]
            else:
                # åªæœ‰ä¸€æ–¹æœ‰å¯¦é«”
                entities = query_entities or doc_entities
            
            if not entities:
                return {'triplet_relations': [], 'entity_count': 0}
            
            # ========== éšæ®µ 4: æŸ¥è©¢é—œä¿‚ ==========
            triplet_relations = self._query_triplet_relationships_limited(entities)
            community_summaries = self._get_communities_for_entities(entities)
            
            print(f"   ğŸ”— ä¸‰å…ƒçµ„å¢å¼·å®Œæˆ: {len(entities)} å€‹å¯¦é«”ï¼Œ{len(triplet_relations)} å€‹é—œä¿‚")
            
            return {
                'triplet_relations': triplet_relations,
                'entity_count': len(entities),
                'extracted_entities': entities,
                'community_summaries': community_summaries,
                'query_entities': query_entities,  # è¨˜éŒ„å•é¡Œå¯¦é«”
                'doc_entities': doc_entities       # è¨˜éŒ„æ–‡æª”å¯¦é«”
            }
            
        except Exception as e:
            print(f"   âŒ ä¸‰å…ƒçµ„åœ–è­œå¢å¼·å¤±æ•—: {e}")
            return {'triplet_relations': [], 'entity_count': 0}

    # ã€æ–°å¢ã€‘é™åˆ¶æŸ¥è©¢ä¸‰å…ƒçµ„é—œä¿‚ï¼ˆåªå–10ç­†ï¼‰
    def _query_triplet_relationships_limited(self, entities: List[str]) -> List[dict]:
        if not entities:
            return []
        
        query = """
        UNWIND $entities AS entity_name
        MATCH (s)-[r]->(o)
        WHERE s.name CONTAINS entity_name 
        OR o.name CONTAINS entity_name
        OR s.name = entity_name 
        OR o.name = entity_name
        WITH s, r, o, entity_name
        WHERE type(r) IN ['ORGANIZATIONAL', 'SPATIAL', 'CONTACT', 'ACADEMIC', 
                        'FACILITY', 'SERVICE', 'PERSONNEL', 'TEMPORAL',
                        'COLLABORATION', 'INFORMATION', 'ADMINISTRATIVE', 'ATTRIBUTE']
        RETURN 
            s.name as subject,
            type(r) as relation_type,
            r.original_relation as original_relation,
            o.name as object,
            entity_name as matched_entity
        LIMIT 10
        """
        
        try:
            with self.triplet_driver.session() as session:
                result = session.run(query, entities=entities)
                relations = []
                
                for record in result:
                    relations.append({
                        'subject': record['subject'],
                        'relation_type': record['relation_type'], 
                        'original_relation': record['original_relation'],
                        'object': record['object'],
                        'matched_entity': record['matched_entity']
                    })
                
                return relations
                
        except Exception as e:
            print(f"âŒ æŸ¥è©¢ä¸‰å…ƒçµ„é—œä¿‚å¤±æ•—: {e}")
            return []
        
    # å»ºç«‹å„ªåŒ–ç‰ˆå¢å¼·æ–‡æª”
    def _create_enhanced_document_optimized(self, neo4j_data: dict, weaviate_info: dict) -> Document:
        try:
            content_parts = []
            
            # Weaviate è‹±æ–‡æ‘˜è¦
            base_content = weaviate_info.get('english_summary', '')
            if base_content:
                content_parts.append(base_content)
            
            # Neo4j Content åŸå§‹æ–‡æœ¬
            content_text = neo4j_data.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            # Article è³‡è¨Š
            article_title = neo4j_data.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
                
            article_url = neo4j_data.get('article_url')
            if article_url:
                content_parts.append(f"ã€æ–‡ç« ç¶²å€ã€‘\n{article_url}")
            
            # çµ„ç¹”è³‡è¨Š
            org_name = neo4j_data.get('org_name')
            if org_name:
                org_info = org_name
                org_type = neo4j_data.get('org_type')
                if org_type:
                    org_info += f" ({org_type})"
                content_parts.append(f"ã€æ‰€å±¬çµ„ç¹”ã€‘\n{org_info}")
            
              # ã€æ–°å¢ã€‘ä¸‰å…ƒçµ„åœ–è­œé—œä¿‚è³‡è¨Š
            triplet_relations = neo4j_data.get('triplet_relations', [])
            if triplet_relations:
                relation_text = "ã€çŸ¥è­˜åœ–è­œé—œä¿‚ã€‘\n"
                for i, rel in enumerate(triplet_relations, 1):
                    subject = rel.get('subject', '')
                    relation_type = rel.get('relation_type', '')
                    original_relation = rel.get('original_relation', '')
                    obj = rel.get('object', '')
                    
                    relation_line = f"{i}. {subject} --[{relation_type}]--> {obj}"
                    if original_relation and original_relation != relation_type:
                        relation_line += f" (åŸå§‹é—œä¿‚: {original_relation})"
                    
                    relation_text += relation_line + "\n"
                
                content_parts.append(relation_text)
            # ã€æ–°å¢ã€‘ç¤¾ç¾¤æ‘˜è¦è³‡è¨Š
            community_summaries = neo4j_data.get('community_summaries', [])
            if community_summaries:
                community_text = "ã€ç›¸é—œé ˜åŸŸèƒŒæ™¯ã€‘\n"
                for i, comm in enumerate(community_summaries, 1):
                    summary = comm.get('summary', '')
                    entity_count = comm.get('entity_count', 0)
                    community_text += f"{i}. {summary} (æ¶µè“‹ {entity_count} å€‹ç›¸é—œå¯¦é«”)\n\n"
                content_parts.append(community_text)
            
            # çµ„åˆå…§å®¹
            enhanced_content = "\n\n".join(content_parts) if content_parts else "ç„¡å…§å®¹"
            
            # å»ºç«‹ metadata
            metadata = {
                'source': 'Weaviate+Neo4j+GraphRAG',
                'neo4j_id': neo4j_data.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'enhanced': True,
                'graphrag_enhanced': len(triplet_relations) > 0,  # ã€æ–°å¢ã€‘
                'relation_count': len(triplet_relations),  # ã€æ–°å¢ã€‘
                'entity_count': neo4j_data.get('entity_count', 0),  # ã€æ–°å¢ã€‘
                 'community_count': len(community_summaries),
                'article_title': article_title or '',
                'article_url': neo4j_data.get('article_url', ''),
                'content_type': neo4j_data.get('content_type', ''),
                'organization': org_name or ''
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ æ–‡æª”å»ºç«‹å¤±æ•—: {e}")
            # è¿”å›åŸºç¤ Weaviate æ–‡æª”
            return Document(
                page_content=weaviate_info.get('english_summary', 'Error'),
                metadata={
                    'source': 'Error_fallback',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'enhanced': False,
                    'graphrag_enhanced': False
                }
            )
    
    # æ ¹æ“š Neo4j æŸ¥è©¢çµæœå’Œ Weaviate è³‡è¨Šå»ºç«‹å¢å¼·æ–‡æª”ï¼ˆä¿ç•™åŸå§‹ç‰ˆæœ¬ä½œç‚ºå‚™ç”¨ï¼‰
    def _create_enhanced_document(self, neo4j_record, weaviate_info: dict) -> Document:
        try:
            # åŸºç¤å…§å®¹å¾ Weaviate çš„ english_summary é–‹å§‹
            base_content = weaviate_info.get('english_summary', '')
            content_parts = [base_content] if base_content else []
            
            # æ·»åŠ  Neo4j Content çš„åŸå§‹æ–‡æœ¬
            content_text = neo4j_record.get('content_text')
            if content_text and content_text != base_content:
                content_parts.append(f"ã€åŸå§‹å…§å®¹ã€‘\n{content_text}")
            
            # æ·»åŠ  Article è³‡è¨Š
            article_title = neo4j_record.get('article_title')
            if article_title:
                content_parts.append(f"ã€æ–‡ç« æ¨™é¡Œã€‘\n{article_title}")
            
            article_domain = neo4j_record.get('article_domain')
            if article_domain:
                content_parts.append(f"ã€ç¶²ç«™ä¾†æºã€‘\n{article_domain}")
            
            # æ·»åŠ æ–‡ç« æ‘˜è¦
            article_summary = neo4j_record.get('article_summary')
            if article_summary:
                content_parts.append(f"ã€æ–‡ç« æ‘˜è¦ã€‘\n{article_summary}")
            
            # æ·»åŠ åœ–ç‰‡è³‡è¨Š
            article_og_image = neo4j_record.get('article_og_image')
            if article_og_image:
                content_parts.append(f"ã€æ–‡ç« åœ–ç‰‡ã€‘\n{article_og_image}")
            
            # æ·»åŠ çµ„ç¹”è³‡è¨Š
            organizations = neo4j_record.get('organizations', [])
            valid_organizations = [o for o in organizations if o.get('name')]
            if valid_organizations:
                org_lines = []
                for org in valid_organizations:
                    org_name = org.get('name', '')
                    org_type = org.get('type', '')
                    unified_number = org.get('unified_number', '')
                    
                    org_line = org_name
                    if org_type:
                        org_line += f" ({org_type})"
                    if unified_number:
                        org_line += f" [çµ±ç·¨: {unified_number}]"
                    org_lines.append(org_line)
                
                if org_lines:
                    content_parts.append(f"ã€æ‰€å±¬çµ„ç¹”ã€‘\n" + "\n".join(org_lines))
            
            # æ·»åŠ éƒ¨é–€è³‡è¨Š
            departments = neo4j_record.get('departments', [])
            valid_departments = [d for d in departments if d.get('name')]
            if valid_departments:
                dept_lines = []
                for dept in valid_departments:
                    dept_name = dept.get('name', '')
                    dept_type = dept.get('type', '')
                    dept_description = dept.get('description', '')
                    
                    dept_line = dept_name
                    if dept_type:
                        dept_line += f" ({dept_type})"
                    if dept_description:
                        dept_line += f" - {dept_description}"
                    dept_lines.append(dept_line)
                
                if dept_lines:
                    content_parts.append(f"ã€æ‰€å±¬éƒ¨é–€ã€‘\n" + "\n".join(dept_lines))
            
            # æ·»åŠ è¯çµ¡è³‡è¨Š
            contacts = neo4j_record.get('contacts', [])
            valid_contacts = [c for c in contacts if c.get('value')]
            if valid_contacts:
                contact_lines = []
                for contact in valid_contacts:
                    contact_type = contact.get('type', 'è¯çµ¡æ–¹å¼')
                    contact_value = contact.get('value', '')
                    contact_description = contact.get('description', '')
                    
                    contact_line = f"{contact_type}: {contact_value}"
                    if contact_description:
                        contact_line += f" ({contact_description})"
                    contact_lines.append(contact_line)
                
                if contact_lines:
                    content_parts.append(f"ã€è¯çµ¡è³‡è¨Šã€‘\n" + "\n".join(contact_lines))
            
            # æ·»åŠ åœ°å€è³‡è¨Š
            addresses = neo4j_record.get('addresses', [])
            valid_addresses = [a for a in addresses if a.get('full_address')]
            if valid_addresses:
                address_lines = []
                for addr in valid_addresses:
                    full_addr = addr.get('full_address', '')
                    campus_name = addr.get('campus_name', '')
                    city = addr.get('city', '')
                    district = addr.get('district', '')
                    postal_code = addr.get('postal_code', '')
                    
                    addr_line = full_addr
                    
                    # æ·»åŠ æ ¡å€åç¨±
                    if campus_name:
                        addr_line = f"{campus_name} - {addr_line}"
                    
                    # æ·»åŠ åŸå¸‚å€åŸŸè³‡è¨Š
                    location_info = []
                    if city and district:
                        location_info.append(f"{city}{district}")
                    elif city:
                        location_info.append(city)
                    
                    if postal_code:
                        location_info.append(f"éƒµéå€è™Ÿ: {postal_code}")
                    
                    if location_info:
                        addr_line += f" ({', '.join(location_info)})"
                    
                    address_lines.append(addr_line)
                
                if address_lines:
                    content_parts.append(f"ã€åœ°å€è³‡è¨Šã€‘\n" + "\n".join(address_lines))
            
            # æ·»åŠ ç›¸é—œå…§å®¹
            related_contents = neo4j_record.get('related_contents', [])
            valid_related = [r for r in related_contents if r.get('text')]
            if valid_related:
                related_lines = []
                for related in valid_related[:3]:  # åªé¡¯ç¤ºå‰3å€‹ç›¸é—œå…§å®¹
                    text = related.get('text', '')
                    content_type = related.get('type', 'æœªçŸ¥é¡å‹')
                    order = related.get('order', '')
                    
                    # é™åˆ¶ç›¸é—œå…§å®¹é•·åº¦
                    preview = text[:150] + "..." if len(text) > 150 else text
                    related_line = f"[{content_type}]"
                    if order:
                        related_line += f" (é †åº: {order})"
                    related_line += f" {preview}"
                    
                    related_lines.append(related_line)
                
                if related_lines:
                    content_parts.append(f"ã€ç›¸é—œå…§å®¹ã€‘\n" + "\n".join(related_lines))
            
            # çµ„åˆå®Œæ•´å…§å®¹
            enhanced_content = "\n\n".join(content_parts)
            
            # å»ºç«‹å®Œæ•´çš„ metadata
            metadata = {
                'source': 'Weaviate+Neo4j',
                'weaviate_uuid': weaviate_info.get('weaviate_uuid', ''),
                'neo4j_id': neo4j_record.get('content_neo4j_id', ''),
                'similarity': weaviate_info.get('similarity', 0),
                'distance': weaviate_info.get('distance', 1.0),
                'enhanced': True,
                
                # Content è³‡è¨Š
                'content_id': neo4j_record.get('content_id', ''),
                'content_type': neo4j_record.get('content_type', ''),
                'content_order': neo4j_record.get('content_order', ''),
                
                # Article è³‡è¨Š
                'article_title': article_title or '',
                'article_url': neo4j_record.get('article_url', ''),
                'article_domain': article_domain or '',
                'article_summary': article_summary or '',
                'article_og_image': article_og_image or '',
                'article_updated_at': neo4j_record.get('article_updated_at', ''),
                
                # çµ„ç¹”è³‡è¨Š
                'organizations': [org.get('name', '') for org in valid_organizations],
                'organization_types': [org.get('type', '') for org in valid_organizations],
                'unified_numbers': [org.get('unified_number', '') for org in valid_organizations],
                
                # éƒ¨é–€è³‡è¨Š
                'departments': [dept.get('name', '') for dept in valid_departments],
                'department_types': [dept.get('type', '') for dept in valid_departments],
                'department_descriptions': [dept.get('description', '') for dept in valid_departments],
                
                # è¯çµ¡è³‡è¨Š
                'contact_types': [c.get('type', '') for c in valid_contacts],
                'contact_values': [c.get('value', '') for c in valid_contacts],
                
                # åœ°å€è³‡è¨Š
                'campus_names': [a.get('campus_name', '') for a in valid_addresses],
                'cities': [a.get('city', '') for a in valid_addresses],
                'districts': [a.get('district', '') for a in valid_addresses],
                'postal_codes': [a.get('postal_code', '') for a in valid_addresses],
                
                # çµ±è¨ˆè³‡è¨Š
                'organization_count': len(valid_organizations),
                'contact_count': len(valid_contacts),
                'address_count': len(valid_addresses),
                'department_count': len(valid_departments),
                'related_content_count': len(valid_related)
            }
            
            return Document(page_content=enhanced_content, metadata=metadata)
            
        except Exception as e:
            print(f"âŒ å»ºç«‹å¢å¼·æ–‡æª”å¤±æ•—: {e}")
            # è¿”å›åŸºç¤æ–‡æª”
            return Document(
                page_content=weaviate_info.get('english_summary', 'å»ºç«‹æ–‡æª”å¤±æ•—'),
                metadata={
                    'source': 'Fallback',
                    'neo4j_id': weaviate_info.get('neo4j_id', ''),
                    'similarity': weaviate_info.get('similarity', 0),
                    'enhanced': False
                }
            )

    # GraphRAG ç³»çµ±æœ€çµ‚å¥åº·æª¢æŸ¥
    def final_graphrag_health_check(self):
        print("\n" + "="*70)
        print("ğŸ¥ GraphRAG ç³»çµ±å¥åº·æª¢æŸ¥")
        print("="*70)
        
        if not self.triplet_driver:
            print("âŒ Neo4j é€£æ¥å¤±æ•—")
            return False
        
        try:
            with self.triplet_driver.session(database="nuu-triplet") as session:
                
                # ========== æª¢æŸ¥ 1: åŸºç¤è³‡æ–™å®Œæ•´æ€§ ==========
                print("\nğŸ“Š æª¢æŸ¥ 1: åŸºç¤è³‡æ–™å®Œæ•´æ€§")
                print("-" * 70)
                
                checks = {
                    "å¯¦é«”ç¯€é»": "MATCH (e:Entity) RETURN count(e) as count",
                    "é—œä¿‚æ•¸é‡": "MATCH ()-[r]->() RETURN count(r) as count",
                    "Community ç¯€é»": "MATCH (c:Community) RETURN count(c) as count",
                    "æœ‰æ‘˜è¦çš„ Community": "MATCH (c:Community) WHERE c.summary IS NOT NULL RETURN count(c) as count",
                    "CHILD_OF é—œä¿‚": "MATCH ()-[r:CHILD_OF]->() RETURN count(r) as count"
                }
                
                all_pass = True
                for name, query in checks.items():
                    result = session.run(query)
                    count = result.single()['count']
                    status = "âœ…" if count > 0 else "âŒ"
                    print(f"   {status} {name}: {count:,}")
                    if count == 0 and name != "CHILD_OF é—œä¿‚":
                        all_pass = False
                
                if not all_pass:
                    print("\nâŒ åŸºç¤è³‡æ–™ä¸å®Œæ•´ï¼Œè«‹å…ˆå®Œæˆç´¢å¼•å»ºç«‹")
                    return False
                
                # ========== æª¢æŸ¥ 2: ç¤¾ç¾¤å±¤ç´šåˆ†å¸ƒ ==========
                print("\nğŸ“Š æª¢æŸ¥ 2: ç¤¾ç¾¤å±¤ç´šåˆ†å¸ƒ")
                print("-" * 70)
                
                level_query = """
                MATCH (c:Community)
                WHERE c.summary IS NOT NULL
                WITH c.level as level, count(c) as count
                ORDER BY level
                RETURN level, count
                """
                
                levels = list(session.run(level_query))
                if not levels:
                    print("   âŒ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å±¤ç´šè³‡æ–™")
                    return False
                
                for record in levels:
                    level = record['level']
                    count = record['count']
                    print(f"   âœ… Level {level}: {count:,} å€‹ç¤¾ç¾¤")
                
                # ========== æª¢æŸ¥ 3: ç¤¾ç¾¤æ‘˜è¦å¤§å°å®‰å…¨æ€§ï¼ˆæ ¸å¿ƒæª¢æŸ¥ï¼‰==========
                print("\nğŸ” æª¢æŸ¥ 3: ç¤¾ç¾¤æ‘˜è¦å¤§å°å®‰å…¨æ€§")
                print("-" * 70)
                
                size_query = """
                MATCH (c:Community)
                WHERE c.summary IS NOT NULL
                WITH c.level as level,
                    c.id as community_id,
                    size(c.summary) as char_count,
                    size(c.summary) / 4 as estimated_tokens
                WHERE estimated_tokens > 4000
                RETURN level, community_id, char_count, estimated_tokens
                ORDER BY estimated_tokens DESC
                """
                
                oversized = list(session.run(size_query))
                
                if oversized:
                    print(f"   âš ï¸  ç™¼ç¾ {len(oversized)} å€‹è¶…å¤§ç¤¾ç¾¤æ‘˜è¦ (>4000 tokens)")
                    print("\n   è¶…å¤§ç¤¾ç¾¤è©³æƒ…:")
                    for i, record in enumerate(oversized[:10], 1):
                        level = record['level']
                        comm_id = record['community_id']
                        char_count = record['char_count']
                        tokens = record['estimated_tokens']
                        print(f"      {i}. Level {level} | ç¤¾ç¾¤ {comm_id} | {char_count:,} å­— (~{tokens:,} tokens)")
                    
                    if len(oversized) > 10:
                        print(f"      ... é‚„æœ‰ {len(oversized) - 10} å€‹")
                    
                    print("\n   âš ï¸  å»ºè­°ï¼šåœ¨ generate_global() ä¸­åŠ å…¥æˆªæ–·é‚è¼¯")
                    print("   ç¯„ä¾‹ç¨‹å¼ç¢¼å·²è¼¸å‡ºåˆ°å»ºè­°å€å¡Š")
                    safety_pass = False
                else:
                    print("   âœ… æ‰€æœ‰ç¤¾ç¾¤æ‘˜è¦å¤§å°éƒ½åœ¨å®‰å…¨ç¯„åœå…§ (â‰¤4000 tokens)")
                    safety_pass = True
                
                # ========== æª¢æŸ¥ 4: æ‘˜è¦å“è³ªæŠ½æ¨£ ==========
                print("\nğŸ“Š æª¢æŸ¥ 4: æ‘˜è¦å“è³ªæŠ½æ¨£ï¼ˆéš¨æ©Ÿ 3 å€‹ï¼‰")
                print("-" * 70)
                
                sample_query = """
                MATCH (c:Community)
                WHERE c.summary IS NOT NULL
                WITH c, rand() as r
                ORDER BY r
                LIMIT 3
                RETURN c.level as level,
                    c.id as community_id,
                    c.summary as summary,
                    c.entity_count as entity_count
                """
                
                samples = list(session.run(sample_query))
                for i, record in enumerate(samples, 1):
                    level = record['level']
                    comm_id = record['community_id']
                    summary = record['summary']
                    entity_count = record['entity_count'] or 0
                    
                    preview = summary[:150] + "..." if len(summary) > 150 else summary
                    
                    print(f"\n   ç¯„ä¾‹ {i}: Level {level} | ç¤¾ç¾¤ {comm_id}")
                    print(f"   å¯¦é«”æ•¸: {entity_count}")
                    print(f"   æ‘˜è¦é è¦½: {preview}")
                
                # ========== æª¢æŸ¥ 5: éšå±¤å®Œæ•´æ€§ ==========
                print("\nğŸ“Š æª¢æŸ¥ 5: éšå±¤å®Œæ•´æ€§")
                print("-" * 70)
                
                hierarchy_query = """
                MATCH (child:Community)-[:CHILD_OF]->(parent:Community)
                WITH child.level as child_level, parent.level as parent_level, count(*) as count
                ORDER BY child_level
                RETURN child_level, parent_level, count
                """
                
                hierarchy = list(session.run(hierarchy_query))
                if hierarchy:
                    print("   âœ… çˆ¶å­é—œä¿‚çµ±è¨ˆ:")
                    for record in hierarchy:
                        child_level = record['child_level']
                        parent_level = record['parent_level']
                        count = record['count']
                        print(f"      Level {child_level} â†’ Level {parent_level}: {count:,} å€‹é—œä¿‚")
                else:
                    print("   âš ï¸  è­¦å‘Š: æ²’æœ‰æ‰¾åˆ° CHILD_OF é—œä¿‚ï¼ˆå¦‚æœåªæœ‰å–®å±¤ç´šå‰‡æ­£å¸¸ï¼‰")
                
                # ========== æœ€çµ‚ç¸½çµ ==========
                print("\n" + "="*70)
                if all_pass and safety_pass:
                    print("âœ… GraphRAG ç³»çµ±å¥åº·æª¢æŸ¥é€šéï¼")
                    print("="*70)
                    print("\nğŸ‰ ç³»çµ±å·²æº–å‚™å°±ç·’ï¼Œå¯ä»¥é–‹å§‹ä½¿ç”¨:")
                    print("   â€¢ Global Search: ä½¿ç”¨ç¤¾ç¾¤æ‘˜è¦å›ç­”å…¨å±€å•é¡Œ")
                    print("   â€¢ Local Search: ä½¿ç”¨å‘é‡æª¢ç´¢å›ç­”å…·é«”å•é¡Œ")
                    print("   â€¢ æ™ºèƒ½ç­–ç•¥: è‡ªå‹•åˆ¤æ–·ä½¿ç”¨å“ªç¨®æœå°‹æ–¹å¼")
                    return True
                elif all_pass and not safety_pass:
                    print("âš ï¸  GraphRAG ç³»çµ±å¯ç”¨ï¼Œä½†å»ºè­°å„ªåŒ–")
                    print("="*70)
                    print("\nğŸ’¡ å»ºè­°:")
                    print("   â€¢ åŠ å…¥æ‘˜è¦æˆªæ–·é‚è¼¯é¿å… token è¶…é™")
                    print("   â€¢ æˆ–è€…é‡æ–°ç”Ÿæˆè¶…å¤§ç¤¾ç¾¤çš„æ‘˜è¦")
                    return True
                else:
                    print("âŒ GraphRAG ç³»çµ±æœªé€šéæª¢æŸ¥")
                    print("="*70)
                    return False
                
        except Exception as e:
            print(f"\nâŒ æª¢æŸ¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()
            return False
# RAG ç³»çµ±ç‹€æ…‹å®šç¾©
class State(TypedDict):
    question: str
    original_question: str
    translated_question: str
    is_chinese_query: bool
    strategy: str  # æ–°å¢ï¼šè¨˜éŒ„æœå°‹ç­–ç•¥
    context: List[Document]
    answer: str

# åˆå§‹åŒ– ContentSummaryEn0913 RAG
content_summary_rag = ContentSummaryEn0913RAG()

# è™•ç†å•é¡Œ - æª¢æŸ¥æ˜¯å¦ç‚ºä¸­æ–‡ä¸¦é€²è¡Œç¿»
def process_question(state: State):
    question = state["question"]
    original_question = question
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
    is_chinese_query = is_chinese(question)
    
    if is_chinese_query:
        print(f"ğŸˆ¶ æª¢æ¸¬åˆ°ä¸­æ–‡æŸ¥è©¢ï¼Œæº–å‚™ç¿»è­¯...")
        try:
            # ä½¿ç”¨ Mistral ç¿»è­¯ä¸­æ–‡ç‚ºè‹±æ–‡
            translated_question = translate_with_mistral(question)
            print(f"ğŸ”„ ç¿»è­¯çµæœ: {translated_question}")
        except Exception as e:
            print(f"âš ï¸ ç¿»è­¯å¤±æ•—ï¼Œä½¿ç”¨åŸå§‹å•é¡Œ: {e}")
            translated_question = question
            is_chinese_query = False
    else:
        print(f"ğŸ”¤ æª¢æ¸¬åˆ°è‹±æ–‡æŸ¥è©¢ï¼Œç›´æ¥ä½¿ç”¨")
        translated_question = question
    
    return {
        "question": translated_question,  # ç”¨æ–¼æª¢ç´¢çš„å•é¡Œï¼ˆè‹±æ–‡ï¼‰
        "original_question": original_question,  # åŸå§‹å•é¡Œ
        "translated_question": translated_question,  # ç¿»è­¯å¾Œçš„å•é¡Œ
        "is_chinese_query": is_chinese_query  # æ˜¯å¦ç‚ºä¸­æ–‡æŸ¥è©¢
    }

# æª¢ç´¢ç›¸é—œæ–‡æª” - Weaviate å‘é‡æœå°‹ + Neo4j è³‡æ–™å¢å¼·
def retrieve(state: State):
    question = state["question"]
    
    # ä½¿ç”¨ Mistral åˆ¤æ–·ç­–ç•¥
    print(f"\nğŸ¤” ä½¿ç”¨ Mistral åˆ¤æ–·æœå°‹ç­–ç•¥...")
    strategy = choose_search_strategy_with_mistral(state["original_question"])
    print(f"ğŸ¯ Mistral åˆ¤æ–·çµæœ: {strategy.upper()}")
    
    if strategy == "global":
        # === Global Search: ä½¿ç”¨æ™ºèƒ½å±¤ç´šé¸æ“‡ ===
        print("ğŸŒ åŸ·è¡Œ Global Searchï¼ˆæ™ºèƒ½å±¤ç´šé¸æ“‡ï¼‰")
        
        # è‡ªå‹•é¸æ“‡æœ€ä½³å±¤ç´šä¸¦æª¢ç´¢
        communities = content_summary_rag.find_relevant_communities(
            query=question,
            limit=None,   # è‡ªå‹•æ±ºå®š
            level=None    # è‡ªå‹•é¸æ“‡
        )
        
        if not communities:
            print("âš ï¸ Global Search ç„¡çµæœï¼Œè‡ªå‹•åˆ‡æ›åˆ° Local Search")
            strategy = "local"
        else:
            print(f"âœ… æ‰¾åˆ° {len(communities)} å€‹ç›¸é—œç¤¾ç¾¤")
            community_docs = []
            for comm in communities:
                doc = Document(
                    page_content=comm['summary'],
                    metadata={
                        'source': 'Community',
                        'strategy': 'global',
                        'community_id': comm['community_id'],
                        'level': comm['level'],
                        'entity_count': comm['entity_count'],
                        'core_relevance': comm.get('core_relevance', 0),
                        'aux_relevance': comm.get('aux_relevance', 0)
                    }
                )
                community_docs.append(doc)
            
            return {"context": community_docs, "strategy": strategy}
    
    # === Local Search: ä½¿ç”¨å‘é‡æª¢ç´¢ + åœ–è­œå¢å¼· ===
    print("ğŸ“ åŸ·è¡Œ Local Search (å‘é‡æª¢ç´¢ + åœ–è­œå¢å¼·)")
    documents = content_summary_rag.search(question, limit=10)
    
    # æ¨™è¨˜ç­–ç•¥
    for doc in documents:
        doc.metadata['strategy'] = 'local'
    
    print(f"\nğŸ“Š æª¢ç´¢çµæœçµ±è¨ˆ:")
    print(f"ğŸ” ç¸½å…±æª¢ç´¢åˆ° {len(documents)} ç­†ç›¸é—œè³‡æ–™")
    
    enhanced_count = sum(1 for doc in documents if doc.metadata.get('enhanced', False))
    print(f"âœ¨ å…¶ä¸­ {enhanced_count} ç­†å·²é€šé Neo4j å¢å¼·")
    
    return {"context": documents, "strategy": strategy}

# è§£æ Mistral å›æ‡‰ä¸­çš„è©•åˆ†å’Œç­”æ¡ˆ
def parse_score_and_answer(content: str) -> tuple:
    """
    Returns:
        (score, answer): è©•åˆ†ï¼ˆintï¼‰å’Œç­”æ¡ˆå…§å®¹ï¼ˆstrï¼‰
    """
    import re
    
    # åŒ¹é…è©•åˆ†
    score_match = re.search(r'ã€è©•åˆ†ã€‘:\s*(\d+)|ã€Scoreã€‘:\s*(\d+)', content)
    score = 0
    if score_match:
        score = int(score_match.group(1) or score_match.group(2))
    
    # åŒ¹é…ç­”æ¡ˆ
    answer_match = re.search(r'ã€ç­”æ¡ˆã€‘:(.*)|ã€Answerã€‘:(.*)', content, re.DOTALL)
    if answer_match:
        answer = (answer_match.group(1) or answer_match.group(2)).strip()
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ ¼å¼ï¼Œè¿”å›å…¨éƒ¨å…§å®¹
        answer = content
    
    return score, answer

# ç”Ÿæˆç­”æ¡ˆ - æ”¹é€²ç‰ˆ Global Search-åŠ å…¥æ‘˜è¦å¤§å°æª¢æŸ¥
def generate_global(state: State):
    communities = state["context"]
    
    # ========== éšæ®µ 0: éš¨æ©Ÿæ‰“äº‚ ==========
    import random
    random.shuffle(communities)
    print(f"\nğŸ”€ éš¨æ©Ÿæ‰“äº‚ {len(communities)} å€‹ç¤¾ç¾¤")
    
    # ========== éšæ®µ 1: åˆ†å¡Šï¼ˆåŠ å…¥å¤§å°æª¢æŸ¥ï¼‰==========
    print(f"\nğŸ“¦ éšæ®µ 1/4: æº–å‚™ç¤¾ç¾¤æ‘˜è¦ï¼ˆå«å¤§å°æª¢æŸ¥ï¼‰...")
    
    MAX_CHUNK_TOKENS = 4000
    
    def estimate_tokens(text: str) -> int:
        return len(text) // 4
    
    # ğŸ”‘ æ–°å¢ï¼šè™•ç†è¶…å¤§æ‘˜è¦
    processed_communities = []
    oversized_count = 0
    
    for comm_doc in communities:
        summary_text = comm_doc.page_content
        summary_tokens = estimate_tokens(summary_text)
        
        # å¦‚æœå–®ä¸€æ‘˜è¦è¶…éé™åˆ¶ï¼Œæˆªæ–·å®ƒ
        if summary_tokens > MAX_CHUNK_TOKENS:
            oversized_count += 1
            print(f"   âš ï¸  ç¤¾ç¾¤ {comm_doc.metadata.get('community_id')} éé•· ({summary_tokens} tokens)ï¼Œæˆªæ–·è‡³ {MAX_CHUNK_TOKENS}")
            
            # æˆªæ–·åˆ° 4000 tokens (ç´„ 16000 å­—å…ƒ)
            truncated_text = summary_text[:MAX_CHUNK_TOKENS * 4]
            
            # å»ºç«‹æ–°çš„ Document ç‰©ä»¶
            truncated_doc = Document(
                page_content=truncated_text,
                metadata={
                    **comm_doc.metadata,
                    'truncated': True,
                    'original_tokens': summary_tokens
                }
            )
            processed_communities.append(truncated_doc)
        else:
            processed_communities.append(comm_doc)
    
    if oversized_count > 0:
        print(f"   âš ï¸  å·²æˆªæ–· {oversized_count} å€‹è¶…å¤§æ‘˜è¦")
    else:
        print(f"   âœ… æ‰€æœ‰æ‘˜è¦å¤§å°æ­£å¸¸")
    
    # ä½¿ç”¨è™•ç†å¾Œçš„ç¤¾ç¾¤åˆ—è¡¨é€²è¡Œåˆ†å¡Š
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    for comm_doc in processed_communities:
        summary_tokens = estimate_tokens(comm_doc.page_content)
        
        # å¦‚æœåŠ å…¥å¾Œè¶…éé™åˆ¶ï¼Œé–‹å§‹æ–° chunk
        if current_tokens + summary_tokens > MAX_CHUNK_TOKENS and current_chunk:
            chunks.append(current_chunk)
            current_chunk = []
            current_tokens = 0
        
        current_chunk.append(comm_doc)
        current_tokens += summary_tokens
    
    # åŠ å…¥æœ€å¾Œä¸€å€‹ chunk
    if current_chunk:
        chunks.append(current_chunk)
    
    print(f"   âœ… åˆ†æˆ {len(chunks)} å€‹ chunks")
    for i, chunk in enumerate(chunks, 1):
        chunk_tokens = sum(estimate_tokens(c.page_content) for c in chunk)
        print(f"      Chunk {i}: {len(chunk)} å€‹ç¤¾ç¾¤, ç´„ {chunk_tokens} tokens")
    
    # ========== éšæ®µ 2: Map - ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆ + è©•åˆ† ==========
    print(f"\nğŸ—ºï¸  éšæ®µ 2/4: Map - ç‚ºæ¯å€‹ chunk ç”Ÿæˆéƒ¨åˆ†ç­”æ¡ˆ...")
    partial_answers = []
    
    for i, chunk in enumerate(chunks, 1):
        print(f"\n   [{i}/{len(chunks)}] è™•ç† Chunk {i}")
        
        # çµ„åˆ chunk å…§çš„ç¤¾ç¾¤æ‘˜è¦
        chunk_text = ""
        for j, comm_doc in enumerate(chunk, 1):
            comm_id = comm_doc.metadata.get('community_id')
            summary = comm_doc.page_content
            chunk_text += f"ã€ç¤¾ç¾¤ {comm_id}ã€‘\n{summary}\n\n"
        
        # Map Promptï¼ˆåŠ å…¥è©•åˆ†è¦æ±‚ï¼‰
        if state["is_chinese_query"]:
            map_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹ç¤¾ç¾¤æ‘˜è¦ï¼Œé‡å°ç”¨æˆ¶å•é¡Œæä¾›ç›¸é—œè³‡è¨Šä¸¦è©•åˆ†ã€‚

ç¤¾ç¾¤æ‘˜è¦ï¼ˆ{len(chunk)} å€‹ï¼‰ï¼š
{chunk_text}

ç”¨æˆ¶å•é¡Œï¼š{state["original_question"]}

è«‹å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š
1. æä¾›é€™äº›ç¤¾ç¾¤çš„ç›¸é—œç­”æ¡ˆï¼ˆå¦‚ç„¡é—œè«‹æ˜ç¢ºèªªæ˜ã€Œç„¡ç›¸é—œè³‡è¨Šã€ï¼‰
2. **è©•ä¼°é€™å€‹ç­”æ¡ˆå°å›ç­”å•é¡Œçš„å¹«åŠ©ç¨‹åº¦ï¼Œçµ¦å‡º 0-100 çš„åˆ†æ•¸**
   - 0 åˆ†ï¼šå®Œå…¨ç„¡é—œ
   - 30 åˆ†ï¼šæœ‰å°‘è¨±ç›¸é—œè³‡è¨Š
   - 60 åˆ†ï¼šéƒ¨åˆ†ç›¸é—œä¸”æœ‰ç”¨
   - 100 åˆ†ï¼šé«˜åº¦ç›¸é—œä¸”é—œéµ

è«‹åš´æ ¼ç”¨ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
ã€è©•åˆ†ã€‘: <0-100 çš„æ•¸å­—>
ã€ç­”æ¡ˆã€‘: <ä½ çš„ç­”æ¡ˆå…§å®¹>
"""
        else:
            map_prompt = f"""Based on the community summaries below, provide relevant information and rate its helpfulness.

Community Summaries ({len(chunk)} communities):
{chunk_text}

User Question: {state["original_question"]}

Please:
1. Provide relevant answer (or clearly state "No relevant information" if not related)
2. **Rate how helpful this answer is (0-100)**
   - 0: Completely irrelevant
   - 30: Slightly relevant
   - 60: Partially relevant and useful
   - 100: Highly relevant and critical

Strictly use this format:
ã€Scoreã€‘: <0-100 number>
ã€Answerã€‘: <your answer content>
"""
        
        try:
            response = call_mistral(map_prompt)
            content = response["choices"][0]["message"]["content"].strip()
            
            # è§£æè©•åˆ†å’Œç­”æ¡ˆ
            score, answer = parse_score_and_answer(content)
            
            partial_answers.append({
                'chunk_id': i,
                'community_count': len(chunk),
                'score': score,
                'answer': answer
            })
            
            print(f"      âœ… å®Œæˆ (è©•åˆ†: {score}/100, ç­”æ¡ˆé•·åº¦: {len(answer)} å­—)")
            
            # æ§åˆ¶é€Ÿç‡
            if i < len(chunks):
                time.sleep(0.5)
                
        except Exception as e:
            print(f"      âŒ å¤±æ•—: {e}")
            continue
    
    # ========== éšæ®µ 3: Filter - éæ¿¾ä½åˆ†ç­”æ¡ˆ ==========
    print(f"\nğŸ” éšæ®µ 3/4: Filter - éæ¿¾ä½åˆ†ç­”æ¡ˆ...")
    before_filter = len(partial_answers)
    filtered_answers = [pa for pa in partial_answers if pa['score'] > 0]
    after_filter = len(filtered_answers)
    
    print(f"   éæ¿¾å‰: {before_filter} å€‹ç­”æ¡ˆ")
    print(f"   éæ¿¾å¾Œ: {after_filter} å€‹ç­”æ¡ˆ (ç§»é™¤ {before_filter - after_filter} å€‹ç„¡é—œç­”æ¡ˆ)")
    
    if not filtered_answers:
        return {"answer": "æŠ±æ­‰ï¼Œç„¡æ³•å¾ç¤¾ç¾¤è³‡æ–™ä¸­æ‰¾åˆ°èˆ‡æ‚¨å•é¡Œç›¸é—œçš„è³‡è¨Šã€‚è«‹å˜—è©¦æ›´å…·é«”çš„å•é¡Œã€‚"}
    
    # ========== éšæ®µ 4: Reduce - Token-aware åˆä½µç­”æ¡ˆ ==========
    print(f"\nğŸ”„ éšæ®µ 4/4: Reduce - ç¶œåˆç­”æ¡ˆï¼ˆToken-awareï¼‰...")
    
    # æŒ‰è©•åˆ†é™åºæ’åºï¼ˆè«–æ–‡è¦æ±‚ï¼‰
    filtered_answers.sort(key=lambda x: x['score'], reverse=True)
    
    print("   ç­”æ¡ˆæ’åºï¼ˆæŒ‰ç›¸é—œåº¦é™åºï¼‰:")
    for i, pa in enumerate(filtered_answers[:5], 1):
        print(f"      {i}. Chunk {pa['chunk_id']} - è©•åˆ† {pa['score']}/100")
    
    # ğŸ”‘ é—œéµæ”¹å‹•ï¼šToken-aware å¡«æ»¿ context window
    MAX_REDUCE_TOKENS = 8000  # Reduce éšæ®µçš„æœ€å¤§ tokens
    
    selected_answers = []
    current_tokens = 0
    
    for pa in filtered_answers:
        answer_tokens = estimate_tokens(pa['answer'])
        
        # å¦‚æœåŠ å…¥å¾Œä¸è¶…éé™åˆ¶ï¼Œå‰‡åŠ å…¥
        if current_tokens + answer_tokens <= MAX_REDUCE_TOKENS:
            selected_answers.append(pa)
            current_tokens += answer_tokens
        else:
            # å·²é” token é™åˆ¶ï¼Œåœæ­¢åŠ å…¥
            break
    
    print(f"\n   âœ… é¸æ“‡ {len(selected_answers)}/{len(filtered_answers)} å€‹ç­”æ¡ˆé€²å…¥ Reduce")
    print(f"   ç¸½ tokens: ç´„ {current_tokens}/{MAX_REDUCE_TOKENS}")
    
    # çµ„åˆé¸ä¸­çš„ç­”æ¡ˆ
    combined_text = "å„ç¤¾ç¾¤æä¾›çš„è³‡è¨Šï¼ˆæŒ‰ç›¸é—œåº¦æ’åºï¼‰ï¼š\n\n"
    for pa in selected_answers:
        combined_text += f"ã€Chunk {pa['chunk_id']} - ç›¸é—œåº¦: {pa['score']}/100ã€‘\n"
        combined_text += f"(æ¶µè“‹ {pa['community_count']} å€‹ç¤¾ç¾¤)\n"
        combined_text += f"{pa['answer']}\n\n"
        combined_text += "-" * 60 + "\n\n"
    
    # Reduce Prompt
    if state["is_chinese_query"]:
        reduce_prompt = f"""ä½ æ˜¯åœ‹ç«‹è¯åˆå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯å¾ä¸åŒçŸ¥è­˜ç¤¾ç¾¤ç²å¾—çš„è³‡è¨Šï¼Œ**å·²æŒ‰ç›¸é—œåº¦é™åºæ’åº**ï¼ˆæœ€ç›¸é—œçš„åœ¨å‰é¢ï¼‰ã€‚

ç”¨æˆ¶å•é¡Œï¼š{state["original_question"]}

{combined_text}

è«‹æ ¹æ“šä»¥ä¸Šè³‡è¨Šæä¾›ä¸€å€‹å®Œæ•´ã€æº–ç¢ºçš„ç­”æ¡ˆï¼š
1. **å„ªå…ˆæ•´åˆé«˜ç›¸é—œåº¦ï¼ˆè©•åˆ†é«˜ï¼‰çš„è³‡è¨Š**
2. å»é™¤é‡è¤‡å…§å®¹
3. ç”¨æ¸…æ™°çš„çµæ§‹çµ„ç¹”ç­”æ¡ˆï¼ˆå¯ä½¿ç”¨æ¨™é¡Œã€æ¢åˆ—ç­‰ï¼‰
4. å¿½ç•¥æ¨™è¨»ã€Œç„¡ç›¸é—œè³‡è¨Šã€çš„éƒ¨åˆ†
5. å¦‚æœæ‰€æœ‰ç¤¾ç¾¤éƒ½ç„¡è¶³å¤ ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦å‘ŠçŸ¥
6. åœ¨æœ€å¾Œç°¡çŸ­èªªæ˜è³‡è¨Šä¾†æºï¼ˆä¾‹å¦‚ï¼šã€Œä»¥ä¸Šè³‡è¨Šç¶œåˆè‡ª X å€‹ç›¸é—œçŸ¥è­˜ç¤¾ç¾¤ã€ï¼‰
7. ç›´æ¥å›ç­”ç”¨æˆ¶å•é¡Œï¼Œä¸è¦é‡è¤‡å•é¡Œæœ¬èº«

æœ€çµ‚ç­”æ¡ˆï¼š"""
    else:
        reduce_prompt = f"""You are an intelligent assistant for National United University. Below are information from different knowledge communities, **sorted by relevance score** (most relevant first).

User Question: {state["original_question"]}

{combined_text}

Please provide a complete answer:
1. **Prioritize integrating high-scored (most relevant) information**
2. Remove duplicate content
3. Organize with clear structure (use headings, bullets)
4. Ignore "No relevant information" parts
5. If all communities lack sufficient info, be honest
6. Briefly mention information sources at the end
7. Answer the question directly

Final Answer:"""
    
    try:
        response = call_mistral(reduce_prompt)
        final_answer = response["choices"][0]["message"]["content"].strip()
        
        # æ¸…ç†æ ¼å¼
        if final_answer.startswith('"') and final_answer.endswith('"'):
            final_answer = final_answer[1:-1]
        
        # çµ±è¨ˆè³‡è¨Š
        stats_text = f"\n\n---\nğŸ’¡ æœ¬å›ç­”ç¶œåˆäº† {len(selected_answers)} å€‹ç›¸é—œçŸ¥è­˜å€å¡Š"
        if len(selected_answers) > 0:
            avg_score = sum(pa['score'] for pa in selected_answers) / len(selected_answers)
            stats_text += f"ï¼ˆå¹³å‡ç›¸é—œåº¦: {avg_score:.1f}/100ï¼‰"
        
        return {"answer": final_answer + stats_text}
        
    except Exception as e:
        print(f"âŒ Reduce éšæ®µå¤±æ•—: {e}")
        # é™ç´šå›æ‡‰
        fallback_answer = "æ ¹æ“šå¤šå€‹ç›¸é—œç¤¾ç¾¤çš„è³‡è¨Šï¼š\n\n" + combined_text
        return {"answer": fallback_answer}

# ========== è¼”åŠ©å‡½æ•¸ï¼šè§£æè©•åˆ†å’Œç­”æ¡ˆ ==========
def parse_score_and_answer(content: str) -> tuple:
    """è§£æ Mistral å›æ‡‰ä¸­çš„è©•åˆ†å’Œç­”æ¡ˆ
    
    Returns:
        (score, answer): è©•åˆ†ï¼ˆintï¼‰å’Œç­”æ¡ˆå…§å®¹ï¼ˆstrï¼‰
    """
    import re
    
    # åŒ¹é…è©•åˆ†
    score_match = re.search(r'ã€è©•åˆ†ã€‘:\s*(\d+)|ã€Scoreã€‘:\s*(\d+)', content)
    score = 0
    if score_match:
        score = int(score_match.group(1) or score_match.group(2))
    
    # åŒ¹é…ç­”æ¡ˆ
    answer_match = re.search(r'ã€ç­”æ¡ˆã€‘:(.*)|ã€Answerã€‘:(.*)', content, re.DOTALL)
    if answer_match:
        answer = (answer_match.group(1) or answer_match.group(2)).strip()
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ ¼å¼ï¼Œè¿”å›å…¨éƒ¨å…§å®¹
        answer = content
    
    return score, answer

# ç”Ÿæˆç­”æ¡ˆ - Local Search
def generate_local(state: State):
    # æ•´ç†æª¢ç´¢å…§å®¹
    context_text = ""
    for i, doc in enumerate(state["context"], 1):
        similarity = doc.metadata.get('similarity', 0)
        enhanced = doc.metadata.get('enhanced', False)
        content = doc.page_content
        article_url = doc.metadata.get('article_url', '')
        article_title = doc.metadata.get('article_title', '')
        
        context_text += f"[è³‡æ–™ {i}] ä¾†æº: {'Weaviate+Neo4jå¢å¼·' if enhanced else 'WeaviateåŸºç¤'} (ç›¸ä¼¼åº¦: {similarity:.3f})\n"
        if article_title:
            context_text += f"æ–‡ç« æ¨™é¡Œ: {article_title}\n"
        if article_url:
            context_text += f"æ–‡ç« ç¶²å€: {article_url}\n"
        context_text += f"{content}\n\n"
    
    # æ ¹æ“šæ˜¯å¦ç‚ºä¸­æ–‡æŸ¥è©¢é¸æ“‡ä¸åŒçš„æç¤ºæ¨¡æ¿
    if state["is_chinese_query"]:
        template = """ä½ æ˜¯åœ‹ç«‹è¯åˆå¤§å­¸çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æª¢ç´¢è³‡æ–™å›ç­”ç”¨æˆ¶å•é¡Œã€‚

        ## æª¢ç´¢èˆ‡å¢å¼·è³‡æ–™ï¼š
        {context}

        ## ç”¨æˆ¶å•é¡Œï¼š
        {original_question}

        ## å›ç­”æŒ‡å¼•ï¼š
        1. è«‹ç”¨ä¸­æ–‡å›ç­”
        2. å„ªå…ˆä½¿ç”¨å¢å¼·å¾Œçš„å®Œæ•´è³‡æ–™
        3. å¦‚æœæœ‰è¯çµ¡è³‡è¨Šã€åœ°å€ç­‰ï¼Œè«‹æ˜ç¢ºåˆ—å‡º
        4. æ¢ç†æ¸…æ™°ï¼Œé‡è¦è³‡è¨Šç”¨é …ç›®ç¬¦è™Ÿ
        5. å¦‚æœ‰ç›¸é—œç¶²å€è«‹æä¾›
        6. ä¸è¦åœ¨å›ç­”ä¸­åŠ å…¥è³‡æ–™ç·¨è™Ÿå¼•ç”¨
        7. ç›´æ¥ä½¿ç”¨è³‡æ–™å…§å®¹ï¼Œä¸éœ€æ¨™è¨»ä¾†æºç·¨è™Ÿ
        8. å¦‚æœè³‡æ–™è·Ÿå•é¡Œç„¡é—œï¼Œè«‹ä¸è¦äº‚ç·¨ç­”æ¡ˆï¼Œèª å¯¦å‘ŠçŸ¥

        å›ç­”ï¼š"""
    else:
         template = """You are an intelligent assistant for National United University. Please answer based on the retrieved data.

        ## Retrieved and Enhanced Data:
        {context}

        ## User Question:
        {original_question}

        ## Guidelines:
        1. Answer in English
        2. Prioritize enhanced comprehensive data
        3. Provide specific contact information and addresses if available
        4. Well-structured with bullet points for important information
        5. Include relevant URLs if available
        6. Do not include data source numbers
        7. Use data content directly without source citations

        Answer:"""
    
    prompt = PromptTemplate.from_template(template)
    messages = prompt.invoke({
        "context": context_text,
        "original_question": state["original_question"]
    })
    
    response = llm.invoke(messages)
    return {"answer": response.content}

# ç”Ÿæˆå›ç­”
def generate(state: State):
    """æ ¹æ“šç­–ç•¥é¸æ“‡å°æ‡‰çš„ç”Ÿæˆæ–¹æ³•"""
    strategy = state.get("strategy", "local")
    print(f"\nğŸ’¡ ä½¿ç”¨ {strategy.upper()} ç­–ç•¥ç”Ÿæˆç­”æ¡ˆ...")
    if strategy == "global":
        return generate_global(state)
    else:
        return generate_local(state)
    
# å»ºç«‹ RAG æµç¨‹
graph_builder = StateGraph(State).add_sequence([process_question, retrieve, generate])
graph_builder.add_edge(START, "process_question")
graph = graph_builder.compile()

# RAG å•ç­”ä¸»å‡½æ•¸
def ask_question(question: str) -> str:
    start_time = time.time()  
    result = graph.invoke({"question": question})
    end_time = time.time()   
    
    print(f"â±ï¸ ç¸½è™•ç†æ™‚é–“ï¼š{end_time - start_time:.2f} ç§’")
    return result["answer"]

# ä¸»ç¨‹å¼åŸ·è¡Œ
if __name__ == "__main__":
    print(f"\n{'='*60}")
    print(f"ğŸ“ åœ‹ç«‹è¯åˆå¤§å­¸ ContentSummaryEn0913 + Neo4j åœ–è­œå¢å¼· RAG å•ç­”ç³»çµ±")
    print(f"{'='*60}")
    print(f"ğŸ“Š ç³»çµ±ç‹€æ…‹: {'âœ… æ­£å¸¸é‹è¡Œ' if content_summary_rag.client and content_summary_rag.neo4j_driver else 'âŒ é€£æ¥å¤±æ•—'}")
    print(f"ğŸ”§ æ¶æ§‹: å•é¡Œè™•ç†(ç¿»è­¯) â†’ å‘é‡æª¢ç´¢ â†’ Neo4jåœ–è­œå¢å¼· â†’ ç”Ÿæˆ")
    print(f"ğŸ“š è³‡æ–™ä¾†æº: ContentSummaryEn0913 é›†åˆ (è‹±æ–‡æ‘˜è¦) + Neo4j åœ–è­œè³‡æ–™")
    print(f"ğŸ§  åµŒå…¥æ¨¡å‹: Gemini text_embedding-001")
    print(f"ğŸ” æœå°‹æ–¹æ³•: å‘é‡ç›¸ä¼¼åº¦æœå°‹ + åœ–è­œé—œä¿‚å¢å¼·")
    print(f"ğŸŒ ç¿»è­¯åŠŸèƒ½: ä¸­æ–‡å•é¡Œè‡ªå‹•ç¿»è­¯ç‚ºè‹±æ–‡é€²è¡Œæª¢ç´¢")
    print(f"ğŸ’¬ å›ç­”èªè¨€: æ ¹æ“šå•é¡Œèªè¨€è‡ªå‹•åˆ¤æ–·ï¼ˆä¸­æ–‡å•é¡Œç”¨ä¸­æ–‡å›ç­”ï¼Œè‹±æ–‡å•é¡Œç”¨è‹±æ–‡å›ç­”ï¼‰")
    print(f"{'='*60}\n")
    

    
    # æª¢æŸ¥ Mistral API Key
    if not MISTRAL_API_KEY:
        print("âš ï¸  è­¦å‘Š: æœªè¨­å®š MISTRAL_API_KEYï¼Œä¸­æ–‡ç¿»è­¯åŠŸèƒ½å°‡ç„¡æ³•ä½¿ç”¨")
    else:
        print("âœ… Mistral API å·²è¨­å®šï¼Œæ”¯æ´ä¸­æ–‡ç¿»è­¯åŠŸèƒ½")
    
    # æª¢æŸ¥ç¿»è­¯æ˜ å°„æª”æ¡ˆ
    try:
        mapping_count = len(translation_manager.translation_dict)
        print(f"âœ… ç¿»è­¯æ˜ å°„æª”æ¡ˆå·²è¼‰å…¥ï¼ŒåŒ…å« {mapping_count} æ¢ç¿»è­¯å°æ‡‰")
    except Exception as e:
        print(f"âš ï¸  è­¦å‘Š: ç¿»è­¯æ˜ å°„æª”æ¡ˆè¼‰å…¥å¤±æ•—: {e}")
    
    # æª¢æŸ¥ç³»çµ±é€£æ¥ç‹€æ…‹
    weaviate_status = "âœ… æ­£å¸¸" if content_summary_rag.client else "âŒ å¤±æ•—"
    neo4j_status = "âœ… æ­£å¸¸" if content_summary_rag.neo4j_driver else "âŒ å¤±æ•—"
    print(f"ğŸ”— Weaviate é€£æ¥: {weaviate_status}")
    print(f"ğŸ”— Neo4j é€£æ¥: {neo4j_status}")
    
    
    print("\n" + "="*60)
    print("ğŸ¥ åŸ·è¡Œ GraphRAG ç³»çµ±å¥åº·æª¢æŸ¥...")
    print("="*60)
    
    is_healthy = content_summary_rag.final_graphrag_health_check()
    
    if not is_healthy:
        print("\nâŒ GraphRAG ç³»çµ±æœªé€šéå¥åº·æª¢æŸ¥")
        print("   è«‹å…ˆä¿®å¾©ä¸Šè¿°å•é¡Œå¾Œå†ä½¿ç”¨ç³»çµ±")
        exit(1)  # å¦‚æœæª¢æŸ¥å¤±æ•—ï¼Œçµ‚æ­¢ç¨‹å¼
    
    print("\nâœ… GraphRAG ç³»çµ±æª¢æŸ¥å®Œæˆï¼Œé–‹å§‹å•ç­”æœå‹™...")
    
    # äº’å‹•å¼å•ç­”
    try:
        while True:
            print("-" * 50)
            question = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (ä¸­æ–‡æˆ–è‹±æ–‡ï¼Œè¼¸å…¥ 'exit' é€€å‡º): ")
            
            if question.lower() in ['exit', 'quit', 'é€€å‡º', 'é›¢é–‹']:
                print("è¬è¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                break
            
            if not question.strip():
                continue
                
            print(f"\nğŸ¤– æ­£åœ¨è™•ç†...")
            try:
                answer = ask_question(question)
                print(f"\nğŸ’¡ å›ç­”:\n{answer}\n")
                        
            except Exception as e:
                print(f"è™•ç†å•é¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                traceback.print_exc()
                
    except KeyboardInterrupt:
        print(f"\n\nç¨‹å¼ä¸­æ–·ï¼Œè¬è¬ä½¿ç”¨ï¼")